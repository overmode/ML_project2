{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "\n",
    "from extract_tokens import extract_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of tweets in the corpus\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "def import_(path, is_test_data):\n",
    "    if is_test_data :\n",
    "        with open(path, 'r') as f:\n",
    "            tweets = [line.strip()[line.find(\",\")+1:] for line in f]     # Make sure to withdraw the \"nbr\", \n",
    "    else: \n",
    "         with open(path, 'r') as f:\n",
    "            tweets = [line.strip() for line in f]    \n",
    "    return tweets\n",
    "\n",
    "def export(tweets, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"\\n\".join(tweets))\n",
    "        \n",
    "def token_to_string(token):\n",
    "    return (str(token)).replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\r",
      "[' j\\'adore le camembert (\"j\\'adore\",\\'le\\') (\\'le\\',\\'camembert\\') (\"j\\'adore\",\\'le\\',\\'camembert\\')', \" roses are red ('roses','are') ('are','red') ('roses','are','red')\"]\n"
     ]
    }
   ],
   "source": [
    "tweets = add_bitri([\"j'adore le camembert\", \"roses are red\"])\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bitri(tweets):\n",
    "    \"append bigrams and trigrams to the tweets \"\n",
    "    nb_tweets = len(tweets)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        to_store = ''\n",
    "        for t in extract_tokens(tweet):\n",
    "            to_store = to_store + \" \" + token_to_string(t)\n",
    "        tweets[i] = to_store\n",
    "        if i%1000==0:\n",
    "            print(\"{:.1f}\".format(i/nb_tweets*100), \"%\", end='\\r')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bitri_tweets(previous_name, dest, is_test_data):\n",
    "    startTime= datetime.now()\n",
    "    tweets = import_(previous_name, is_test_data)\n",
    "    tweets = add_bitri(tweets)\n",
    "    export(tweets, dest)    \n",
    "    timeElapsed=datetime.now()-startTime\n",
    "    print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elpased (hh:mm:ss.ms) 0:03:23.988828 % % %% % % % % % % %%% %% % % %% %%% % % % % %\n"
     ]
    }
   ],
   "source": [
    "create_bitri_tweets('preprocessed_tweet_data_full.txt', 'bitri_tweet_ndata_full.txt', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0d8c582c68be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.3 %\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-869c03a2c63b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             if t[1] not in ('TO', \"'\", \"PRP$\")  \"\"\"\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mfinal_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\extract_tokens.py\u001b[0m in \u001b[0;36mextract_tokens\u001b[1;34m(tweet_string)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0munigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\extract_tokens.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0munigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step5b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_step4\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'ous'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m'ive'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[1;34m'ize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m         ])\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    264\u001b[0m                     \u001b[1;31m# Don't try any further rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# add stemmed tokens, bigrams and trigrams\n",
    "\n",
    "final_set = Counter()\n",
    "read_file_name = 'preprocessed_tweet_pos_full.txt'\n",
    "\n",
    "startTime= datetime.now()\n",
    "\n",
    "with open(read_file_name, \"r\") as inputfile:\n",
    "    counter = 0\n",
    "    for line in inputfile:\n",
    "        \"\"\" sentence = extract_tokens(line)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        new_sentence =[]\n",
    "        for t in tagged :\n",
    "            if t[1] not in ('TO', \"'\", \"PRP$\")  \"\"\"\n",
    "        final_set.update(extract_tokens(line))\n",
    "        \n",
    "        if counter%1000==1:\n",
    "            print(\"{:.1f}\".format(counter/file_len(read_file_name)*100), \"%\", end='\\r')\n",
    "            #break\n",
    "        counter+=1\n",
    "        \n",
    "# remove less frequent items\n",
    "threshold = 30\n",
    "print(\"removing items present less than\", threshold, \"times\")\n",
    "final_set = Counter(token for token in final_set.elements() if final_set[token] >= threshold)\n",
    "    \n",
    "timeElapsed=datetime.now()-startTime \n",
    "\n",
    "print(len(final_set))\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<user>', 5149), ('!', 3662), ('i', 3461), ('the', 2582), ('to', 2495), ('a', 2117), ('<url>', 2069), ('you', 2066), ('(', 1868), ('and', 1649), ('it', 1493), ('my', 1454), ('of', 1343), ('me', 1224), ('?', 1211), ('is', 1160), (\"'\", 1097), ('in', 1054), ('for', 1047), ('that', 972), (('!', '!'), 928), ('\"', 882), ('thi', 819), ('be', 763), (',', 760), ('so', 745), ('with', 732), ('-', 718), ('your', 715), ('on', 709), (('<user>', '<user>'), 669), ('have', 651), (')', 630), ('but', 592), (\"i'm\", 590), ('love', 555), ('go', 533), ('day', 531), ('just', 516), ('out', 476), ('get', 464), ('rt', 464), ('like', 456), ('not', 449), ('are', 438), (':', 428), ('frame', 422), ('at', 408), ('all', 406), ('wa', 399), (('<user>', 'i'), 397), ('&', 384), ('/', 383), ('do', 382), ('thank', 381), ('good', 371), ('lol', 366), ('u', 363), ('follow', 360), ('what', 358), (('rt', '<user>'), 358), ('one', 358), ('haha', 356), ('when', 352), ('know', 349), ('up', 346), ('want', 344), ('no', 338), ('now', 322), (\"don't\", 321), ('can', 300), ('time', 296), ('we', 292), ('see', 290), ('will', 287), ('never', 283), ('from', 278), ('he', 277), ('if', 273), ('plea', 272), ('2', 266), ('too', 266), (('!', '!', '!'), 264), ('back', 257), (('it', \"'\"), 253), ('make', 252), ('come', 242), ('got', 240), ('miss', 240), (('in', 'the'), 236), ('think', 232), ('im', 231), ('how', 230), ('need', 228), ('today', 226), ('smile', 226), (('<user>', '<user>', '<user>'), 222), ('as', 221), ('there', 218), ('<3', 217)]\n"
     ]
    }
   ],
   "source": [
    "print(final_set.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to file\n",
    "write_file_name = 'vocab_bitri_preprocessed_pos_full'\n",
    "\n",
    "with open(write_file_name, \"w\") as inputfile:\n",
    "    for token, count in final_set.most_common():\n",
    "        inputfile.write(str(count))\n",
    "        inputfile.write(\" \")\n",
    "        inputfile.write(str(token))\n",
    "        inputfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
