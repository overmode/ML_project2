{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book for the project 2 \n",
    "\n",
    "Kaggle competition link: [Submition]('https://www.kaggle.com/c/epfml17-text/submit')\n",
    "\n",
    "## Pipeline: \n",
    "\n",
    "\n",
    "### Create cooc matrix\n",
    "\n",
    "1. sh build_vocab.sh\n",
    "2. sh cut_vocab.sh\n",
    "3. python3 pickle_vocab.py\n",
    "4. python3 cooc.py\n",
    "\n",
    "Now given the co-occurrence matrix and the vocabulary, it is not hard to train GloVe word embeddings, that is to compute an embedding vector for wach word in the vocabulary. We suggest to implement SGD updates to train the matrix factorization, as in\n",
    "\n",
    "5. python3 glove_template.py\n",
    "\n",
    "Once you tested your system on the small set of 10% of all tweets, we suggest you run on the full datasets pos_train_full.txt, neg_train_full.txt\n",
    "\n",
    "### Building a Text Classifier:\n",
    "\n",
    "1. Construct Features for the Training Texts: Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet).\n",
    "\n",
    "2. Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels indicate if a tweet used to contain a üôÇ or üôÅ smiley.\n",
    "\n",
    "3. Prediction: Predict labels for all tweets in the test set.\n",
    "\n",
    "4. Submission / Evaluation: Submit your predictions to kaggle, and verify the obtained misclassification error score. (You can also use a local separate validation set to get faster feedback on the accuracy of your system). Try to tune your system for best evaluation score.\n",
    "\n",
    "### Extensions:\n",
    "Naturally, there are many ways to improve your solution, both in terms of accuracy and computation speed. More advanced techniques can be found in the recent literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing usefull library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_storing_ts = 'embeddings.npy'\n",
    "file_storing_pos_ts_tweets = 'train_pos.txt'\n",
    "file_storing_neg_ts_tweets = 'train_neg.txt'\n",
    "file_storing_te_tweets = 'test_data.txt'\n",
    "nb_dim = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(file_name='cooc.pkl', destination='embeddings.npy'):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(file_name, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "    \n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "    epochs = 10\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            \n",
    "            f = ((n / nmax)**alpha) if n < nmax else 1\n",
    "            inter_cost = (xs[ix]@(ys[jy]) - np.log(n))\n",
    "            # We compute the gradients for both context and main vector words\n",
    "            grad_main = f * inter_cost * ys[jy]\n",
    "            grad_context = f * inter_cost * xs[ix]\n",
    "    \n",
    "            # Update the vector words\n",
    "            xs[ix] = xs[ix] - (eta * grad_main)\n",
    "            ys[jy] = ys[jy] - (eta * grad_context)\n",
    "            \n",
    "    np.save(destination, xs)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct words_embeddings for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load words for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f :\n",
    "    vocab = pickle.load(f)\n",
    "xs = np.load(file_storing_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.Construct features for training tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = open(file_storing_pos_ts_tweets, 'r').readlines() \n",
    "neg_tweets = open(file_storing_neg_ts_tweets, 'r').readlines()\n",
    "\n",
    "\n",
    "#Construct features for positive tweets\n",
    "pos_features = []\n",
    "\n",
    "for line in pos_tweets:\n",
    "    sum_w = np.zeros(nb_dim)\n",
    "    count = 0\n",
    "    for word in line.split():\n",
    "        local_w = vocab.get(word, -1)\n",
    "        if local_w != -1:\n",
    "            count += 1\n",
    "            sum_w += xs[local_w]\n",
    "     \n",
    "    # If no match between the vocab and the tweet, we set the feature vector to zeros (zero best value ?)\n",
    "    if(count == 0):\n",
    "        count = 1\n",
    "    pos_features.append(sum_w/count)\n",
    "pos_features = np.array(pos_features)    \n",
    "    \n",
    "#Construct features for negative tweets\n",
    "neg_features = []\n",
    "\n",
    "for line in neg_tweets:\n",
    "    sum_w = np.zeros(nb_dim)\n",
    "    count = 0\n",
    "    for word in line.split():\n",
    "        local_w = vocab.get(word, -1)\n",
    "        if local_w != -1:\n",
    "            count += 1\n",
    "            sum_w += xs[local_w]\n",
    "     \n",
    "    # If no match between the vocab and the tweet, we set the feature vector to zeros (zero best value ?)\n",
    "    if(count == 0):\n",
    "        count = 1\n",
    "    neg_features.append(sum_w/count)\n",
    "    \n",
    "neg_features = np.array(neg_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train the linear classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-860726fc3af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(max_iter=1000);\n",
    "X = np.concatenate((pos_features, neg_features))\n",
    "Y = np.concatenate((np.ones(len(pos_features)), np.full(len(neg_features), -1)))\n",
    "clf.fit(X, Y)\n",
    "                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
