{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book for the project 2 \n",
    "\n",
    "Kaggle competition link: [Submition]('https://www.kaggle.com/c/epfml17-text/submit')\n",
    "\n",
    "## Pipeline: \n",
    "\n",
    "\n",
    "### Create cooc matrix\n",
    "\n",
    "1. sh build_vocab.sh\n",
    "2. sh cut_vocab.sh\n",
    "3. python3 pickle_vocab.py\n",
    "4. python3 cooc.py\n",
    "\n",
    "Now given the co-occurrence matrix and the vocabulary, it is not hard to train GloVe word embeddings, that is to compute an embedding vector for wach word in the vocabulary. We suggest to implement SGD updates to train the matrix factorization, as in\n",
    "\n",
    "5. python3 glove_template.py\n",
    "\n",
    "Once you tested your system on the small set of 10% of all tweets, we suggest you run on the full datasets pos_train_full.txt, neg_train_full.txt\n",
    "\n",
    "### Building a Text Classifier:\n",
    "\n",
    "1. Construct Features for the Training Texts: Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet).\n",
    "\n",
    "2. Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels indicate if a tweet used to contain a üôÇ or üôÅ smiley.\n",
    "\n",
    "3. Prediction: Predict labels for all tweets in the test set.\n",
    "\n",
    "4. Submission / Evaluation: Submit your predictions to kaggle, and verify the obtained misclassification error score. (You can also use a local separate validation set to get faster feedback on the accuracy of your system). Try to tune your system for best evaluation score.\n",
    "\n",
    "### Extensions:\n",
    "Naturally, there are many ways to improve your solution, both in terms of accuracy and computation speed. More advanced techniques can be found in the recent literature.\n",
    "\n",
    "- find algorithm for equivalent words and rewrite tweets\n",
    "- parallelize feature construction\n",
    "- consider tuple with GloVe\n",
    "- try feature construction alternatives\n",
    "- improve ratio (Done by stiing lower bound lb on somme)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing usefull library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from split_data import split_data\n",
    "%matplotlib inline\n",
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "from sklearn import linear_model, preprocessing, neural_network\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define filenames and variables\n",
    "\n",
    "#embeddings\n",
    "embeddings_ts =      'embeddings_their_GloVe.npy'\n",
    "embeddings_te =      'embeddings_te.npy'\n",
    "embeddings_ts_full = 'embeddings_ts_full.npy'\n",
    "embeddings_te_full = 'embeddings_te_full.npy'\n",
    "\n",
    "#tweets\n",
    "pos_ts_tweets =      'train_pos.txt'\n",
    "neg_ts_tweets =      'train_neg.txt'\n",
    "pos_ts_full_tweets = 'train_pos_full.txt'\n",
    "neg_ts_full_tweets = 'train_neg_full.txt'\n",
    "te_full_tweets =     'test_data.txt'\n",
    "\n",
    "#vocab\n",
    "file_vocab = 'vocab.pkl'\n",
    "\n",
    "#coocurrence matrices\n",
    "cooc_full = 'ccoc_full.pkl'\n",
    "cooc_partial = 'cooc.pkl'\n",
    "\n",
    "\n",
    "#Features variables\n",
    "#pertinence = see construct_features.py\n",
    "nb_dim = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GloVe(file_name=cooc_partial, destination=embeddings_ts):\n",
    "    #load coocurence matrix\n",
    "    with open(file_name, 'rb') as f:\n",
    "        cooc = pickle.load(f)    \n",
    "    \n",
    "    nmax = 100\n",
    "    embedding_dim = 20\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "    epochs = 3\n",
    "    \n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "   \n",
    "    #Construct vector representations xs for words\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            \n",
    "            f = ((n / nmax)**alpha) if n < nmax else 1\n",
    "            inter_cost = (xs[ix]@(ys[jy]) - np.log(n))\n",
    "            # We compute the gradients for both context and main vector words\n",
    "            grad_main = f * inter_cost * ys[jy]\n",
    "            grad_context = f * inter_cost * xs[ix]\n",
    "    \n",
    "            # Update the vector words\n",
    "            xs[ix] = xs[ix] - (eta * grad_main)\n",
    "            ys[jy] = ys[jy] - (eta * grad_context)\n",
    "            \n",
    "    #Store xs in destination file\n",
    "    np.save(file=destination, arr=xs)\n",
    "\n",
    "\n",
    "def construct_features(tweets, embeddings, weights):\n",
    "    features = []\n",
    "    invalid_features = [];\n",
    "    nb_dim = 20\n",
    "    pertinence = 35\n",
    "\n",
    "    with open('vocab.pkl', 'rb') as f :\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    #Load words from tweet set\n",
    "    xs = np.load(embeddings)\n",
    "    \n",
    "    for indl, line in enumerate(tweets):\n",
    "        #we differentiate tweets containing pertinent words, those in dictionnary 'weights'\n",
    "        sum_w_pertinent = np.zeros(nb_dim)\n",
    "        sum_w_others = np.zeros(nb_dim)\n",
    "\n",
    "        count_pertinent = 0\n",
    "        count_other = 0\n",
    "\n",
    "        for word in line.split():\n",
    "            local_w = vocab.get(word, -1)\n",
    "            if local_w != -1:\n",
    "                weight = weights.get(word, -1)\n",
    "                if weight != -1:\n",
    "\n",
    "                    #If the word is pertinent, we add its word representation to others pertinents word's representation\n",
    "                    count_pertinent += weight*pertinence\n",
    "                    sum_w_pertinent += xs[local_w] * (weight*pertinence)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #If the word is not pertinent, we add its representation to non-pertinent words representations\n",
    "                    count_other += 1\n",
    "                    sum_w_others += xs[local_w]\n",
    "\n",
    "            # If we found pertinent words, we only use them\n",
    "        if(count_pertinent != 0):\n",
    "            features.append(sum_w_pertinent/count_pertinent)\n",
    "\n",
    "            #if we found only non-pertinent words, we use them anyway\n",
    "        elif count_other!= 0:\n",
    "            features.append(sum_w_others/count_other)\n",
    "\n",
    "            #if we did not find words that have representation, we do not try to create features and signal their indices\n",
    "        else:\n",
    "            invalid_features.append(indl)\n",
    "\n",
    "    invalid_features = np.array(invalid_features)\n",
    "    features = np.array(features)\n",
    "\n",
    "    return features, invalid_features\n",
    "\n",
    "\n",
    "def policy_unpredictable():\n",
    "    return np.random.choice((1,-1))\n",
    "\n",
    "def assemble(valid, indices):\n",
    "    cur = 0\n",
    "    nb_inserted = 0\n",
    "    result = [0]*(len(valid) + len(indices))\n",
    "    for i in range((len(valid) + len(indices))):\n",
    "        if(cur in indices):\n",
    "            result[cur] = policy_unpredictable()\n",
    "            cur = cur + 1\n",
    "        else:\n",
    "            result[cur] = valid[nb_inserted]\n",
    "            cur = cur + 1\n",
    "            nb_inserted = nb_inserted + 1\n",
    "    return np.array(result)\n",
    "\n",
    "def accuracy(prediction, actual_emotions):\n",
    "    return (1 - (np.sum(np.abs(actual_emotions-prediction))/(2*len(actual_emotions)))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct words_embeddings for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GloVe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_predict(pertinent_lb):    \n",
    "    \n",
    "    \n",
    "    #define relevant_vocab file to use\n",
    "    relevant_vocab = 'relevant_vocab_lb='+str(pertinent_lb)+'.txt'\n",
    "    \n",
    "    #load ratios into a dictionary\n",
    "    new_vocab = pd.read_csv(filepath_or_buffer=relevant_vocab, sep=\" \")\n",
    "    weights = new_vocab[[\"word\", \"ratio\"]]\n",
    "    weights = dict(zip(weights.word, weights.ratio))\n",
    "    \n",
    "    #Split positive tweets into training and testing sets\n",
    "    pos_tweets = np.array(open(pos_ts_tweets, 'r').readlines()) \n",
    "    labels_pos = np.ones(len(pos_tweets))\n",
    "    pos_tr, labels_pos_tr, pos_te, labels_pos_te = split_data(pos_tweets, labels_pos, 0.996)\n",
    "    \n",
    "   \n",
    "\n",
    "    #Split negative tweets into training and testing sets\n",
    "    neg_tweets = np.array(open(neg_ts_tweets, 'r').readlines())\n",
    "    labels_neg = np.full(len(neg_tweets), -1)\n",
    "    neg_tr, label_neg_tr, neg_te, labels_neg_te = split_data(neg_tweets, labels_neg, 0.996)\n",
    "            \n",
    "    #Find features for each tweet with at least one word within vocab, get indices of unpredictable tweets\n",
    "    #for pos tweets\n",
    "    pos_tr_feat, invalid_pos_tr = construct_features(pos_tr, embeddings_ts, weights)\n",
    "    pos_te_feat, invalid_pos_te = construct_features(pos_te, embeddings_ts, weights)\n",
    "\n",
    "    #for neg tweets\n",
    "    neg_tr_feat, invalid_neg_tr = construct_features(neg_tr, embeddings_ts, weights)\n",
    "    neg_te_feat, invalid_neg_te = construct_features(neg_te, embeddings_ts, weights)\n",
    "    \n",
    "    print(len(pos_tr_feat), len(invalid_pos_tr))\n",
    "    print(len(pos_te_feat), len(invalid_pos_te))\n",
    "    print(len(neg_tr_feat), len(invalid_neg_tr))\n",
    "    print(len(neg_te_feat), len(invalid_neg_te))\n",
    "    \n",
    "    #Initialize classifier and scaler\n",
    "    neural = neural_network.MLPClassifier()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    #fit classifier on predictable tweets\n",
    "    X = np.concatenate((pos_tr_feat, neg_tr_feat))\n",
    "    Y = np.concatenate((np.ones(len(pos_tr_feat)), np.full(len(neg_tr_feat), -1)))\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "    neural = neural.fit(X, Y)\n",
    "\n",
    "    #scale data that should be predicted\n",
    "    pos_te_feat_scaled = pos_te_feat\n",
    "    pos_te_feat_scaled = scaler.fit_transform(pos_te_feat_scaled, np.ones(len(pos_te_feat))) \n",
    "\n",
    "    neg_te_feat_scaled = neg_te_feat\n",
    "    neg_te_feat_scaled = scaler.fit_transform(neg_te_feat_scaled, np.ones(len(neg_te_feat))) \n",
    "\n",
    "    #predict predictable tweets\n",
    "    pos_prediction = neural.predict(pos_te_feat_scaled)\n",
    "    neg_prediction = neural.predict(neg_te_feat_scaled)\n",
    "\n",
    "    #merge with unpredictable tweets predictions\n",
    "    pos_labels = assemble(pos_prediction, invalid_pos_te)\n",
    "    neg_labels = assemble(neg_prediction, invalid_neg_te)\n",
    "\n",
    "    #merge all predictions\n",
    "    labels = np.concatenate((pos_labels, neg_labels))\n",
    "    true_labels = np.concatenate((np.ones(len(pos_labels)), np.full(len(neg_labels), -1)))\n",
    "\n",
    "    return labels, true_labels    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit():\n",
    "    #Load words from tweet set\n",
    "    #xs = np.load(embeddings_ts_full)\n",
    "    \n",
    "    #define relevant_vocab file to use\n",
    "    relevant_vocab = 'relevant_vocab_full_lb=5000.txt'\n",
    "    \n",
    "    #load ratios into a dictionary\n",
    "    new_vocab = pd.read_csv(filepath_or_buffer=relevant_vocab, sep=\" \")\n",
    "    weights = new_vocab[[\"word\", \"ratio\"]]\n",
    "    weights = dict(zip(weights.word, weights.ratio))\n",
    "    \n",
    "    print(len(new_vocab))\n",
    "    \n",
    "    #Find features for each tweet with at least one word within vocab, get indices of unpredictable tweets\n",
    "    #for training tweets\n",
    "    pos_ts_full_feat, invalid_pos_ts_full = construct_features(pos_ts_full_tweets, 'embeddings_full_epoch_10.npy', weights)\n",
    "    neg_ts_full_feat, invalid_neg_ts_full = construct_features(neg_ts_full_tweets, 'embeddings_full_epoch_10.npy', weights)\n",
    "    \n",
    "    print(len(pos_ts_full_feat), len(invalid_pos_ts_full) )\n",
    "    print(len(neg_ts_full_feat), len(invalid_neg_ts_full) )\n",
    "    \n",
    "    #for test tweets\n",
    "    te_full_feat, invalid_te_full = construct_features(te_full_tweets, 'embeddings_full_epoch_10.npy', weights)\n",
    "    print(len(te_full_feat), len(invalid_te_full) )\n",
    "    \n",
    "     #Initialize classifier and scaler\n",
    "    neural = neural_network.MLPClassifier()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    #fit classifier on predictable tweets\n",
    "    X = np.concatenate((pos_ts_full_feat, neg_ts_full_feat))\n",
    "    Y = np.concatenate((np.ones(len(pos_ts_full_feat)), np.full(len(neg_ts_full_feat), -1)))\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "    neural = neural.fit(X, Y)\n",
    "\n",
    "    #scale data that should be predicted\n",
    "    te_full_feat_scaled = te_full_feat\n",
    "    te_full_feat_scaled = scaler.fit_transform(te_full_feat_scaled, np.ones(len(te_full_feat))) \n",
    "\n",
    "\n",
    "    #predict predictable tweets\n",
    "    te_prediction = neural.predict(te_full_feat_scaled)\n",
    "\n",
    "    #merge with unpredictable tweets predictions\n",
    "    labels = assemble(te_prediction, invalid_te_full)\n",
    "   \n",
    "    return labels\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lb in [] :\n",
    "    label_nn, true_labels_nn = build_and_predict(lb)\n",
    "    print('accuracy ='+str(accuracy(label_nn, true_labels_nn))+' for lb = ' + str(lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedict_te = submit()\n",
    "with open('submission.csv', 'w',) as f2:\n",
    "    fields = ('Id', 'Prediction')\n",
    "    wr = csv.DictWriter(f2, fieldnames=fields, lineterminator = '\\n')\n",
    "    wr.writeheader()\n",
    "    \n",
    "    for id_tweet, prediction in enumerate(predict_te):\n",
    "        wr.writerow({'Id':id_tweet+1, 'Prediction': (int)(prediction)})\n",
    "        print(id_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
