{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book for the project 2 \n",
    "\n",
    "Kaggle competition link: [Submition]('https://www.kaggle.com/c/epfml17-text/submit')\n",
    "\n",
    "## Pipeline: \n",
    "\n",
    "\n",
    "### Create cooc matrix\n",
    "\n",
    "1. sh build_vocab.sh\n",
    "2. sh cut_vocab.sh\n",
    "3. python3 pickle_vocab.py\n",
    "4. python3 cooc.py\n",
    "\n",
    "Now given the co-occurrence matrix and the vocabulary, it is not hard to train GloVe word embeddings, that is to compute an embedding vector for wach word in the vocabulary. We suggest to implement SGD updates to train the matrix factorization, as in\n",
    "\n",
    "5. python3 glove_template.py\n",
    "\n",
    "Once you tested your system on the small set of 10% of all tweets, we suggest you run on the full datasets pos_train_full.txt, neg_train_full.txt\n",
    "\n",
    "### Building a Text Classifier:\n",
    "\n",
    "1. Construct Features for the Training Texts: Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet).\n",
    "\n",
    "2. Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels indicate if a tweet used to contain a üôÇ or üôÅ smiley.\n",
    "\n",
    "3. Prediction: Predict labels for all tweets in the test set.\n",
    "\n",
    "4. Submission / Evaluation: Submit your predictions to kaggle, and verify the obtained misclassification error score. (You can also use a local separate validation set to get faster feedback on the accuracy of your system). Try to tune your system for best evaluation score.\n",
    "\n",
    "### Extensions:\n",
    "Naturally, there are many ways to improve your solution, both in terms of accuracy and computation speed. More advanced techniques can be found in the recent literature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing usefull library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from split_data import split_data\n",
    "%matplotlib inline\n",
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "from sklearn import linear_model, preprocessing, neural_network\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Define filenames and variables\n",
    "\n",
    "#embeddings\n",
    "embeddings_ts =      'embeddings_their_GloVe.npy'\n",
    "embeddings_te =      'embeddings_te.npy'\n",
    "embeddings_ts_full = 'embeddings_ts_full.npy'\n",
    "embeddings_te_full = 'embeddings_te_full.npy'\n",
    "\n",
    "#tweets\n",
    "pos_ts_tweets =      'train_pos.txt'\n",
    "neg_ts_tweets =      'train_neg.txt'\n",
    "te_tweets =          'test_data.txt'\n",
    "pos_ts_full_tweets = 'train_pos_full.txt'\n",
    "neg_ts_full_tweets = 'train_neg_full.txt'\n",
    "te_full_tweets =     'test_data_full'\n",
    "\n",
    "#relevant vocab\n",
    "relevant_vocab = 'new_vocab'\n",
    "file_vocab =          'vocab.pkl'\n",
    "\n",
    "#coocurrence matrices\n",
    "cooc_full = 'ccoc_full.pkl'\n",
    "cooc_partial = 'cooc.pkl'\n",
    "\n",
    "\n",
    "#Features variables\n",
    "pertinence = 20\n",
    "nb_dim = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GloVe(file_name=cooc_partial, destination=embeddings_ts):\n",
    "    #load coocurence matrix\n",
    "    with open(file_name, 'rb') as f:\n",
    "        cooc = pickle.load(f)    \n",
    "    \n",
    "    nmax = 100\n",
    "    embedding_dim = 20\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "    epochs = 3\n",
    "    \n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "   \n",
    "    #Construct vector representations xs for words\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            \n",
    "            f = ((n / nmax)**alpha) if n < nmax else 1\n",
    "            inter_cost = (xs[ix]@(ys[jy]) - np.log(n))\n",
    "            # We compute the gradients for both context and main vector words\n",
    "            grad_main = f * inter_cost * ys[jy]\n",
    "            grad_context = f * inter_cost * xs[ix]\n",
    "    \n",
    "            # Update the vector words\n",
    "            xs[ix] = xs[ix] - (eta * grad_main)\n",
    "            ys[jy] = ys[jy] - (eta * grad_context)\n",
    "            \n",
    "    #Store xs in destination file\n",
    "    np.save(destination, xs)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct words_embeddings for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n"
     ]
    }
   ],
   "source": [
    "GloVe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load words from tweet set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(file_vocab, 'rb') as f :\n",
    "    vocab = pickle.load(f)\n",
    "xs = np.load(embeddings_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.Construct features for all tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vocab = pd.read_csv(filepath_or_buffer=relevant_vocab, sep=\" \")\n",
    "weights = new_vocab[[\"word\", \"ratio\"]]\n",
    "weights = dict(zip(weights.word, weights.ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_features(tweets):\n",
    "    features = []\n",
    "    invalid_features = [];\n",
    "\n",
    "    for indl, line in enumerate(tweets):\n",
    "        #we differentiate tweets containing pertinent words, those in dictionnary 'weights'\n",
    "        sum_w_pertinent = np.zeros(nb_dim)\n",
    "        sum_w_others = np.zeros(nb_dim)\n",
    "        \n",
    "        count_pertinent = 0\n",
    "        count_other = 0\n",
    "        \n",
    "        for word in line.split():\n",
    "            local_w = vocab.get(word, -1)\n",
    "            if local_w != -1:\n",
    "                weight = weights.get(word, -1)\n",
    "                if weight != -1:                \n",
    "                    \n",
    "                    #If the word is pertinent, we add its word representation to others pertinents word's representation\n",
    "                    count_pertinent += 1 + (weight*pertinence)\n",
    "                    sum_w_pertinent += xs[local_w] * (1 + (weight*pertinence))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    #If the word is not pertinent, we add its representation to non-pertinent words representations\n",
    "                    count_other += 1 \n",
    "                    sum_w_others += xs[local_w] \n",
    "     \n",
    "            # If we found pertinent words, we only use them\n",
    "        if(count_pertinent != 0):            \n",
    "            features.append(sum_w_pertinent/count_pertinent)\n",
    "            \n",
    "            #if we found only non-pertinent words, we use them anyway\n",
    "        elif count_other!= 0:\n",
    "            features.append(sum_w_others/count_other)\n",
    "            \n",
    "            #if we did not find words that have representation, we do not try to create features and signal their indices\n",
    "        else:\n",
    "            invalid_features.append(indl)\n",
    "    \n",
    "    invalid_features = np.array(invalid_features)\n",
    "    features = np.array(features)\n",
    "    \n",
    "    return features, invalid_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_unpredictable():\n",
    "    return np.random.choice((1,-1))\n",
    "\n",
    "def assemble(valid, indices):\n",
    "    cur = 0\n",
    "    nb_inserted = 0\n",
    "    result = [0]*(len(valid) + len(indices))\n",
    "    for i in range((len(valid) + len(indices))):\n",
    "        if(cur in indices):\n",
    "            result[cur] = policy_unpredictable()\n",
    "            cur = cur + 1\n",
    "        else:\n",
    "            result[cur] = valid[nb_inserted]\n",
    "            cur = cur + 1\n",
    "            nb_inserted = nb_inserted + 1\n",
    "    return np.array(result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape =  (100000,)\n",
      "y shape =  (100000,)\n",
      "x shape =  (100000,)\n",
      "y shape =  (100000,)\n",
      "(99595, 20) 5\n",
      "(400, 20) 0\n",
      "(99596, 20) [18231 56205 66615 91753]\n",
      "(400, 20) []\n"
     ]
    }
   ],
   "source": [
    "#Split positive tweets into training and testing sets\n",
    "pos_tweets = np.array(open(pos_ts_tweets, 'r').readlines()) \n",
    "labels_pos = np.ones(len(pos_tweets))\n",
    "pos_tr, labels_pos_tr, pos_te, labels_pos_te = split_data(pos_tweets, labels_pos, 0.996)\n",
    "\n",
    "#Split negative tweets into training and testing sets\n",
    "neg_tweets = np.array(open(neg_ts_tweets, 'r').readlines())\n",
    "labels_neg = np.full(len(neg_tweets), -1)\n",
    "neg_tr, label_neg_tr, neg_te, labels_neg_te = split_data(neg_tweets, labels_neg, 0.996)\n",
    "\n",
    "#Find features for each tweet with at least one word within vocab, get indices of unpredictable tweets\n",
    "#for pos tweets\n",
    "pos_tr_feat, invalid_pos_tr = construct_features(pos_tr)\n",
    "pos_te_feat, invalid_pos_te = construct_features(pos_te)\n",
    "print(pos_tr_feat.shape, len(invalid_pos_tr)) \n",
    "print(pos_te_feat.shape, len(invalid_pos_te)) \n",
    "\n",
    "#for neg tweets\n",
    "neg_tr_feat, invalid_neg_tr = construct_features(neg_tr)\n",
    "neg_te_feat, invalid_neg_te = construct_features(neg_te)\n",
    "print(neg_tr_feat.shape, invalid_neg_tr) \n",
    "print(neg_te_feat.shape, invalid_neg_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train the linear classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize classifier and scaler\n",
    "clf = linear_model.SGDClassifier(n_iter=80000, warm_start=True);\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#fit classifier on predictable tweets\n",
    "X = np.concatenate((pos_tr_feat, neg_tr_feat))\n",
    "Y = np.concatenate((np.ones(len(pos_tr_feat)), np.full(len(neg_tr_feat), -1)))\n",
    "X = scaler.fit_transform(X, Y)\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "#scale data that should be predicted\n",
    "pos_te_feat_scaled = pos_te_feat\n",
    "pos_te_feat_scaled = scaler.fit_transform(pos_te_feat_scaled, np.ones(len(pos_te_feat))) \n",
    "\n",
    "neg_te_feat_scaled = neg_te_feat\n",
    "neg_te_feat_scaled = scaler.fit_transform(neg_te_feat_scaled, np.ones(len(neg_te_feat))) \n",
    "\n",
    "#predict predictable tweets\n",
    "pos_prediction = clf.predict(pos_te_feat_scaled)\n",
    "neg_prediction = clf.predict(neg_te_feat_scaled)\n",
    "\n",
    "#merge with unpredictable tweets predictions\n",
    "pos_labels = assemble(pos_prediction, invalid_pos_te)\n",
    "neg_labels = assemble(neg_prediction, invalid_neg_te)\n",
    "\n",
    "#merge all predictions\n",
    "labels = np.concatenate((pos_labels, neg_labels))\n",
    "true_labels = np.concatenate((np.ones(len(pos_labels)), np.full(len(neg_labels), -1)))\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize classifier and scaler\n",
    "neural = neural_network.MLPClassifier()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#fit classifier on predictable tweets\n",
    "X = np.concatenate((pos_tr_feat, neg_tr_feat))\n",
    "Y = np.concatenate((np.ones(len(pos_tr_feat)), np.full(len(neg_tr_feat), -1)))\n",
    "X = scaler.fit_transform(X, Y)\n",
    "neural = neural.fit(X, Y)\n",
    "\n",
    "#scale data that should be predicted\n",
    "pos_te_feat_scaled = pos_te_feat\n",
    "pos_te_feat_scaled = scaler.fit_transform(pos_te_feat_scaled, np.ones(len(pos_te_feat))) \n",
    "\n",
    "neg_te_feat_scaled = neg_te_feat\n",
    "neg_te_feat_scaled = scaler.fit_transform(neg_te_feat_scaled, np.ones(len(neg_te_feat))) \n",
    "\n",
    "#predict predictable tweets\n",
    "pos_prediction = neural.predict(pos_te_feat_scaled)\n",
    "neg_prediction = neural.predict(neg_te_feat_scaled)\n",
    "\n",
    "#merge with unpredictable tweets predictions\n",
    "pos_labels = assemble(pos_prediction, invalid_pos_te)\n",
    "neg_labels = assemble(neg_prediction, invalid_neg_te)\n",
    "\n",
    "#merge all predictions\n",
    "labels = np.concatenate((pos_labels, neg_labels))\n",
    "true_labels = np.concatenate((np.ones(len(pos_labels)), np.full(len(neg_labels), -1)))\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.125"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(prediction, actual_emotions):\n",
    "    return (1 - (np.sum(np.abs(actual_emotions-prediction))/(2*len(actual_emotions)))) * 100\n",
    "\n",
    "accuracy(labels, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_prediction[pos_prediction==1])\n",
    "len(neg_prediction[neg_prediction==-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_prediction)\n",
    "len(neg_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
