{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data one is given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple words do we have, are there some duplicates, is there any odds with the tweets? How are they classified (we are given two datasets, one for the positive tweets one for negatives) i.e. what is a typical word that occurs when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may contain words that are not usefull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than three following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Helper functions and files\n",
    "\n",
    "IOTweets contains the functions :\n",
    "- build_df\n",
    "- import_\n",
    "- export\n",
    "\n",
    "ProcessTweets contains the functions :\n",
    "- merging\n",
    "- filter_single_rep\n",
    "- powerful_words\n",
    "- sem_by_repr\n",
    "- sem_by_repr2\n",
    "- no_dot\n",
    "- find_repetition\n",
    "- no_s\n",
    "- stem_tweet\n",
    "- set_min_diff\n",
    "- contains\n",
    "- process_tweets_1\n",
    "- process_tweets_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An insight into the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import csv\n",
    "#%pylab inline # depreceated, use individual imports instead\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos = \"./vocab_cut_pos.txt\"\n",
    "path_neg = \"./vocab_cut_neg.txt\"\n",
    "\n",
    "path_pos_full = \"./vocab_cut_pos_full.txt\"\n",
    "path_neg_full = \"./vocab_cut_neg_full.txt\"\n",
    "path_test_data = \"./vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"./train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"./train_neg_full.txt\"\n",
    "path_tweets_test_full = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab Dataframe (DF) for full positive and negative tweets\n",
    "pos_full = build_df(path_pos_full)\n",
    "neg_full = build_df(path_neg_full)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_data = build_df(path_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Study of word pertinence \n",
    "The idea is that the greater a word ratio i, the more pertinent it will be to cosider it when trying to label a tweet that contains it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge the pos and neg vocabs and study penrtinence of words via ratio\n",
    "merged_full = merging(neg_full, pos_full)\n",
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>\n",
    "    \n",
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Study of names with same beginning \n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# are words (filtered by length) whose beginning match another word close to this word?\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "\n",
    "#We visualize which word would be mapped on which one \n",
    "filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expected syntaxic divergences\n",
    "As expected, words like \"haha\" can be written in many ways (extended length, typo..). We know that they mean the same,it is therefore interesting to spot them in order to later stem them to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words beginning with \"haha\" or \"ahah\" are sure to be \"instances\" of haha\n",
    "word_haha = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "\n",
    "\n",
    "print(\"Occurence of the words that can be remplaced by haha = \"+ str(word_haha.occurence.sum() - word_haha[word_haha.word = \"haha\"].occurence))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)\n",
    "list(word_haha.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Study duplicates\n",
    "We noticed that several tweets were duplicated in test, pos and neg tweets. We study ho many they are, and which tweets (if any) are shared between the test set and the pos/neg set.\n",
    "We noticed after running that there are no tweets shared between all tweet sets, thus we can label duplicate of the test dataset by directly looking for corresponding duplicates in pos/neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pos_data = []\n",
    "common_neg_data = []\n",
    "\n",
    "pos = sorted(list(set(pos)))\n",
    "neg = sorted(list(set(neg)))\n",
    "data = sorted(list(set(data)))\n",
    "len_data = len(data)\n",
    "for i, data_tweet in enumerate(data) :\n",
    "    for pos_tweet in pos :\n",
    "        if data_tweet == pos_tweet : \n",
    "            common_pos_data.append(data_tweet)\n",
    "            print(\"duplicate pos-data\")\n",
    "            print(\"\\ttweet :\" + str(pos_tweet))  \n",
    "            break\n",
    "    for neg_tweet in neg:\n",
    "        if data_tweet == neg_tweet : \n",
    "            common_neg_data.append(data_tweet)\n",
    "            print(\"duplicate neg-data\")\n",
    "            print(\"\\ttweet :\" + str(pos_tweet))\n",
    "            break\n",
    "    print(\"{:.1f}\".format(i/len_data*100), \"%\", end='\\r')\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occurences(tweet, tweets):\n",
    "    count = 0\n",
    "    for t in tweets:\n",
    "        if t == tweet:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 data pre-processing\n",
    "In this part we start from raw tweets and preprocess them to achieve uniformization of the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Standardization\n",
    "We start by removing repetitions of more than 3 letters, and replace dots by spaces\n",
    "ex : \n",
    "\n",
    "1) `no_dot` : \"Funny.I\" becomes \"Funny I\" \n",
    "\n",
    "2) `find_repetitions` : \"I looooove\" becomes \"I love\"\n",
    "\n",
    "3) `no_s` : \"dummies\" becomes \"dummy\" and \"carresses\" becomes \"carresse\"\n",
    "            \n",
    "\n",
    "This is done by the function `standardize_tweets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = standardize_tweets(pos)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = standardize_tweets(neg) \n",
    "print(\"Process data test\")\n",
    "processed_tweets_data_full = standardize_tweets(data) \n",
    "\n",
    "\n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"processed_tweet_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"processed_tweet_pos_full\")\n",
    "export(processed_tweets_data_full, \"processed_tweet_data_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the vocabs\n",
    "Using shell commands `vocab_cut`, we build vocabs for positive, negative and test tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build and Merge the dataframes \n",
    "We build dataframes containing several informations for each type of tweets, and we combine them all in order to facilitate the stemming we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos_full = \"vocab_pos_full_to_stem.txt\"\n",
    "path_neg_full = \"vocab_neg_full_to_stem.txt\"\n",
    "path_test_data = \"vocab_data_full_to_stem.txt\"\n",
    "\n",
    "    \n",
    "pos_full =  build_df(path_pos_full)\n",
    "neg_full =  build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build semantic equivalences\n",
    "We define which word is equivalent to each word (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building semantic\")\n",
    "#Build equivalence list\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "print(\"Building haha semantic\")\n",
    "#build \"haha\" equivalences\n",
    "word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    \n",
    "word_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\n",
    "word_haha_list.remove(\"haha\")\n",
    "word_haha_list.insert(0, \"haha\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"filter semantic\")\n",
    "#append the two semantic lists and filter them\n",
    "same_begin.append(list(word_haha_list))\n",
    "semantic = filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Map the equivalent words, stem them all thanks to nltk and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d'un tableau de tweet   \n",
    "print(\"Process data test\")\n",
    "processed_tweets_data_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"preprocessed_tweet_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"preprocessed_tweet_pos_full\")\n",
    "export(processed_tweets_data_full, \"preprocessed_tweet_data_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Create bigrams and trigrams\n",
    "We consider also combinations of two and three following words And create a new file for each type of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_bitri_tweets('preprocessed_tweet_neg_full.txt', 'bitri_tweet_data_full.txt', False)\n",
    "create_bitri_tweets('preprocessed_tweet_pos_full.txt', 'bitri_tweet_data_full.txt', False)\n",
    "create_bitri_tweets('preprocessed_tweet_data_full.txt', 'bitri_tweet_data_full.txt', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the vocabs (with occurences) to build the dataframes\n",
    "This is done by using shell commands, as presented before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Find characteristic words\n",
    "We set a list of words that appear only in neg/pos at least min_diff times such that if we see such a word in a tweet we assume it is a neg/pos tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tweets to be labelled\n",
    "import_without_id('bitri_tweet_data_full.txt')\n",
    "\n",
    "#Build the new dataframes\n",
    "path_pos_full = \"vocab_pos_full_bitri.txt\"\n",
    "path_neg_full = \"vocab_neg_full_bitri.txt\"\n",
    "path_test_data = \"vocab_data_bitri.txt\"\n",
    "\n",
    "\n",
    "pos_full = build_df(path_pos_full)\n",
    "neg_full = build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "\n",
    "\n",
    "#Search minimal difference so that two opposite characteristic words cannot be seen together in data\n",
    "min_diff = set_min_diff(tweets, merged_full)\n",
    "\n",
    "#export characteristic words\n",
    "characteristic_words = characteristic_words(merged_full, min_diff)\n",
    "characteristic_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes tweets to be labelled, and a merged instance of pos and neg vocabs\n",
    "def set_min_diff(data_tweets, merged_full):\n",
    "    ratio_one = merged_full[(merged_full.ratio == 1)]\n",
    "    differences = (list(ratio_one.difference)).sort(reverse=True)\n",
    "    for d in differences:\n",
    "        words_to_check = list((ratio_one[(ratio_one.ratio == 1) & (ratio_one.difference >= d)]).word)\n",
    "        if not contains(data_tweets, words_to_check):\n",
    "            return d\n",
    "        print(str(d) + \"not sucessful\")\n",
    "    return 0\n",
    "            \n",
    "        \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#return true iff at least one of the tweets contains at least one of the words\n",
    "#tweets is an array of strings\n",
    "#words is a list of words\n",
    "def contains(tweets, words):\n",
    "    for t in tweets:\n",
    "        for w1 in t.split():\n",
    "            for w2 in words:\n",
    "                if w1 == w2:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def drop_duplicates(tweets):\n",
    "    tweets = list(set(tweets))\n",
    "    return tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n",
      "Process data pos\n",
      "Process data neg\n",
      "Process data test\n",
      "build vocab data pos\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:18.789939\n",
      "build vocab data neg\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:21.127750\n",
      "build vocab test data\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:02.082462\n",
      "export data\n"
     ]
    }
   ],
   "source": [
    "process_data_no_stem(\"train_pos.txt\", \"train_neg.txt\", \"test_data.txt\", False, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(filepath_or_buffer = filepath, encoding=\"utf-8\", quoting=csv.QUOTE_NONE,  header=None, names=[\"word\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 2: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14858)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 2: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ed482589d73c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-523f98e97441>\u001b[0m in \u001b[0;36mstemming\u001b[1;34m(is_full)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m#import vocabs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mpos_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocessed_vocab_neg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mneg_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocessed_vocab_pos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"preprocessed_vocab_test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\IOTweets.py\u001b[0m in \u001b[0;36mbuild_df\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"occurence\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1717\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1719\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11138)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas\\_libs\\parsers.c:12175)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas\\_libs\\parsers.c:14136)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14972)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 2: invalid start byte"
     ]
    }
   ],
   "source": [
    "stemming(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Recapitulative Functions\n",
    "These functions do the work presented above, we created them for clarity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data_no_stem(path_pos, path_neg, path_test, is_full, cut_threshold):\n",
    "    ########----------Build DF----------------------------------------\n",
    "    #### -------------- Partial tweets -------------------------------\n",
    "   \n",
    "    print(\"Import Data\")\n",
    "    #import tweets\n",
    "    tweets_test = import_without_id(path_test)     \n",
    "    tweets_pos =  import_(path_pos)\n",
    "    tweets_neg =  import_(path_neg)\n",
    "    \n",
    "    #remove duplicates\n",
    "    tweets_pos = drop_duplicates(tweets_pos)\n",
    "    tweets_neg = drop_duplicates(tweets_neg)\n",
    "    \n",
    "    #process tweets\n",
    "    print(\"Process data pos\")\n",
    "    preprocessed_pos = standardize_tweets( tweets_pos)\n",
    "    print(\"Process data neg\")\n",
    "    preprocessed_neg = standardize_tweets( tweets_neg) #Sous form d'un tableau de tweet   \n",
    "    print(\"Process data test\")\n",
    "    preprocessed_test = standardize_tweets( tweets_test) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    print(\"build vocab data pos\")\n",
    "    vocab_pos = build_vocab_counter(preprocessed_pos, cut_threshold)\n",
    "    print(\"build vocab data neg\")\n",
    "    vocab_neg = build_vocab_counter(preprocessed_neg, cut_threshold)\n",
    "    print(\"build vocab test data\")\n",
    "    vocab_test = build_vocab_counter(preprocessed_test, cut_threshold)\n",
    "    \n",
    "    \n",
    "    print(\"export data\")\n",
    "    #export tweets\n",
    "    if(is_full):\n",
    "        \n",
    "        #export tweets\n",
    "        export(preprocessed_pos,  \"preprocessed_pos_full\")\n",
    "        export(preprocessed_neg,  \"preprocessed_neg_full\")\n",
    "        export(preprocessed_test, \"preprocessed_test_full\")\n",
    "        \n",
    "        #export vocabs\n",
    "        write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos_full\")\n",
    "        write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg_full\")\n",
    "        write_vocab_to_file(vocab_test, \"preprocessed_vocab_test_full\")\n",
    "        \n",
    "        \n",
    "    else :\n",
    "        #export tweets\n",
    "        export(preprocessed_pos,  \"preprocessed_pos\")\n",
    "        export(preprocessed_neg,  \"preprocessed_neg\")\n",
    "        export(preprocessed_test, \"preprocessed_test\")\n",
    "        \n",
    "        #export vocabs\n",
    "        write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos\")\n",
    "        write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg\")\n",
    "        write_vocab_to_file(vocab_test, \"preprocessed_vocab_test\")\n",
    "    \n",
    "\n",
    "def stemming(is_full):\n",
    "    \n",
    "    print(\"Import Data\")\n",
    "    \n",
    "    if(is_full):\n",
    "        #import tweets\n",
    "        preprocessed_neg = import_(\"preprocessed_neg_full\")\n",
    "        preprocessed_pos = import_(\"preprocessed_pos_full\")    \n",
    "        preprocessed_test = import_(\"preprocessed_test_full\")\n",
    "        \n",
    "        #import vocabs\n",
    "        pos_df = build_df(\"preprocessed_vocab_neg_full\")\n",
    "        neg_df = build_df(\"preprocessed_vocab_pos_full\")\n",
    "        test_df = build_df(\"preprocessed_vocab_test_full\")\n",
    "   \n",
    "    else :\n",
    "        #import tweets\n",
    "        preprocessed_neg = import_(\"preprocessed_neg\")\n",
    "        preprocessed_pos = import_(\"preprocessed_pos\")    \n",
    "        preprocessed_test = import_(\"preprocessed_test\")\n",
    "        \n",
    "        #import vocabs\n",
    "        pos_df = build_df(\"preprocessed_vocab_neg\")\n",
    "        neg_df = build_df(\"preprocessed_vocab_pos\")\n",
    "        test_df = build_df(\"preprocessed_vocab_test\")\n",
    "   \n",
    "       \n",
    "    \n",
    "    #Merge Pos and neg dataframes    \n",
    "    merged = merging(neg_df, pos_df, True)\n",
    "    merged = merging(merged, test_df, True)\n",
    "    \n",
    "    \n",
    "    print(\"Building semantic\")\n",
    "    \n",
    "    #Build equivalence list\n",
    "    same_begin = list(merged[merged[\"len\"]==8][\"word\"])\n",
    "    same_begin = [list(merged.loc[(merged.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "    print(\"Building haha semantic\")\n",
    "    \n",
    "    #build \"haha\" equivalences    \n",
    "    word_haha_list = merged.loc[merged.word.str.startswith(\"haha\") | merged.word.str.startswith(\"ahah\")]\n",
    "                                \n",
    "    word_haha_list.remove(\"haha\")\n",
    "    word_haha_list.insert(0, \"haha\")\n",
    "    same_begin.append(list(word_haha_list))\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"filter semantic\")\n",
    "    \n",
    "    #fiter roots alone\n",
    "    semantic = filter_single_rep(same_begin)\n",
    "    \n",
    "    \n",
    "   #process tweets\n",
    "    print(\"Process data pos\")\n",
    "    stemmed_pos = stem_tweets( preprocessed_pos, semantic)\n",
    "    print(\"Process data neg\")\n",
    "    stemmed_neg = stem_tweets( preprocessed_neg, semantic) #Sous form d'un tableau de tweet   \n",
    "    print(\"Process data test\")\n",
    "    stemmed_test = stem_tweets( preprocessed_test, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"export data\")\n",
    "    \n",
    "    #export data    \n",
    "    if(is_full):\n",
    "        #export tweets\n",
    "        export(stemmed_neg,  \"stemmed_neg_full\")\n",
    "        export(stemmed_pos,  \"stemmed_pos_full\")\n",
    "        export(stemmed_test, \"stemmed_test_full\") \n",
    "        \n",
    "        #export vocabs\n",
    "        build_vocab(preprocessed_neg, \"cleaned_vocab_neg_full\")\n",
    "        build_vocab(preprocessed_pos, \"cleaned_vocab_pos_full\")\n",
    "        build_vocab(preprocessed_test, \"cleaned_vocab_test_full\")\n",
    "    \n",
    "    else :\n",
    "        #export tweets\n",
    "        export(stemmed_neg,  \"stemmed_neg\")\n",
    "        export(stemmed_pos,  \"stemmed_pos\")\n",
    "        export(stemmed_test, \"stemmed_test\")  \n",
    "        \n",
    "        #export vocabs\n",
    "        build_vocab(preprocessed_neg, \"cleaned_vocab_neg\")\n",
    "        build_vocab(preprocessed_pos, \"cleaned_vocab_pos\")\n",
    "        build_vocab(preprocessed_test, \"cleaned_vocab_test\")\n",
    "        \n",
    "        \n",
    "#clean raw tweets\n",
    "def clean_tweets(path_pos, path_neg, path_test, is_full):\n",
    "    \n",
    "    process_data_no_stem(path_pos, path_neg, path_test, is_full)\n",
    "    stemming(is_full)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "def bitri_tweets():\n",
    "    print(\"bitri for neg running...\")\n",
    "    create_bitri_tweets('preprocessed_tweet_neg_full.txt', 'bitri_tweet_data_full.txt', False)\n",
    "    \n",
    "    print(\"bitri for pos running...\")    \n",
    "    create_bitri_tweets('preprocessed_tweet_pos_full.txt', 'bitri_tweet_data_full.txt', False)\n",
    "    \n",
    "    print(\"bitri for test running...\")\n",
    "    create_bitri_tweets('preprocessed_tweet_test_full.txt', 'bitri_tweet_test_full.txt', False)\n",
    "    \n",
    "    # load the tweets to be labelled\n",
    "    import_without_comma('bitri_tweet_data_full.txt')\n",
    "\n",
    "    #Build the new dataframes\n",
    "    path_pos_full = \"vocab_pos_full_bitri.txt\"\n",
    "    path_neg_full = \"vocab_neg_full_bitri.txt\"\n",
    "    path_test_data = \"vocab_data_bitri.txt\"\n",
    "\n",
    "\n",
    "    pos_full = build_df(path_pos_full)\n",
    "    neg_full = build_df(path_neg_full)\n",
    "    test_data = build_df(path_test_data)\n",
    "\n",
    "\n",
    "    #Merge Pos and neg vocabs    \n",
    "    merged_full = merging(neg_full, pos_full, True)\n",
    "    merged_full = merging(merged_full, test_data, True)\n",
    "\n",
    "\n",
    "    #Search minimal difference so that two opposite characteristic words cannot be seen together in data\n",
    "    min_diff = set_min_diff(tweets, merged_full)\n",
    "\n",
    "    #export characteristic words\n",
    "    characteristic_words = characteristic_words(merged_full, min_diff)\n",
    "    characteristic_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_relevant_vocab(pertinence_thres, min_count, dataframe):\n",
    "    relevant = dataframe[dataframe[\"ratio\"] >= pertinence_thres]\n",
    "    relevant = relevant[(relevant[\"occurence_pos\"] + relevant[\"occurence_neg\"]) >= min_count]\n",
    "    relevant = relevant[[\"word\",\"ratio\"]]\n",
    "    relevant = relevant.set_index(\"word\")\n",
    "    dict_relevant = relevant.to_dict()\n",
    "    relevant.to_pickle(\"relevant_vocab_pert=\"+str(pertinence_thres)+\"_count=\"+str(min_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_relevant_vocab(pertinence_thres, min_count, dataframe):\n",
    "    relevant = dataframe[dataframe[\"ratio\"] >= pertinence_thres]\n",
    "    relevant = relevant[(relevant[\"occurence_pos\"] + relevant[\"occurence_neg\"]) >= min_count]\n",
    "    relevant = relevant[[\"word\",\"ratio\"]]\n",
    "    relevant = relevant.set_index(\"word\")\n",
    "    dict_relevant = relevant.to_dict()\n",
    "    relevant.to_pickle(\"relevant_vocab_pert=\"+str(pertinence_thres)+\"_count=\"+str(min_count))\n",
    "    \n",
    "\n",
    "\n",
    "def create_data(lower_bound):\n",
    "    \n",
    "    #paths of positive and negative vocabs\n",
    "    path_pos = \"./vocab_cut_pos_full.txt\"\n",
    "    path_neg = \"./vocab_cut_neg_full.txt\"\n",
    "    \n",
    "    # pos is mapping of words in happy tweets with their occurences in all happy tweets\n",
    "    pos = pd.read_table(filepath_or_buffer = path_pos, header=None, names=[\"word\"])\n",
    "    pos[\"occurence\"] = pos[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    pos[\"word\"] = pos[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "\n",
    "    # neg is mapping of words in sad tweets with their occurences in all sad tweets\n",
    "    neg = pd.read_table(filepath_or_buffer = path_neg, header=None, names=[\"word\"])\n",
    "    neg[\"occurence\"] = neg[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    neg[\"word\"] = neg[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "    \n",
    "    # We merge the two dataframe in order to better handle them\n",
    "    merged = pd.merge(left=neg, right=pos, left_on = \"word\", right_on = \"word\", suffixes=('_neg', '_pos'),  how=\"outer\")\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "    #We only consider words whose occurences dfferences between sad and happy tweets is greater or equal than 5 \n",
    "    merged[\"difference\"] = abs((merged[\"occurence_neg\"]-merged[\"occurence_pos\"]))\n",
    "    merged = merged[merged[\"difference\"]>=5]\n",
    "\n",
    "    #We compute the sum of occurences\n",
    "    merged[\"somme\"] = merged[\"occurence_neg\"]+merged[\"occurence_pos\"]\n",
    "\n",
    "    #The ratio si how relevant it is to judge happyness/sadness of the tweet using the word : 0 if not relevant, 1 if truly relevant\n",
    "    merged[\"ratio\"] = 2* abs(0.5 - merged[\"occurence_pos\"]/(merged[\"occurence_pos\"]+merged[\"occurence_neg\"]))\n",
    "    \n",
    "    \n",
    "    def lower_ratio(x) :\n",
    "        if(x[\"somme\"]<MIN_SOMME):\n",
    "            return 0\n",
    "        else:\n",
    "            return x[\"ratio\"]\n",
    "    \n",
    "    \n",
    "    #We only consider with more than 'lower_bound' occurences\n",
    "    merged[\"ratio\"] = merged.apply(lower_ratio, axis = 1) \n",
    "    \n",
    "    #sort the array by ratio and then sum\n",
    "    merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False])\n",
    "    \n",
    "    #store the data\n",
    "    filename = \"relevant_vocab_full_lb=\"+str(lower_bound)+\".txt\"\n",
    "    merged.to_csv(path_or_buf=filename, sep=' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for lb in [50, 100, 500, 1000, 2000, 5000, 10000, 20000, 50000]:\n",
    "create_data(5000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type of the word investigation.\n",
    "\n",
    "Here we investigate if a type of word seems to be in a certain category of tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorizer(w):\n",
    "    w = nltk.word_tokenize(w)\n",
    "    c = nltk.pos_tag(w)\n",
    "    return c[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorizer(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged[\"categorize\"] = merged[\"word\"].map(categorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_ratio = merged.groupby(\"categorize\").mean().sort_values(\"ratio\")\n",
    "\n",
    "t_r = type_ratio[\"ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_r.plot.bar(figsize = (15,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
