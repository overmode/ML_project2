{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data one is given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple words do we have, are there some duplicates, is there any odds with the tweets? How are they classified (we are given two datasets, one for the positive tweets one for negatives) i.e. what is a typical word that occurs when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may contain words that are not usefull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than three following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Helper functions and files\n",
    "\n",
    "IOTweets contains the functions :\n",
    "- build_df\n",
    "- import_\n",
    "- export\n",
    "\n",
    "ProcessTweets contains the functions :\n",
    "- merging\n",
    "- filter_single_rep\n",
    "- powerful_words\n",
    "- sem_by_repr\n",
    "- sem_by_repr2\n",
    "- no_dot\n",
    "- find_repetition\n",
    "- no_s\n",
    "- stem_tweet\n",
    "- set_min_diff\n",
    "- contains\n",
    "- process_tweets_1\n",
    "- process_tweets_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An insight into the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import csv\n",
    "#%pylab inline # depreceated, use individual imports instead\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_pos = \"vocab_cut_pos.txt\"\n",
    "vocab_neg = \"vocab_cut_neg.txt\"\n",
    "\n",
    "vocab_pos_full = \"vocab_cut_pos_full.txt\"\n",
    "vocab_neg_full = \"vocab_cut_neg_full.txt\"\n",
    "vocab_test = \"vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos = \"train_pos.txt\"\n",
    "path_tweets_neg = \"train_neg.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"train_neg_full.txt\"\n",
    "path_tweets_test = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:15.337498\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:23.590561\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:02.446240\n",
      "removing items present less than 5 times %% % % % % % % % % % % % % % % %%%% %% % % %% % % % % %% %% % % % % % %%%% %\n",
      "Time elpased (hh:mm:ss.ms) 0:03:16.801615\n",
      "removing items present less than 5 times% %\n",
      "Time elpased (hh:mm:ss.ms) 0:03:54.105598\n"
     ]
    }
   ],
   "source": [
    "#import tweets\n",
    "pos_tweets = import_(path_tweets_pos)\n",
    "neg_tweets = import_(path_tweets_neg)\n",
    "pos_full_tweets = import_(path_tweets_pos_full)\n",
    "neg_full_tweets = import_(path_tweets_neg_full)\n",
    "test_tweets = import_without_id(path_tweets_test)\n",
    "\n",
    "#Count for vocab\n",
    "cut_threshold = 5\n",
    "pos_counter = build_vocab_counter(pos_tweets, cut_threshold, True)\n",
    "neg_counter = build_vocab_counter(neg_tweets, cut_threshold, True)\n",
    "test_counter = build_vocab_counter(test_tweets, cut_threshold, True)\n",
    "pos_full_counter = build_vocab_counter(pos_full_tweets, cut_threshold, True)\n",
    "neg_full_counter = build_vocab_counter(neg_full_tweets, cut_threshold, True)\n",
    "\n",
    "#build raw vocabs\n",
    "write_vocab_to_file(pos_counter, (\"raw_vocab_pos_cut=\"+str(cut_threshold)))\n",
    "write_vocab_to_file(neg_counter, (\"raw_vocab_neg_cut=\"+str(cut_threshold)))\n",
    "write_vocab_to_file(test_counter, (\"raw_vocab_test_cut=\"+str(cut_threshold)))\n",
    "write_vocab_to_file(pos_full_counter, (\"raw_vocab_pos_full_cut=\"+str(cut_threshold)))\n",
    "write_vocab_to_file(neg_full_counter, (\"raw_vocab_neg_full_cut=\"+str(cut_threshold)))\n",
    "\n",
    "# build the vocab Dataframe (DF) for full positive and negative tweets\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold))\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold))\n",
    "\n",
    "# build the vocab Dataframe (DF) for partial positive and negative tweets\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold))\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold))\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Study of word pertinence \n",
    "The idea is that the greater a word ratio i, the more pertinent it will be to cosider it when trying to label a tweet that contains it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence_neg</th>\n",
       "      <th>occurence_pos</th>\n",
       "      <th>difference</th>\n",
       "      <th>somme</th>\n",
       "      <th>ratio</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>a-tech</td>\n",
       "      <td>5329</td>\n",
       "      <td>0</td>\n",
       "      <td>5329</td>\n",
       "      <td>5329</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ddr</td>\n",
       "      <td>4844</td>\n",
       "      <td>0</td>\n",
       "      <td>4844</td>\n",
       "      <td>4844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>manufactured</td>\n",
       "      <td>4676</td>\n",
       "      <td>0</td>\n",
       "      <td>4676</td>\n",
       "      <td>4676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1gb</td>\n",
       "      <td>4216</td>\n",
       "      <td>0</td>\n",
       "      <td>4216</td>\n",
       "      <td>4216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>misc</td>\n",
       "      <td>3635</td>\n",
       "      <td>0</td>\n",
       "      <td>3635</td>\n",
       "      <td>3635</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  occurence_neg  occurence_pos  difference  somme  ratio  len\n",
       "429        a-tech           5329              0        5329   5329    1.0    6\n",
       "472           ddr           4844              0        4844   4844    1.0    3\n",
       "483  manufactured           4676              0        4676   4676    1.0   12\n",
       "527           1gb           4216              0        4216   4216    1.0    3\n",
       "614          misc           3635              0        3635   3635    1.0    4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge the pos and neg vocabs and study penrtinence of words via ratio\n",
    "merged_full = merging(neg_full_df, pos_full_df)\n",
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>\n",
    "    \n",
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Study of names with same beginning \n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['complete', 'completely', 'completed', 'completes', 'completeness'],\n",
       " ['tomorrow',\n",
       "  \"tomorrow's\",\n",
       "  'tomorrows',\n",
       "  'tomorrowww',\n",
       "  'tomorroww',\n",
       "  'tomorrowland'],\n",
       " ['everyone', \"everyone's\", 'everyones', 'everyonee', 'everyoneee'],\n",
       " ['watching', 'watchingg', 'watchinggg'],\n",
       " ['notebook', 'notebooks', \"notebook's\"],\n",
       " ['american', 'americans', 'americana', \"american's\"],\n",
       " ['birthday', 'birthdays', 'birthdayyy', \"birthday's\", 'birthdayy'],\n",
       " ['remember',\n",
       "  'remembered',\n",
       "  'remembering',\n",
       "  'remembers',\n",
       "  \"remember's\",\n",
       "  'rememberd'],\n",
       " ['business',\n",
       "  'businesses',\n",
       "  'businessman',\n",
       "  'businessweek',\n",
       "  'businessmen',\n",
       "  'businesswoman'],\n",
       " ['together', 'togetherr', 'togethers', 'togetherness'],\n",
       " ['original', 'originally', 'originals', 'original.title', 'originali'],\n",
       " ['official', 'officially', 'officials', 'officialy'],\n",
       " ['personal',\n",
       "  'personalized',\n",
       "  'personality',\n",
       "  'personally',\n",
       "  'personalize',\n",
       "  'personalities',\n",
       "  'personaliz',\n",
       "  'personalit',\n",
       "  'personalizing',\n",
       "  'personalised',\n",
       "  'personals'],\n",
       " ['computer', 'computers', 'computerworld', \"computer's\", 'computerized'],\n",
       " ['saturday', 'saturdays', \"saturday's\", 'saturdayy'],\n",
       " ['horrible', 'horribleee'],\n",
       " ['headache', 'headaches', 'headacheee', 'headachee', \"headache's\"],\n",
       " ['wireless', 'wirelessly'],\n",
       " ['favorite',\n",
       "  'favorites',\n",
       "  'favorited',\n",
       "  'favoritest',\n",
       "  'favoritee',\n",
       "  'favoriteee'],\n",
       " ['standard', 'standards', 'standardized'],\n",
       " ['internet', 'internets', \"internet's\", 'internetworking'],\n",
       " ['everyday', 'everydayy'],\n",
       " ['football', 'footballs', \"football's\", 'footballing'],\n",
       " ['somebody', \"somebody's\", 'somebodys', 'somebodyyy'],\n",
       " ['followed', 'followedyou', 'followed.follow', 'followedrt'],\n",
       " ['practice', 'practices', 'practicee', 'practiceee'],\n",
       " ['hospital', 'hospitals', 'hospitality', 'hospitalized', 'hospitalization'],\n",
       " ['pictures', 'picturesque', 'picturess'],\n",
       " ['japanese', 'japaneses'],\n",
       " ['manufactured',\n",
       "  'manufact',\n",
       "  'manufactur',\n",
       "  'manufactu',\n",
       "  'manufacture',\n",
       "  'manufacturer',\n",
       "  'manufacturing',\n",
       "  'manufacturers',\n",
       "  \"manufacturer's\",\n",
       "  'manufactures'],\n",
       " ['ultimate', 'ultimately', 'ultimates', \"ultimate's\"],\n",
       " ['homework', 'homeworks', 'homeworkkk'],\n",
       " ['supposed', 'supposedly'],\n",
       " ['children', \"children's\", 'childrens'],\n",
       " ['provides', 'providesquiet'],\n",
       " ['annoying', 'annoyinggg', 'annoyingg'],\n",
       " ['facebook', \"facebook's\", 'facebooks', 'facebooking'],\n",
       " ['straight',\n",
       "  'straightened',\n",
       "  'straighten',\n",
       "  'straightener',\n",
       "  'straightening',\n",
       "  'straightforward',\n",
       "  'straighteners',\n",
       "  'straightner',\n",
       "  'straightners',\n",
       "  'straightaway',\n",
       "  'straightness',\n",
       "  'straightt'],\n",
       " ['shoulder', 'shoulders', \"shoulder's\", 'shoulderrest'],\n",
       " ['thursday', 'thursdays', \"thursday's\", 'thursdayyy', 'thursdayy'],\n",
       " ['southern', 'southerner', 'southerners'],\n",
       " ['portable', 'portables'],\n",
       " ['national',\n",
       "  'nationals',\n",
       "  'nationality',\n",
       "  'nationalism',\n",
       "  'nationally',\n",
       "  'nationalities'],\n",
       " ['language', 'languages'],\n",
       " ['feelings', 'feelingss'],\n",
       " ['handbook', 'handbooks'],\n",
       " ['historic', 'historical', 'historically', 'historica'],\n",
       " ['studying', 'studyinggg'],\n",
       " ['research', 'researchers', 'researched', 'researcher', 'researches'],\n",
       " ['terrible', 'terribleee'],\n",
       " ['security', 'securityman'],\n",
       " ['hahahaha',\n",
       "  'hahahahaha',\n",
       "  'hahahahah',\n",
       "  'hahahahahaha',\n",
       "  'hahahahahahaha',\n",
       "  'hahahahahah',\n",
       "  'hahahahahahahaha',\n",
       "  'hahahahaa',\n",
       "  'hahahahahha',\n",
       "  'hahahahahahahahaha',\n",
       "  'hahahahahahah',\n",
       "  'hahahahahahahah',\n",
       "  'hahahahahahahahahahaha',\n",
       "  'hahahahahaa',\n",
       "  'hahahahaah',\n",
       "  'hahahahahahahahahah',\n",
       "  'hahahahahaaa',\n",
       "  'hahahahahahahahah',\n",
       "  'hahahahaahah',\n",
       "  'hahahahahhaa',\n",
       "  'hahahahahahahha',\n",
       "  'hahahahahahaa',\n",
       "  'hahahahahahahahahahahahahahahaha'],\n",
       " ['european', 'europeans', 'europeanfanswant'],\n",
       " ['download', 'downloaded', 'downloading', 'downloadable', 'downloader'],\n",
       " ['lifetime', 'lifetimes'],\n",
       " ['princess', 'princesses', 'princesss', \"princess's\"],\n",
       " ['exterior', 'exteriors'],\n",
       " ['building', 'buildings'],\n",
       " ['diameter', 'diameters'],\n",
       " ['freaking', 'freakingg'],\n",
       " ['creative', 'creativeworx', 'creatives'],\n",
       " ['magazine', 'magazines', \"magazine's\"],\n",
       " ['medicine', 'medicines'],\n",
       " ['breaking', 'breakingnews'],\n",
       " ['keyboard', 'keyboards', 'keyboardist', 'keyboarding'],\n",
       " ['electric',\n",
       "  'electrical',\n",
       "  'electricity',\n",
       "  'electrician',\n",
       "  'electrics',\n",
       "  'electrically',\n",
       "  'electricians'],\n",
       " ['questions',\n",
       "  'question',\n",
       "  'questioning',\n",
       "  'questionable',\n",
       "  'questioned',\n",
       "  'questionnaire',\n",
       "  'questionnaires'],\n",
       " ['whatever', 'whateverrr', 'whateverr', 'whatevers'],\n",
       " ['baseball', 'baseballs', \"baseball's\", 'baseballblogs', 'baseballl'],\n",
       " ['multiple', 'multiples', 'multiplex'],\n",
       " ['whenever', 'wheneverr'],\n",
       " ['avengers', 'avengersss'],\n",
       " ['revision', 'revisions'],\n",
       " ['powerful', 'powerfully'],\n",
       " ['laughing', 'laughingg'],\n",
       " ['mountain',\n",
       "  'mountains',\n",
       "  'mountaineers',\n",
       "  'mountaineering',\n",
       "  'mountaineer',\n",
       "  'mountainous',\n",
       "  'mountaintop',\n",
       "  'mountainsmith'],\n",
       " ['although', 'althought'],\n",
       " ['daughter', 'daughters', \"daughter's\"],\n",
       " ['gorgeous', 'gorgeously', 'gorgeousss', 'gorgeousness', 'gorgeouss'],\n",
       " ['director',\n",
       "  'directory',\n",
       "  \"director's\",\n",
       "  'directors',\n",
       "  'directorial',\n",
       "  'directories'],\n",
       " ['freezing', 'freezinggg'],\n",
       " ['thoughts', 'thoughtstream'],\n",
       " ['backpack',\n",
       "  'backpacks',\n",
       "  'backpacking',\n",
       "  'backpacker',\n",
       "  \"backpacker's\",\n",
       "  'backpackers'],\n",
       " ['industry', \"industry's\", 'industrys', 'industryweek'],\n",
       " ['metallic', 'metallicum', 'metallica', \"metallica's\", 'metallics'],\n",
       " ['exciting', 'excitinggg'],\n",
       " ['designer', 'designers', \"designer's\"],\n",
       " ['accident',\n",
       "  'accidentally',\n",
       "  'accidental',\n",
       "  'accidently',\n",
       "  'accidents',\n",
       "  'accidentaly'],\n",
       " ['approach', 'approaches', 'approaching', 'approached'],\n",
       " ['vacation', 'vacations', 'vacationing', 'vacationland'],\n",
       " ['solution', 'solutions'],\n",
       " ['continue', 'continues', 'continued'],\n",
       " ['discover',\n",
       "  'discovery',\n",
       "  'discovered',\n",
       "  'discovering',\n",
       "  'discovers',\n",
       "  'discoveries',\n",
       "  'discoverer',\n",
       "  'discoverers',\n",
       "  \"discovery's\"],\n",
       " ['calendar', 'calendars'],\n",
       " ['midnight', \"midnight's\"],\n",
       " ['schedule', 'scheduled', 'schedules'],\n",
       " ['material', 'materials', 'materialpatented', 'materialism'],\n",
       " ['response', 'responses'],\n",
       " ['adorable', 'adorablee', 'adorableee', 'adorableness'],\n",
       " ['decision', 'decisions'],\n",
       " ['religion', 'religions'],\n",
       " ['strength',\n",
       "  'strengths',\n",
       "  'strengthen',\n",
       "  'strengthening',\n",
       "  'strengthens',\n",
       "  'strengthened'],\n",
       " ['canadian', 'canadians'],\n",
       " ['standing', 'standings'],\n",
       " ['critical', 'critically'],\n",
       " ['physical', 'physically'],\n",
       " ['bathroom', 'bathrooms'],\n",
       " ['external', 'externally'],\n",
       " ['teaching', 'teachings'],\n",
       " ['shoutout', 'shoutouts', 'shoutouttt', 'shoutoutt'],\n",
       " ['timeline', 'timelines', \"timeline's\"],\n",
       " ['unfollow',\n",
       "  'unfollowed',\n",
       "  'unfollowing',\n",
       "  'unfollows',\n",
       "  'unfollowers',\n",
       "  'unfollower',\n",
       "  'unfollowin'],\n",
       " ['sandwich', 'sandwiches'],\n",
       " ['exercise', 'exercises', 'exerciser', 'exercised'],\n",
       " ['position',\n",
       "  'positions',\n",
       "  'positioning',\n",
       "  'positioner',\n",
       "  'positional',\n",
       "  'positioned'],\n",
       " ['cookbook', 'cookbooks'],\n",
       " ['romantic',\n",
       "  'romanticism',\n",
       "  'romantics',\n",
       "  'romantica',\n",
       "  'romantically',\n",
       "  'romantical'],\n",
       " ['clinical', 'clinically', 'clinicals'],\n",
       " ['survival', 'survivalist'],\n",
       " ['graduate', 'graduated', 'graduates'],\n",
       " ['stranger', 'strangers', 'strangerrr'],\n",
       " ['magnetic', 'magnetics'],\n",
       " ['stunning', 'stunningly'],\n",
       " ['starving', 'starvinggg'],\n",
       " ['acoustic', 'acoustics'],\n",
       " ['festival', 'festivals'],\n",
       " ['pressure', 'pressures'],\n",
       " ['imperial', 'imperialism'],\n",
       " ['slightly', 'slightlyyy'],\n",
       " ['struggle', 'struggles', 'struggled'],\n",
       " ['semester', 'semesters'],\n",
       " ['colorful', 'colorfully'],\n",
       " ['customer', 'customers', \"customer's\"],\n",
       " ['receiver', 'receivers'],\n",
       " ['sometimes', 'sometime', 'sometimess', 'sometimeee'],\n",
       " ['surprise', 'surprised', 'surprises'],\n",
       " ['softball', 'softballs'],\n",
       " ['friendly', \"friendly's\", 'friendlys'],\n",
       " ['necklace', 'necklaces'],\n",
       " ['universe', 'universes'],\n",
       " ['bracelet', 'bracelets'],\n",
       " ['thousand', 'thousands'],\n",
       " ['republic', 'republican', 'republicans', 'republicanism'],\n",
       " ['specific',\n",
       "  'specifically',\n",
       "  'specifications',\n",
       "  'specification',\n",
       "  'specificati',\n",
       "  'specifica',\n",
       "  'specificall',\n",
       "  'specifical',\n",
       "  'specifics'],\n",
       " ['northern', 'northerners'],\n",
       " ['thankyou', 'thankyouuu', 'thankyouu'],\n",
       " ['latitude', 'latitudes'],\n",
       " ['explorer', 'explorers', \"explorer's\"],\n",
       " ['superior', 'superiority'],\n",
       " ['platform', 'platforms', 'platformer'],\n",
       " ['virginia', \"virginia's\", 'virginian'],\n",
       " ['marriage', 'marriages'],\n",
       " ['marathon', 'marathons'],\n",
       " ['swimsuit', 'swimsuits'],\n",
       " ['internal', 'internally', 'internals'],\n",
       " ['absolutely', 'absolute'],\n",
       " ['thriller', 'thrillers'],\n",
       " ['envelope', 'envelopes'],\n",
       " ['resource', 'resources', 'resourceful'],\n",
       " ['painting', 'paintings', 'paintingiant'],\n",
       " ['positive', 'positives'],\n",
       " ['engineering', 'engineer', 'engineers', 'engineered', \"engineer's\"],\n",
       " ['fabulous', 'fabulously', 'fabulousness'],\n",
       " ['darkness', 'darkness.product'],\n",
       " ['function',\n",
       "  'functional',\n",
       "  'functions',\n",
       "  'functionality',\n",
       "  'functioning',\n",
       "  'functional.boxwave',\n",
       "  'functionally',\n",
       "  'functiona'],\n",
       " ['carolina', 'carolinas'],\n",
       " ['#believe',\n",
       "  '#believealbumcover',\n",
       "  '#believetour',\n",
       "  '#believetourinmanila',\n",
       "  '#believeinblue',\n",
       "  '#believetouriscoming'],\n",
       " ['workbook', 'workbooks'],\n",
       " ['sunshine', 'sunshineee', 'sunshines', 'sunshiney'],\n",
       " ['innocent', 'innocents'],\n",
       " ['followers',\n",
       "  'follower',\n",
       "  'followers.please',\n",
       "  'followers.but',\n",
       "  'followerss',\n",
       "  'followersss',\n",
       "  'followerr',\n",
       "  \"follower's\",\n",
       "  'followerrr'],\n",
       " ['transfer', 'transferring', 'transfers', 'transferred', 'transfered'],\n",
       " ['waterloo', 'waterlooroad'],\n",
       " ['location', 'locations'],\n",
       " ['handsome', 'handsomely', 'handsomee', 'handsomeee', 'handsomes'],\n",
       " ['charming', 'charmingly'],\n",
       " ['workshop', 'workshops'],\n",
       " ['michigan', \"michigan's\"],\n",
       " ['airplane', 'airplanes'],\n",
       " ['nintendo', \"nintendo's\"],\n",
       " ['interesting', 'interest', 'interested', 'intereste', 'interestin'],\n",
       " ['handheld', 'handhelditems'],\n",
       " ['scotland', \"scotland's\"],\n",
       " ['migraine', 'migraines'],\n",
       " ['portrait', 'portraits'],\n",
       " ['economic', 'economics', 'economical', 'economically'],\n",
       " ['congrats', 'congratsss', 'congratss'],\n",
       " ['williams', 'williamsburg', 'williamson', \"williams's\"],\n",
       " ['carriage', 'carriages'],\n",
       " ['dramatic', 'dramatically'],\n",
       " ['december', 'decemberists'],\n",
       " ['separate', 'separates', 'separated', 'separately'],\n",
       " ['champions', 'champion', 'championship', \"champion's\"],\n",
       " ['umbrella', 'umbrellas'],\n",
       " ['treasure', 'treasures', 'treasured', 'treasurer'],\n",
       " ['prepared', 'preparedness'],\n",
       " ['reaction', 'reactions'],\n",
       " ['beliebers', 'belieber', \"belieber's\"],\n",
       " ['cassette', 'cassettes'],\n",
       " ['goodness', 'goodness.fits'],\n",
       " ['shooting', 'shootings'],\n",
       " ['documentary',\n",
       "  'document',\n",
       "  'documents',\n",
       "  'documentation',\n",
       "  'documenting',\n",
       "  'documented',\n",
       "  'documentaries',\n",
       "  'documenta'],\n",
       " ['whatsapp', 'whatsapps', 'whatsapping'],\n",
       " ['progress',\n",
       "  'progressive',\n",
       "  'progresso',\n",
       "  'progression',\n",
       "  'progressing',\n",
       "  'progressions',\n",
       "  'progresses',\n",
       "  'progressively'],\n",
       " ['malaysia', 'malaysian', 'malaysians', \"malaysia's\"],\n",
       " ['pakistan', 'pakistani', \"pakistan's\"],\n",
       " ['addition', 'additional', 'additions'],\n",
       " ['opposite', 'opposites'],\n",
       " ['pleasure', 'pleasures'],\n",
       " ['hangover', 'hangovers'],\n",
       " ['bullshit', 'bullshitting', 'bullshiting', 'bullshitter', 'bullshitt'],\n",
       " ['contract',\n",
       "  'contracts',\n",
       "  'contractor',\n",
       "  'contractions',\n",
       "  'contractors',\n",
       "  'contracted',\n",
       "  'contracting',\n",
       "  'contraction'],\n",
       " ['delivery', 'delivery.frame'],\n",
       " ['columbia', 'columbian', \"columbia's\"],\n",
       " ['parallel', 'parallels'],\n",
       " ['movement', 'movements'],\n",
       " ['graphite', 'graphites'],\n",
       " ['kingston', 'kingstons', \"kingston's\"],\n",
       " ['chilling', 'chillingg', 'chillinggg'],\n",
       " ['recorder', 'recorders'],\n",
       " ['textbook', 'textbooks'],\n",
       " ['offering', 'offerings'],\n",
       " ['describes', 'describe', 'described'],\n",
       " ['tommorow', 'tommorows'],\n",
       " ['athletic', 'athletics', 'athletico'],\n",
       " ['dressing', 'dressings'],\n",
       " ['innovate', 'innovateunlike'],\n",
       " ['regional', 'regionals'],\n",
       " ['guardian', 'guardians', 'guardianmeet'],\n",
       " ['timeless', 'timelessness'],\n",
       " ['piercing', 'piercings'],\n",
       " ['attitude', 'attitudes'],\n",
       " ['previous', 'previously'],\n",
       " ['catholic', 'catholicism', 'catholics'],\n",
       " ['campaign',\n",
       "  'campaigns',\n",
       "  'campaigners',\n",
       "  'campaigner',\n",
       "  'campaignlive',\n",
       "  'campaigned'],\n",
       " ['thailand', \"thailand's\"],\n",
       " ['announce',\n",
       "  'announces',\n",
       "  'announced',\n",
       "  'announcement',\n",
       "  'announcements',\n",
       "  'announcers'],\n",
       " ['polaroid', 'polaroids', \"polaroid's\"],\n",
       " ['roommate', \"roommate's\", 'roommates'],\n",
       " ['something',\n",
       "  'somethin',\n",
       "  \"something's\",\n",
       "  'somethings',\n",
       "  'somethingg',\n",
       "  'somethinn'],\n",
       " ['interior', 'interiors'],\n",
       " ['mistakes', 'mistakes.here'],\n",
       " ['negative', 'negatives'],\n",
       " ['chemical', 'chemicals', 'chemically'],\n",
       " ['jennifer', \"jennifer's\"],\n",
       " ['victoria', 'victorian'],\n",
       " ['frontier', 'frontiers', 'frontiercycle'],\n",
       " ['criminal', 'criminals'],\n",
       " ['elephant', 'elephants', \"elephant's\", 'elephantmen'],\n",
       " ['database', 'databases'],\n",
       " ['exchange', 'exchanged'],\n",
       " ['shocking', 'shockingly'],\n",
       " ['premiere', 'premieres'],\n",
       " ['franklin', \"franklin's\"],\n",
       " ['creation', 'creations'],\n",
       " ['tutorial', 'tutorials'],\n",
       " ['purchase', 'purchased', 'purchases', 'purchasers', 'purchaser'],\n",
       " ['behavior', 'behavioral', 'behaviors', 'behaviorally'],\n",
       " ['colorado', \"colorado's\"],\n",
       " ['disaster', 'disasters'],\n",
       " ['playlist', 'playlists'],\n",
       " ['adhesive', 'adhesives'],\n",
       " ['producer', 'producers', \"producer's\"],\n",
       " ['teenager', 'teenagers'],\n",
       " ['passport', \"passport's\"],\n",
       " ['increase', 'increased', 'increases'],\n",
       " ['tactical', 'tactically'],\n",
       " ['considering',\n",
       "  'considered',\n",
       "  'consider',\n",
       "  'considers',\n",
       "  'considerable',\n",
       "  'considerations'],\n",
       " ['grateful', 'gratefully'],\n",
       " ['danielle', \"danielle's\"],\n",
       " ['resistor', 'resistors'],\n",
       " ['wardrobe', 'wardrobes'],\n",
       " ['conflict', 'conflicts', 'conflicting', 'conflicted'],\n",
       " ['iloveyou',\n",
       "  'iloveyouu',\n",
       "  'iloveyoussosomuch',\n",
       "  'iloveyouuu',\n",
       "  'iloveyoubieber',\n",
       "  'iloveyoutoo'],\n",
       " ['fountain', 'fountains', 'fountainhead'],\n",
       " ['reporter', \"reporter's\"],\n",
       " ['abstract', 'abstraction', 'abstracts'],\n",
       " ['intimate', 'intimates', 'intimately'],\n",
       " ['compound', 'compounds'],\n",
       " ['spelling', 'spellings'],\n",
       " ['explicit', 'explicitly'],\n",
       " ['keychain', 'keychains'],\n",
       " ['constantly',\n",
       "  'constant',\n",
       "  'constantine',\n",
       "  'constantin',\n",
       "  'constantinople',\n",
       "  'constants',\n",
       "  'constantinian'],\n",
       " ['hamilton', \"hamilton's\"],\n",
       " ['backyard', 'backyardigans'],\n",
       " ['complain',\n",
       "  'complaining',\n",
       "  'complaints',\n",
       "  'complaint',\n",
       "  'complained',\n",
       "  'complains',\n",
       "  'complainin',\n",
       "  'complainers'],\n",
       " ['greeting', 'greetings'],\n",
       " ['shredder', 'shredders'],\n",
       " ['forehead', 'foreheads'],\n",
       " ['#missyou', '#missyoualready', '#missyousomuch'],\n",
       " ['brooklyn', 'brooklynvegan', 'brooklynn', \"brooklyn's\"],\n",
       " ['#jealous', '#jealoustweet'],\n",
       " ['forecast', 'forecasting', 'forecaster', 'forecasts', 'forecasted'],\n",
       " ['landmark', 'landmarks'],\n",
       " ['colonial', 'colonialism'],\n",
       " ['#imagine', '#imagines'],\n",
       " ['egyptian', 'egyptians'],\n",
       " ['#loveyou',\n",
       "  '#loveyouguys',\n",
       "  '#loveyoutoo',\n",
       "  '#loveyouall',\n",
       "  '#loveyouzayn',\n",
       "  '#loveyouuu',\n",
       "  '#loveyouu',\n",
       "  '#loveyousomuch',\n",
       "  '#loveyoumore',\n",
       "  '#loveyougirls',\n",
       "  '#loveyouliam',\n",
       "  '#loveyouforever',\n",
       "  '#loveyoureyes',\n",
       "  '#loveyougirl',\n",
       "  '#loveyouboo',\n",
       "  '#loveyoubaby',\n",
       "  '#loveyoubestfriend',\n",
       "  '#loveyouboth',\n",
       "  '#loveyous'],\n",
       " ['disorder', 'disorders', 'disorderly', 'disordered'],\n",
       " ['alphabet', 'alphabets', 'alphabetically'],\n",
       " ['humanity', \"humanity's\"],\n",
       " ['anderson', 'andersonville', \"anderson's\", 'andersons'],\n",
       " ['suspense', 'suspenseful'],\n",
       " ['thankful', 'thankfully', 'thankfull'],\n",
       " ['splitter', 'splitters'],\n",
       " ['seamless', 'seamlessly'],\n",
       " ['organizer', 'organize', 'organized', 'organizes', 'organizers'],\n",
       " ['montreal', \"montreal's\"],\n",
       " ['coldplay', \"coldplay's\"],\n",
       " ['discount', 'discounted', 'discounts'],\n",
       " ['beginners', \"beginner's\", 'beginner'],\n",
       " ['reported', 'reportedly'],\n",
       " ['sentence', 'sentences', 'sentenced'],\n",
       " ['domestic', 'domestically'],\n",
       " ['ornament', 'ornaments', 'ornamental'],\n",
       " ['diamonds', 'diamondscreen'],\n",
       " ['decorate', 'decorated', 'decorates'],\n",
       " ['terminal', 'terminals', 'terminally'],\n",
       " ['crossing', 'crossings'],\n",
       " ['surround', 'surrounded', 'surrounding', 'surroundbar', 'surrounds'],\n",
       " ['pleeease', 'pleeeaseee', 'pleeeasee'],\n",
       " ['buddhist', 'buddhists'],\n",
       " ['broadway', \"broadway's\"],\n",
       " ['outdoors', 'outdoorsman'],\n",
       " ['yearbook', 'yearbooks'],\n",
       " ['thorough', 'thoroughly', 'thoroughbred'],\n",
       " ['marshall', 'marshalltown', 'marshalls'],\n",
       " ['division', 'divisions', 'divisional'],\n",
       " ['convince', 'convinced', 'convinces'],\n",
       " ['showcase', 'showcases', 'showcased'],\n",
       " ['mosquito', 'mosquitoes', 'mosquitos'],\n",
       " ['defender', 'defenders'],\n",
       " ['argument', 'arguments', 'argumentation'],\n",
       " ['dinosaur', 'dinosaurs'],\n",
       " ['consumer', 'consumers', \"consumer's\", 'consumerism'],\n",
       " ['alliance', 'alliances'],\n",
       " ['headband', 'headbands'],\n",
       " ['goodluck', 'goodluckk'],\n",
       " ['kentucky', \"kentucky's\"],\n",
       " ['audition', 'auditions', 'auditioning', 'auditioned'],\n",
       " ['hardwood', 'hardwoods'],\n",
       " ['insomnia', 'insomniac'],\n",
       " ['accurate', 'accurately'],\n",
       " ['username', 'usernames'],\n",
       " ['infinite', 'infinitely', \"infinite's\"],\n",
       " ['valuable', 'valuables'],\n",
       " ['neighborhood', 'neighbors', 'neighbor', 'neighborhoods', 'neighboring'],\n",
       " ['district', 'districts'],\n",
       " ['maintain', 'maintaining', 'maintains', 'maintained', 'maintainer'],\n",
       " ['thompson', 'thompsons'],\n",
       " ['traveler', 'travelers', \"traveler's\"],\n",
       " ['unlocked', 'unlocked.just'],\n",
       " ['resident', 'residential', 'residents'],\n",
       " ['verbatim', \"verbatim's\"],\n",
       " ['peaceful', 'peacefully', 'peacefull'],\n",
       " ['election', 'elections'],\n",
       " ['registered', 'register', 'registers'],\n",
       " ['#notcool', '#notcoolbro'],\n",
       " ['incident', 'incidents', 'incidentally'],\n",
       " ['hopeless', 'hopelessly'],\n",
       " ['triangle', 'triangles'],\n",
       " ['trendnet', 'trendnets'],\n",
       " ['aquarium', 'aquariums'],\n",
       " ['giveaway', 'giveaways'],\n",
       " ['delicate', 'delicately'],\n",
       " ['miracles', 'miraclesuit'],\n",
       " ['creatures', 'creature'],\n",
       " ['deadline', 'deadlines'],\n",
       " ['audience', 'audiences'],\n",
       " ['brittany', \"brittany's\", 'brittanys'],\n",
       " ['highland', 'highlander', 'highlands', 'highlanders'],\n",
       " ['squirrel', 'squirrels'],\n",
       " ['contrast', 'contrasting', 'contrasts'],\n",
       " ['mushroom', 'mushrooms'],\n",
       " ['missouri', \"missouri's\"],\n",
       " ['haunting', 'hauntingly', 'hauntings'],\n",
       " ['faithful', 'faithfully', 'faithfull'],\n",
       " ['blackout', 'blackouts'],\n",
       " ['circular', 'circulars'],\n",
       " ['occasion', 'occasional', 'occasions', 'occasionally'],\n",
       " ['mitchell', \"mitchell's\"],\n",
       " ['babygirl', 'babygirlll'],\n",
       " ['basement', 'basements'],\n",
       " ['congress', 'congressional', 'congressman'],\n",
       " ['lawrence', 'lawrenceville'],\n",
       " ['savannah', \"savannah's\"],\n",
       " ['#notgood', '#notgoodenough'],\n",
       " ['cardinal', 'cardinals'],\n",
       " ['pleaaase', 'pleaaaseee'],\n",
       " ['survivor', 'survivors', \"survivor's\"],\n",
       " ['collapse', 'collapsed', 'collapses'],\n",
       " ['robinson', \"robinson's\"],\n",
       " ['partners', 'partnership', 'partnerships'],\n",
       " ['hedgehog', 'hedgehogs'],\n",
       " ['campbell', \"campbell's\", 'campbells'],\n",
       " ['striking', 'strikingly'],\n",
       " ['vocalist', \"vocalist's\", 'vocalists'],\n",
       " ['betrayal', 'betrayals'],\n",
       " ['eternity', \"eternity's\"],\n",
       " ['mandarin', 'mandarins'],\n",
       " ['immortal', 'immortals', 'immortality', 'immortalis'],\n",
       " ['peterson', \"peterson's\"],\n",
       " ['cellular', 'cellularfactory'],\n",
       " ['forensic', 'forensics'],\n",
       " ['flawless', 'flawlesss'],\n",
       " ['facetime', 'facetimed'],\n",
       " ['weathers', 'weatherstrip', 'weatherspoons'],\n",
       " ['glorious', 'gloriously'],\n",
       " ['mortgage', 'mortgages'],\n",
       " ['tailgate', 'tailgater'],\n",
       " ['composer', 'composers', \"composer's\"],\n",
       " ['brighten', 'brightens', 'brightened'],\n",
       " ['vanguard', 'vanguardia'],\n",
       " ['operator', 'operators'],\n",
       " ['musicians', 'musician', \"musician's\"],\n",
       " ['chestnut', \"chestnut's\", 'chestnuts'],\n",
       " ['cosmetic', 'cosmetics'],\n",
       " ['railroad', 'railroads'],\n",
       " ['steelers', 'steelersmobile'],\n",
       " ['benjamin', \"benjamin's\"],\n",
       " ['figurine', 'figurines'],\n",
       " ['novelist', 'novelists'],\n",
       " ['nigerian', 'nigerians'],\n",
       " ['maximize', 'maximizer', 'maximizes'],\n",
       " ['predator', 'predators', 'predatory'],\n",
       " ['cherokee', 'cherokees'],\n",
       " ['keepsake', 'keepsakes'],\n",
       " ['harrison', \"harrison's\"],\n",
       " ['majestic', \"majestic's\"],\n",
       " ['artistic', 'artistically'],\n",
       " ['mechanics', 'mechanical', 'mechanic', 'mechanicsville'],\n",
       " ['generous', 'generously'],\n",
       " ['filipino', 'filipinos', \"filipino's\"],\n",
       " ['electrol',\n",
       "  'electroluminescent',\n",
       "  'electrolyte',\n",
       "  'electroline',\n",
       "  'electrolytes',\n",
       "  'electrolux',\n",
       "  'electrolysis'],\n",
       " ['conquest', 'conquests'],\n",
       " ['pleasant', 'pleasanton', 'pleasantly'],\n",
       " ['concerto', 'concertos'],\n",
       " ['marinade', 'marinades'],\n",
       " ['thirteen', 'thirteenth'],\n",
       " ['cylinder', 'cylinders'],\n",
       " ['gemstone', 'gemstones'],\n",
       " ['entrance', 'entranced'],\n",
       " ['provider', 'providers'],\n",
       " ['hispanic', 'hispanics'],\n",
       " ['sounding', 'soundings'],\n",
       " ['membrane', 'membranes'],\n",
       " ['breakout', 'breakouts'],\n",
       " ['oriental', 'orientale'],\n",
       " ['samantha', \"samantha's\"],\n",
       " ['casebook', 'casebooks'],\n",
       " ['minister', \"minister's\", 'ministering'],\n",
       " ['napoleon', \"napoleon's\", 'napoleonic'],\n",
       " ['blessing', 'blessings'],\n",
       " ['colombia', 'colombian', \"colombia's\", 'colombiana'],\n",
       " ['variable', 'variables'],\n",
       " ['detector', 'detectors'],\n",
       " ['snapshot', 'snapshots'],\n",
       " ['rosemary', \"rosemary's\"],\n",
       " ['assassin',\n",
       "  'assassins',\n",
       "  'assassination',\n",
       "  \"assassin's\",\n",
       "  'assassinated',\n",
       "  'assassinate'],\n",
       " ['feedback', 'feedbacks'],\n",
       " ['jamaican', 'jamaicans'],\n",
       " ['retainer', 'retainers'],\n",
       " ['postcards', 'postcard'],\n",
       " ['jealousy', \"jealousy's\"],\n",
       " ['dialogue', 'dialogues'],\n",
       " ['hazelnut', 'hazelnuts'],\n",
       " ['instinct', 'instincts', 'instinctive'],\n",
       " ['nickname', 'nicknames', 'nicknamed'],\n",
       " ['woodland', 'woodlands'],\n",
       " ['corvette', 'corvettes'],\n",
       " ['makeover', 'makeovers'],\n",
       " ['refollow', 'refollowed'],\n",
       " ['canisters', 'canister'],\n",
       " ['pacifier', 'pacifiers'],\n",
       " ['garfield', \"garfield's\"],\n",
       " ['assembled', 'assemble', 'assembles', 'assembler'],\n",
       " ['crawford', \"crawford's\"],\n",
       " ['pedestal', 'pedestals'],\n",
       " ['exposure', 'exposures'],\n",
       " ['troubles',\n",
       "  'troubleshooting',\n",
       "  'troublesome',\n",
       "  'troubleshooters',\n",
       "  'troubleshoot'],\n",
       " ['earphones', 'earphone'],\n",
       " ['motivated', 'motivate'],\n",
       " ['lecturer', 'lecturers'],\n",
       " ['backdrop', 'backdrops'],\n",
       " ['tortilla', 'tortillas'],\n",
       " ['spitfire', 'spitfires'],\n",
       " ['feminist', 'feminists'],\n",
       " ['helpless', 'helplessly'],\n",
       " ['fastener', 'fasteners'],\n",
       " ['dedicated', 'dedicate', 'dedicates'],\n",
       " ['rechargeable', 'recharge', 'recharged'],\n",
       " ['illusion', 'illusions'],\n",
       " ['cambodia', 'cambodian'],\n",
       " ['happening', 'happenin'],\n",
       " ['restless', 'restlessness'],\n",
       " ['moderate', 'moderately', 'moderated'],\n",
       " ['doctrine', 'doctrines'],\n",
       " ['chipmunk', 'chipmunks'],\n",
       " ['stockings', 'stocking'],\n",
       " ['fearless', 'fearlessly'],\n",
       " ['identify', 'identifying'],\n",
       " ['morrison', \"morrison's\"],\n",
       " ['pinnacle', \"pinnacle's\"],\n",
       " ['inverter', 'inverters'],\n",
       " ['comedian', 'comedians'],\n",
       " ['prisoner', 'prisoners'],\n",
       " ['advocate', 'advocates'],\n",
       " ['judgmental', 'judgment'],\n",
       " ['citizens', 'citizenship'],\n",
       " ['downside', 'downsides'],\n",
       " ['inventor', 'inventory', 'inventors', \"inventor's\"],\n",
       " ['massacre', 'massacred'],\n",
       " ['einstein', \"einstein's\", 'einsteins'],\n",
       " ['margaret', \"margaret's\"],\n",
       " ['barefoot', 'barefootstudent'],\n",
       " ['grandmas', 'grandmaster'],\n",
       " ['intrigue', 'intrigued'],\n",
       " ['magician', 'magicians', \"magician's\"],\n",
       " ['province', 'provinces'],\n",
       " ['disagree', 'disagreed', 'disagreement', 'disagreeing'],\n",
       " ['balsamic', 'balsamico'],\n",
       " ['commuter', 'commuters'],\n",
       " ['moroccan', 'moroccanoil'],\n",
       " ['angelina', \"angelina's\"],\n",
       " ['magnolia', 'magnolias'],\n",
       " ['scholarship', 'scholars', 'scholarships'],\n",
       " ['valencia', \"valencia's\"],\n",
       " ['sequence', 'sequences', 'sequencer'],\n",
       " ['purifier', 'purifiers'],\n",
       " ['radiator', 'radiators'],\n",
       " ['bookmark', 'bookmarks', 'bookmarked', 'bookmarking'],\n",
       " ['enhancer', 'enhancers'],\n",
       " ['benedict', 'benedictine', \"benedict's\"],\n",
       " ['freshmen', 'freshmens'],\n",
       " ['wrangler', 'wranglers'],\n",
       " ['memorize', 'memorized'],\n",
       " ['hallmark', 'hallmarks'],\n",
       " ['bulletin', 'bulletins'],\n",
       " ['abortion', 'abortions'],\n",
       " ['potpourri', 'potpourr'],\n",
       " ['mustache', 'mustaches'],\n",
       " ['hehehehe', 'hehehehehe', 'heheheheh'],\n",
       " ['imessage', 'imessages'],\n",
       " ['sentinel', 'sentinels'],\n",
       " ['interactive',\n",
       "  'interaction',\n",
       "  'interactions',\n",
       "  'interact',\n",
       "  'interacting',\n",
       "  'interactivity',\n",
       "  'interacti',\n",
       "  'interacts'],\n",
       " ['monogram', 'monogrammed'],\n",
       " ['headline', 'headlines', 'headliner', 'headliners'],\n",
       " ['sunglasses', 'sunglass'],\n",
       " ['profound', 'profoundly'],\n",
       " ['maverick', 'mavericks'],\n",
       " ['orthodox', 'orthodoxy'],\n",
       " ['investors', 'investor', \"investor's\"],\n",
       " ['seashell', 'seashells'],\n",
       " ['disgrace', 'disgraced', 'disgraceful'],\n",
       " ['#toronto', '#torontofree'],\n",
       " ['murderer', 'murderers'],\n",
       " ['finalist', 'finalists'],\n",
       " ['technology',\n",
       "  'technologies',\n",
       "  'technological',\n",
       "  'technolo',\n",
       "  'technolog',\n",
       "  \"technology's\",\n",
       "  'technologist',\n",
       "  'technologically',\n",
       "  'technologists',\n",
       "  'technologie',\n",
       "  'technologi'],\n",
       " ['apparently', 'apparent'],\n",
       " ['relationship', 'relationships', 'relations', 'relation', 'relational'],\n",
       " ['fourteen', 'fourteenth'],\n",
       " ['coupling', 'couplings'],\n",
       " ['shepherd', \"shepherd's\", 'shepherds'],\n",
       " ['bradford', 'bradfordian'],\n",
       " ['overload', 'overloaded'],\n",
       " ['epidemic', 'epidemics'],\n",
       " ['bromance', 'bromances'],\n",
       " ['#calgary', '#calgaryexpo'],\n",
       " ['preserve', 'preserved', 'preserves', 'preserver'],\n",
       " ['rational', 'rationality', 'rationales', 'rationale'],\n",
       " ['enormous', 'enormously'],\n",
       " ['bachelor', 'bachelorette', \"bachelor's\"],\n",
       " ['scorpion', 'scorpions'],\n",
       " ['supplier', 'suppliers'],\n",
       " ['armenian', 'armenians'],\n",
       " ['ivanovic', \"ivanovic's\"],\n",
       " ['activist', 'activists'],\n",
       " ['sidewalk', 'sidewalks'],\n",
       " ['huhuhuhu', 'huhuhuhuhu'],\n",
       " ['manicure', 'manicures'],\n",
       " ['template', 'templates'],\n",
       " ['serenade', 'serenaded'],\n",
       " ['rotation', 'rotational', 'rotationally'],\n",
       " ['christie', \"christie's\"],\n",
       " ['distracted', 'distracting', 'distract', 'distractions'],\n",
       " ['disguise', 'disguised', 'disguises'],\n",
       " ['dickhead', 'dickheads'],\n",
       " ['forgiveness', 'forgiven'],\n",
       " ['chewable', 'chewables'],\n",
       " ['ensemble', 'ensembles'],\n",
       " ['jalapeno', 'jalapenos'],\n",
       " ['aromatic', 'aromatics'],\n",
       " ['#country', '#countrymusic', '#countrygirl', '#countrywisekitchen'],\n",
       " ['fractured', 'fracture', 'fractures'],\n",
       " ['meltdown', 'meltdowns'],\n",
       " ['governor', 'governors'],\n",
       " ['songbook', 'songbooks'],\n",
       " ['diagonal', 'diagonally'],\n",
       " ['flourish', 'flourished', 'flourishing'],\n",
       " ['brunette', 'brunettes', \"brunette's\"],\n",
       " ['everlast', 'everlasting', 'everlastingfans'],\n",
       " ['mcknight', \"mcknight's\"],\n",
       " ['subtweet', 'subtweeting', 'subtweets', 'subtweeted', 'subtweetin'],\n",
       " ['potencies', 'potencie'],\n",
       " ['eighteen', 'eighteenth'],\n",
       " ['bacteria', 'bacterial'],\n",
       " ['pamphlet', 'pamphlets'],\n",
       " ['following',\n",
       "  'followin',\n",
       "  'followingg',\n",
       "  'followinggg',\n",
       "  'followinq',\n",
       "  'followinqq'],\n",
       " ['cardigan', 'cardigans'],\n",
       " [\"mcdonald's\", 'mcdonalds', 'mcdonald'],\n",
       " ['stiletto', 'stilettos'],\n",
       " ['renegade', 'renegades'],\n",
       " ['collection',\n",
       "  'collections',\n",
       "  'collectible',\n",
       "  'collecting',\n",
       "  'collective',\n",
       "  'collectibles',\n",
       "  'collecti',\n",
       "  'collectio'],\n",
       " ['laminated', 'laminate'],\n",
       " ['#android', '#androidproblems'],\n",
       " ['democratic', 'democrat', 'democrats', 'democratization'],\n",
       " ['veronica', 'veronicas'],\n",
       " ['starships', 'starship'],\n",
       " ['anymoreee', 'anymoree'],\n",
       " ['vineyard', 'vineyards'],\n",
       " ['crumpler', \"crumpler's\"],\n",
       " ['frequently', 'frequent'],\n",
       " ['remastered', 'remaster', 'remastering'],\n",
       " ['firewall', 'firewalls'],\n",
       " ['coworker', 'coworkers'],\n",
       " ['generally', 'generall'],\n",
       " ['postponed', 'postpone', 'postpones'],\n",
       " ['scenario', 'scenarios'],\n",
       " ['repeated', 'repeatedly'],\n",
       " ['paradigm', 'paradigms'],\n",
       " ['#winning', '#winninggg', '#winningg'],\n",
       " ['teleport', 'teleportd', 'teleportation'],\n",
       " ['mistaken', 'mistakenly'],\n",
       " ['commando', 'commandos'],\n",
       " ['semantic', 'semantics'],\n",
       " ['particle', 'particles'],\n",
       " ['michaels', 'michaelson'],\n",
       " ['#twitter',\n",
       "  '#twitterpeopleilove',\n",
       "  '#twitterafterdark',\n",
       "  '#twitteroff',\n",
       "  '#twitterless',\n",
       "  '#twittercrush',\n",
       "  '#twitterproblems',\n",
       "  '#twitterpartytime',\n",
       "  '#twittergrouphug',\n",
       "  '#twitterparty',\n",
       "  '#twitterlove',\n",
       "  '#twitterjail',\n",
       "  '#twitter92',\n",
       "  '#twitterfriends',\n",
       "  '#twitterwhites',\n",
       "  '#twitterkurds'],\n",
       " ['cavaliers', 'cavalier'],\n",
       " ['oversized', 'oversize'],\n",
       " ['slipcase', 'slipcased'],\n",
       " ['abrasive', 'abrasives'],\n",
       " ['takeaway', 'takeaways'],\n",
       " ['ligament', 'ligaments'],\n",
       " ['manifold', 'manifolds'],\n",
       " ['nineteenth', 'nineteen'],\n",
       " ['definitely', 'definite'],\n",
       " ['blooming', 'bloomington'],\n",
       " ['prospects',\n",
       "  'prospect',\n",
       "  'prospectus',\n",
       "  'prospective',\n",
       "  'prospector',\n",
       "  'prospecting'],\n",
       " ['devotional', 'devotions', 'devotion', 'devotionals'],\n",
       " ['paycheck', 'paychecks'],\n",
       " ['takeover', 'takeoverchatter.com'],\n",
       " ['davidson', \"davidson's\"],\n",
       " ['distinctive',\n",
       "  'distinction',\n",
       "  'distinct',\n",
       "  'distinctions',\n",
       "  'distinctly',\n",
       "  'distinctively'],\n",
       " ['snapback', 'snapbacks'],\n",
       " ['rapunzel', \"rapunzel's\"],\n",
       " ['listening', 'listenin', 'listeninq', 'listeningg'],\n",
       " ['threatened', 'threatening', 'threatens', 'threaten'],\n",
       " ['generate', 'generated', 'generates'],\n",
       " ['distressed', 'distress', 'distressing', 'distressor', 'distresses'],\n",
       " ['teardrop', 'teardrops'],\n",
       " ['protocols', 'protocol'],\n",
       " ['waitress', 'waitresses'],\n",
       " ['granddad', 'granddaddy', \"granddad's\", 'granddads'],\n",
       " ['seperate', 'seperated', 'seperately'],\n",
       " ['dominate', 'dominated'],\n",
       " ['tonighttt', 'tonightt'],\n",
       " ['splinter', 'splinters', 'splintered'],\n",
       " ['traditional',\n",
       "  'tradition',\n",
       "  'traditions',\n",
       "  'traditionally',\n",
       "  'traditio',\n",
       "  'traditiona'],\n",
       " ['galactica', 'galactic'],\n",
       " ['zimbabwe', \"zimbabwe's\"],\n",
       " ['heirloom', 'heirlooms'],\n",
       " ['labrador', 'labradorite'],\n",
       " ['bisexual', 'bisexuality'],\n",
       " ['leathers', 'leatherskin'],\n",
       " ['relatives', 'relative'],\n",
       " ['felicity', \"felicity's\"],\n",
       " ['roadtrip', 'roadtrips'],\n",
       " ['#chelsea', '#chelseafc', '#chelseacanwinif'],\n",
       " ['electronics', 'electronic', 'electron', 'electronica', 'electronically'],\n",
       " ['indirect', 'indirects'],\n",
       " ['optimized', 'optimize'],\n",
       " ['equations', 'equation'],\n",
       " ['rhetoric', 'rhetorical'],\n",
       " ['pedicure', 'pedicures'],\n",
       " ['doughnuts', 'doughnut'],\n",
       " ['crankset', 'cranksets'],\n",
       " ['preacher', \"preacher's\", 'preachers'],\n",
       " ['withdrawal',\n",
       "  'withdrawals',\n",
       "  'withdraw',\n",
       "  'withdrawls',\n",
       "  'withdrawl',\n",
       "  'withdraws',\n",
       "  'withdrawn'],\n",
       " ['#praying', '#prayingforyou'],\n",
       " ['stuffing', 'stuffings'],\n",
       " ['cartridge', 'cartridges', 'cartridg'],\n",
       " ['simplify', 'simplifying'],\n",
       " ['sundress', 'sundresses'],\n",
       " ['retailer', 'retailers'],\n",
       " ['civilian', 'civilians'],\n",
       " ['salvador', 'salvadorian'],\n",
       " ['#excited',\n",
       "  '#excitedtweet',\n",
       "  '#excitedmuch',\n",
       "  '#exciteddd',\n",
       "  '#excitedforcallmyname',\n",
       "  '#excitedd'],\n",
       " ['ahahahaha',\n",
       "  'ahahahah',\n",
       "  'ahahahahaha',\n",
       "  'ahahahahah',\n",
       "  'ahahahahahah',\n",
       "  'ahahahahha',\n",
       "  'ahahahahahahahaha',\n",
       "  'ahahahahahahah'],\n",
       " ['prescott', \"prescott's\"],\n",
       " ['#reading', '#readingfc'],\n",
       " ['sprinkler', 'sprinkle', 'sprinkles'],\n",
       " ['#missher', '#misshersomuch'],\n",
       " ['pavement', 'pavements'],\n",
       " ['observer', 'observers', \"observer's\"],\n",
       " ['sorcerer', \"sorcerer's\", 'sorcerers'],\n",
       " ['#college',\n",
       "  '#collegeproblems',\n",
       "  '#collegelife',\n",
       "  '#collegekidproblems',\n",
       "  '#collegeprobz',\n",
       "  '#collegebound'],\n",
       " ['examiner', 'examiners'],\n",
       " ['bulgaria', 'bulgarian'],\n",
       " ['knockout', 'knockouts'],\n",
       " ['happiness', 'happines'],\n",
       " ['technical', 'technically', 'technica'],\n",
       " ['briefing', 'briefings'],\n",
       " ['asteroid', 'asteroids'],\n",
       " ['listener', 'listeners', \"listener's\"],\n",
       " ['stainless', 'stainles'],\n",
       " ['merchant', 'merchants'],\n",
       " ['bahahaha',\n",
       "  'bahahahaha',\n",
       "  'bahahahahaha',\n",
       "  'bahahahah',\n",
       "  'bahahahahahaha',\n",
       "  'bahahahahah'],\n",
       " ['seahorse', 'seahorses'],\n",
       " ['soulmate', 'soulmates'],\n",
       " ['dumbbell', 'dumbbells'],\n",
       " ['introduction',\n",
       "  'introducing',\n",
       "  'introduces',\n",
       "  'introduced',\n",
       "  'introductory',\n",
       "  'introductionthis',\n",
       "  'introductions',\n",
       "  'introduc',\n",
       "  'introductionthe',\n",
       "  'introducti',\n",
       "  'introduct',\n",
       "  'introductio',\n",
       "  'introducin'],\n",
       " ['skincare', 'skincarerx'],\n",
       " ['xoxoxoxo', 'xoxoxoxox', 'xoxoxoxoxox', 'xoxoxoxoxoxoxo', 'xoxoxoxoxoxo'],\n",
       " ['stephens', 'stephenson'],\n",
       " ['#fashion', '#fashionstar'],\n",
       " ['diplomat', 'diplomatic', 'diplomats'],\n",
       " ['fraction', 'fractions', 'fractional'],\n",
       " ['outsiders', 'outsider'],\n",
       " ['educators', 'educator'],\n",
       " ['diabetic', 'diabetics'],\n",
       " ['bookcase', 'bookcases'],\n",
       " ['singular', 'singularity'],\n",
       " ['employer', 'employers', \"employer's\"],\n",
       " ['goodnite', 'goodnites', 'goodnitee'],\n",
       " ['billiard', 'billiards'],\n",
       " ['overdose', 'overdosed'],\n",
       " ['ethiopia', 'ethiopian'],\n",
       " ['griffith', 'griffiths'],\n",
       " ['diaspora', 'diasporas'],\n",
       " ['#craving', '#cravings'],\n",
       " ['dictator', 'dictatorship', 'dictators'],\n",
       " ['succinct', 'succinctly'],\n",
       " ['richardson', 'richards'],\n",
       " ['subtitles', 'subtitled', 'subtitle'],\n",
       " ['organise', 'organised', 'organisers'],\n",
       " ['shamrock', 'shamrocks'],\n",
       " ['nutrient', 'nutrients'],\n",
       " ['duckling', 'ducklings'],\n",
       " ['bankruptcy', 'bankrupt'],\n",
       " ['backwards', 'backward'],\n",
       " ['infusion', 'infusions'],\n",
       " ['lucchese', \"lucchese's\"],\n",
       " ['wearable', 'wearables'],\n",
       " ['information',\n",
       "  'informative',\n",
       "  'informatics',\n",
       "  'informat',\n",
       "  'informatio',\n",
       "  'informati',\n",
       "  'informationin',\n",
       "  'informational',\n",
       "  'informationthe'],\n",
       " ['memorise', 'memorised'],\n",
       " ['parental', 'parentals'],\n",
       " ['emoticons', 'emoticon'],\n",
       " ['#scorpio', '#scorpios', \"#scorpio's\"],\n",
       " ['outbreak', 'outbreaks'],\n",
       " ['abundant', 'abundantly'],\n",
       " ['published', 'publisher', 'publishers', 'publishe', 'publishes'],\n",
       " ['beautiful',\n",
       "  'beautifully',\n",
       "  'beautifu',\n",
       "  'beautifull',\n",
       "  'beautifulll',\n",
       "  'beautifuls',\n",
       "  'beautifulest'],\n",
       " ['moccasins', 'moccasin'],\n",
       " ['meatballs', 'meatball'],\n",
       " ['metaphor', 'metaphors'],\n",
       " ['#awesome', '#awesomeness'],\n",
       " ['spanking', 'spankings'],\n",
       " ['landlord', 'landlords', \"landlord's\"],\n",
       " ['chocolate', 'chocolates', 'chocolat', 'chocolatier', 'chocolatey'],\n",
       " ['spreader', 'spreaders'],\n",
       " ['squadron', 'squadrons'],\n",
       " ['charismatic', 'charisma'],\n",
       " ['jonghyun', \"jonghyun's\"],\n",
       " ['motorway', 'motorways'],\n",
       " ['kourtney', \"kourtney's\"],\n",
       " ['jeremiah', \"jeremiah's\"],\n",
       " [\"everyman's\", 'everyman'],\n",
       " ['performance', 'performances', 'performa', 'performanc', 'performan'],\n",
       " ['slovenia', 'slovenian'],\n",
       " ['sabotage', 'sabotaged'],\n",
       " ['mongolia', 'mongolian', 'mongoliad'],\n",
       " ['waterproof', 'waterpro', 'waterproofing', 'waterproofs'],\n",
       " ['energizer', 'energize'],\n",
       " ['lovelies', 'loveliest'],\n",
       " ['harmonica', 'harmonic'],\n",
       " ['polisher', 'polishers'],\n",
       " ['essentials', 'essential', 'essentially', 'essentia'],\n",
       " ['delusion', 'delusions'],\n",
       " ['handicap', 'handicapped', 'handicapping'],\n",
       " ['homegirl', 'homegirls'],\n",
       " ['politico', 'politicoann'],\n",
       " ['skateboard',\n",
       "  'skateboarding',\n",
       "  'skateboards',\n",
       "  'skateboa',\n",
       "  'skateboar',\n",
       "  'skateboarders',\n",
       "  'skateboarder'],\n",
       " ['#loveher', '#lovehersomuch'],\n",
       " ['rigorous', 'rigorously'],\n",
       " ['illustrated',\n",
       "  'illustrations',\n",
       "  'illustrator',\n",
       "  'illustration',\n",
       "  'illustrates',\n",
       "  'illustrate',\n",
       "  'illustra',\n",
       "  'illustrators',\n",
       "  'illustrat',\n",
       "  'illustrating',\n",
       "  'illustrative',\n",
       "  \"illustrator's\"],\n",
       " ['irritated', 'irritates', 'irritate'],\n",
       " ['#special', '#specialolympics', '#specialshoutout'],\n",
       " ['gingrich', \"gingrich's\"],\n",
       " ['uglydoll', 'uglydolls'],\n",
       " ['painfully', 'painfull'],\n",
       " ['confronting', 'confront', 'confrontation', 'confronts'],\n",
       " ['#typical', '#typicalloveproblem'],\n",
       " ['retriever', 'retrieve', 'retrievers'],\n",
       " ['coverall', 'coveralls'],\n",
       " ['disciple', 'discipleship', 'disciples'],\n",
       " ['elective', 'electives'],\n",
       " ['congratz', 'congratzz'],\n",
       " ['manifesto', 'manifest', 'manifesting', 'manifestation', 'manifested'],\n",
       " ['trimming', 'trimmings'],\n",
       " ['melville', \"melville's\"],\n",
       " ['professional',\n",
       "  'professionals',\n",
       "  'profession',\n",
       "  'professionally',\n",
       "  'professi',\n",
       "  'professiona',\n",
       "  'professio',\n",
       "  'professions',\n",
       "  \"professional's\"],\n",
       " ['clippings', 'clipping'],\n",
       " ['crusader', 'crusaders'],\n",
       " ['specimen', 'specimens'],\n",
       " ['regiment', 'regimented'],\n",
       " ['especially', 'especial'],\n",
       " ['hahahhaa', 'hahahhaah'],\n",
       " ['marketer', 'marketers'],\n",
       " ['redefined', 'redefine', 'redefines'],\n",
       " ['daydream', 'daydreaming', 'daydreamer'],\n",
       " ['gardeners', 'gardener', \"gardener's\"],\n",
       " ['important', 'importance', 'importantly', 'importan'],\n",
       " ['eloquent', 'eloquently'],\n",
       " ['character',\n",
       "  'characters',\n",
       "  'characteristics',\n",
       "  'characte',\n",
       "  'characteristic',\n",
       "  'characterized',\n",
       "  'characterization',\n",
       "  'characterisation'],\n",
       " ['donations', 'donation'],\n",
       " ['photography',\n",
       "  'photographs',\n",
       "  'photograph',\n",
       "  'photographer',\n",
       "  'photographic',\n",
       "  'photographers',\n",
       "  \"photographer's\",\n",
       "  'photographing',\n",
       "  'photogra',\n",
       "  'photographed',\n",
       "  'photograp'],\n",
       " ['sighting', 'sightings'],\n",
       " ['nominated', 'nominate'],\n",
       " ['olympian', 'olympians'],\n",
       " ['#missingyou',\n",
       "  '#missingout',\n",
       "  '#missing',\n",
       "  '#missinghim',\n",
       "  '#missingher',\n",
       "  '#missingit'],\n",
       " ['amazinggg', 'amazingg'],\n",
       " ['windmill', 'windmills'],\n",
       " ['practical', 'practically', 'practica', 'practicality'],\n",
       " ['cannibal', 'cannibals'],\n",
       " ['revolver', 'revolvers'],\n",
       " ['sculptor', 'sculptors'],\n",
       " ['dispatch', 'dispatches', 'dispatched', 'dispatcher'],\n",
       " ['reproduction',\n",
       "  'reproduced',\n",
       "  'reproduc',\n",
       "  'reproductions',\n",
       "  'reproductive',\n",
       "  'reproducti',\n",
       "  'reproducible',\n",
       "  'reproduct',\n",
       "  'reproduces',\n",
       "  'reproductio',\n",
       "  'reproducing'],\n",
       " ['osbourne', 'osbournes'],\n",
       " ['different',\n",
       "  'difference',\n",
       "  'differences',\n",
       "  'differential',\n",
       "  'differen',\n",
       "  'differentiate',\n",
       "  'differentiation',\n",
       "  'differenc'],\n",
       " ['paraguayan', 'paraguay'],\n",
       " ['#foreveralone',\n",
       "  '#forever',\n",
       "  '#foreversad',\n",
       "  '#foreversingle',\n",
       "  '#foreverunnoticed',\n",
       "  '#foreverproudof1d',\n",
       "  '#foreverandalways',\n",
       "  '#foreversupporting1d'],\n",
       " ['analytical', 'analytics', 'analytic'],\n",
       " ['teammates', 'teammate'],\n",
       " ['pleassse', 'pleassseee', 'pleasssee'],\n",
       " ['realllyyy', 'realllyy'],\n",
       " ['estimates', 'estimated', 'estimate'],\n",
       " ['decrease', 'decreased', 'decreases'],\n",
       " ['animator', 'animators'],\n",
       " ['talisman', 'talismans'],\n",
       " ['fleximaps', 'fleximap'],\n",
       " ['#shootme', '#shootmenow'],\n",
       " ['salesman', \"salesman's\"],\n",
       " ['adventure',\n",
       "  'adventures',\n",
       "  'adventurous',\n",
       "  'adventurer',\n",
       "  'adventur',\n",
       "  'adventurers',\n",
       "  'adventurerating',\n",
       "  'adventura',\n",
       "  'adventuring'],\n",
       " ['misplaced', 'misplace'],\n",
       " ['components', 'component', 'componen'],\n",
       " ['artifact', 'artifacts'],\n",
       " ['#germany', '#germanywantsolly'],\n",
       " ['development', 'developments', 'developmental', 'developm', 'developme'],\n",
       " ['shoelace', 'shoelaces'],\n",
       " ['splatter', 'splattered'],\n",
       " ['minimize', 'minimized'],\n",
       " ['botanicals', 'botanical', 'botanica', 'botanicare'],\n",
       " ['#amazing', '#amazingfriends'],\n",
       " ['experience',\n",
       "  'experienced',\n",
       "  'experiences',\n",
       "  'experiencing',\n",
       "  'experienc',\n",
       "  'experien'],\n",
       " ['reinventing', 'reinvent', 'reinvented', 'reinvention'],\n",
       " ['alrighty', 'alrightyy', 'alrightyyy'],\n",
       " ['asperger', \"asperger's\"],\n",
       " ['playmate', 'playmates'],\n",
       " ['reviewer', 'reviewers'],\n",
       " ['compiler', 'compilers'],\n",
       " ['compatible', 'compatibility', 'compatibl', 'compatib'],\n",
       " ['hardship', 'hardships'],\n",
       " ['replacement', 'replaceme', 'replacemen', 'replacem', 'replacements'],\n",
       " ['hermetic', 'hermetics'],\n",
       " ['magister', 'magisterial'],\n",
       " ['existent', 'existential', 'existentialism'],\n",
       " ['thinkcentre', 'thinkcent', 'thinkcen', 'thinkcentr'],\n",
       " ['resistant', 'resistance', 'resistan'],\n",
       " ['mainstay', 'mainstays'],\n",
       " ['shortcuts', 'shortcut'],\n",
       " ['closeouts', 'closeout'],\n",
       " ['hesitate', 'hesitated'],\n",
       " ['periodical', 'periodic', 'periodicals'],\n",
       " ['lightweight', 'lightwei', 'lightweigh', 'lightweig', 'lightweights'],\n",
       " ['monumental', 'monuments', 'monument'],\n",
       " ['multiply', 'multiplying'],\n",
       " ['himalayan', 'himalaya', 'himalayas'],\n",
       " ['morninggg', 'morningg'],\n",
       " ['bodywork', 'bodyworks'],\n",
       " ['thaaanks', 'thaaanksss', 'thaaankss'],\n",
       " ['bookshop', 'bookshops'],\n",
       " ['sponsors', 'sponsorship'],\n",
       " ['foreverrr', 'foreverr'],\n",
       " ['vigorous', 'vigorously'],\n",
       " ['smoothes', 'smoothest'],\n",
       " ['anatomical', 'anatomic', 'anatomically'],\n",
       " ['#support', '#supportbaloch'],\n",
       " ['puncture', 'punctured'],\n",
       " ['#perfect',\n",
       "  '#perfection',\n",
       "  '#perfectionbaby',\n",
       "  '#perfectday',\n",
       "  '#perfectnight',\n",
       "  '#perfectly'],\n",
       " ['#arsenal', '#arsenaltillidie'],\n",
       " ['protective',\n",
       "  'protection',\n",
       "  'protecting',\n",
       "  'protectio',\n",
       "  'protecti',\n",
       "  'protectiv'],\n",
       " ['sediment', 'sedimentary', 'sedimentology'],\n",
       " ['spiritual', 'spirituality', 'spiritually', 'spiritua', 'spirituals'],\n",
       " ['tolerate', 'tolerated'],\n",
       " ['casecrown', 'casecrow'],\n",
       " ['polyform', \"polyform's\"],\n",
       " ['midfield', 'midfielder'],\n",
       " ['newcomer', 'newcomers', \"newcomer's\"],\n",
       " ['godbless', 'godblessyou'],\n",
       " ['autopole', 'autopoles'],\n",
       " ['fiendish', 'fiendishly'],\n",
       " ['comprised', 'comprises', 'comprise'],\n",
       " ['featuring', 'featurin'],\n",
       " ['spectral', 'spectralink'],\n",
       " ['chipping', 'chippings'],\n",
       " ['dumplings', 'dumpling'],\n",
       " ['ahhahaha', 'ahhahahaha'],\n",
       " ['microstar', 'microsta', 'microstation'],\n",
       " ['diagnosed', 'diagnose', 'diagnoses'],\n",
       " ['dortmund', 'dortmunder'],\n",
       " ['thunderstorm', 'thunderstorms', 'thunders'],\n",
       " ['nocturne', 'nocturnes'],\n",
       " ['theorist', 'theorists'],\n",
       " ['initiate', 'initiated'],\n",
       " ['dissolve', 'dissolved', 'dissolves'],\n",
       " ['coincidence', 'coincide', 'coincides'],\n",
       " ['sophmore', 'sophmores'],\n",
       " ['thatcher', 'thatchers'],\n",
       " ['industrial', 'industries', 'industria', 'industri'],\n",
       " ['christian',\n",
       "  'christianity',\n",
       "  'christians',\n",
       "  'christia',\n",
       "  'christianae',\n",
       "  'christiane',\n",
       "  \"christianity's\"],\n",
       " ['historian', 'historians', 'historia', 'historias', 'historiatus'],\n",
       " ['including', 'includin'],\n",
       " ['daffodil', 'daffodils'],\n",
       " ['endeavor', 'endeavors'],\n",
       " ['outsmart', 'outsmarted'],\n",
       " ['hardwired', 'hardwire'],\n",
       " ['innovative',\n",
       "  'innovation',\n",
       "  'innovations',\n",
       "  'innovati',\n",
       "  'innovativ',\n",
       "  'innovatively'],\n",
       " ['mysterious', 'mysteriously', 'mysterio'],\n",
       " ['maryjane', 'maryjanesfarm'],\n",
       " ['barrette', 'barrettes'],\n",
       " ['everything',\n",
       "  \"everything's\",\n",
       "  'everythings',\n",
       "  'everythin',\n",
       "  'everythi',\n",
       "  'everythingg',\n",
       "  'everythinq',\n",
       "  'everything.stay',\n",
       "  'everythink'],\n",
       " ['proactive', 'proactiv'],\n",
       " ['turnover', 'turnovers'],\n",
       " ['generation',\n",
       "  'generations',\n",
       "  'generatio',\n",
       "  'generating',\n",
       "  'generati',\n",
       "  'generational',\n",
       "  'generative'],\n",
       " ['#friends', '#friendship', '#friendsofricki'],\n",
       " ['investigation',\n",
       "  'investigations',\n",
       "  'investigator',\n",
       "  'investigates',\n",
       "  'investigating',\n",
       "  'investigate',\n",
       "  'investigative',\n",
       "  'investig',\n",
       "  'investigators',\n",
       "  'investiga',\n",
       "  'investigated'],\n",
       " ['bestselling',\n",
       "  'bestseller',\n",
       "  'bestsellers',\n",
       "  'bestsell',\n",
       "  'bestselle',\n",
       "  'bestsellin',\n",
       "  'bestselli'],\n",
       " ['truthful', 'truthfully'],\n",
       " ['entertainment',\n",
       "  'entertaining',\n",
       "  'entertain',\n",
       "  'entertained',\n",
       "  'entertainer',\n",
       "  'entertainments',\n",
       "  'entertainme',\n",
       "  'entertainm',\n",
       "  'entertainmen',\n",
       "  'entertai',\n",
       "  'entertainers',\n",
       "  'entertains'],\n",
       " ['roleplaying', 'roleplay', 'roleplayer'],\n",
       " ['scandyna', \"scandyna's\"],\n",
       " ['orthotics', 'orthotic'],\n",
       " ['jealousss', 'jealouss'],\n",
       " ['phonetics', 'phonetic'],\n",
       " ['understand',\n",
       "  'understanding',\n",
       "  'understands',\n",
       "  'understatement',\n",
       "  'understandable',\n",
       "  'understated',\n",
       "  'understa',\n",
       "  'understan',\n",
       "  'understandi'],\n",
       " ['woodworking', 'woodwork', 'woodworkers', 'woodworker', \"woodworker's\"],\n",
       " ['crunchie', 'crunchies'],\n",
       " ['recommended',\n",
       "  'recommend',\n",
       "  'recommendations',\n",
       "  'recommendation',\n",
       "  'recommends',\n",
       "  'recommen',\n",
       "  'recommending'],\n",
       " ['catapult', 'catapulted'],\n",
       " ['gulliver', \"gulliver's\"],\n",
       " ['effective', 'effectively', 'effectiveness', 'effectiv'],\n",
       " ['#thunder', '#thunderup'],\n",
       " ['additives', 'additive'],\n",
       " ['wheelock', \"wheelock's\"],\n",
       " ['finallly', 'finalllyyy'],\n",
       " ['snowshoe', 'snowshoes'],\n",
       " ['glycerin', 'glycerine'],\n",
       " ['#forreal', '#forrealthough', '#forrealtho'],\n",
       " ['ancestors', 'ancestor'],\n",
       " ['annyeong', 'annyeonghaseyo'],\n",
       " ['scribbles', 'scribble'],\n",
       " ['rhodesia', 'rhodesian'],\n",
       " ['repierce', 'repierced'],\n",
       " ['transmitter', 'transmit', 'transmitted', 'transmits'],\n",
       " ['#verysad', '#verysadtweet'],\n",
       " ['individual',\n",
       "  'individually',\n",
       "  'individuall',\n",
       "  'individuals',\n",
       "  'individuality',\n",
       "  'individu',\n",
       "  'individua'],\n",
       " ['projector', 'projectors', 'projecto'],\n",
       " ['flapjack', 'flapjacks'],\n",
       " ['screaming', 'screamin'],\n",
       " ['mapguide', 'mapguides'],\n",
       " ['horsemanship', 'horseman'],\n",
       " ['bleachers', 'bleacher'],\n",
       " ['chaplain', 'chaplains', 'chaplaincy'],\n",
       " ['pahahaha', 'pahahahaha', 'pahahahahaha'],\n",
       " ['construction',\n",
       "  'constructed',\n",
       "  'construct',\n",
       "  'constructing',\n",
       "  'construc',\n",
       "  'constructio',\n",
       "  'constructi',\n",
       "  'constructivism',\n",
       "  'constructe',\n",
       "  'constructs'],\n",
       " ['connection',\n",
       "  'connections',\n",
       "  'connecting',\n",
       "  'connecticut',\n",
       "  'connectivity',\n",
       "  'connecti',\n",
       "  'connectin',\n",
       "  'connectio'],\n",
       " ['available', 'available.genre', 'availableno', 'availablemedia', 'availabl'],\n",
       " ['description',\n",
       "  'descriptive',\n",
       "  'descriptions',\n",
       "  'descriptio',\n",
       "  'descript',\n",
       "  'descripti',\n",
       "  'descriptionthis'],\n",
       " ['hillside', 'hillsides'],\n",
       " ['international',\n",
       "  'internationally',\n",
       "  'internati',\n",
       "  'internat',\n",
       "  'internatio',\n",
       "  'internationa',\n",
       "  'internation'],\n",
       " ['sensitive', 'sensitivity', 'sensitiv'],\n",
       " ['excellent', 'excellence', 'excellen'],\n",
       " ['kotobukiya', 'kotobuki'],\n",
       " ['condition',\n",
       "  'conditioner',\n",
       "  'conditions',\n",
       "  'conditioning',\n",
       "  'conditio',\n",
       "  'conditioners'],\n",
       " ['transport',\n",
       "  'transportation',\n",
       "  'transported',\n",
       "  'transporter',\n",
       "  'transports',\n",
       "  'transpor',\n",
       "  'transporting'],\n",
       " ['ingredients', 'ingredient', 'ingredie'],\n",
       " ['restricted', 'restrictions', 'restrict', 'restricting', 'restricts'],\n",
       " ['beatiful', 'beatifully', 'beatifull'],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are words (filtered by length) whose beginning match another word close to this word?\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "\n",
    "#We visualize which word would be mapped on which one \n",
    "filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expected syntaxic divergences\n",
    "As expected, words like \"haha\" can be written in many ways (extended length, typo..). We know that they mean the same,it is therefore interesting to spot them in order to later stem them to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haha',\n",
       " 'hahaha',\n",
       " 'hahah',\n",
       " 'hahahaha',\n",
       " 'ahaha',\n",
       " 'ahahaha',\n",
       " 'hahaa',\n",
       " 'hahahahaha']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words beginning with \"haha\" or \"ahah\" are sure to be \"instances\" of haha\n",
    "word_haha = test_df.loc[test_df.word.str.startswith(\"haha\") | test_df.word.str.startswith(\"ahah\")]\n",
    "\n",
    "\n",
    "#print(\"Occurence of the words that can be remplaced by haha = \"+ str(int(word_haha.occurence.sum()) - int(word_haha[word_haha.word == \"haha\"].occurence)))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)\n",
    "list(word_haha.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Study duplicates\n",
    "We noticed that several tweets were duplicated in test, pos and neg tweets. We study ho many they are, and which tweets (if any) are shared between the test set and the pos/neg set.\n",
    "We noticed after running that there are no tweets shared between all tweet sets, thus we can label duplicate of the test dataset by directly looking for corresponding duplicates in pos/neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 %% % %% %% %% %%%% % % % % %% % % % % %% % %% % % %%% % % %% % % % %% %% %%%%%% % % % % %% % %% % %%% %% % % % %%%% % % % %% %% % %% % %%% %% % % %% % %% % %%% % % %% % % % % % %% % %% %% %% %% % % %% %%%% %%% %% % %% % % %% % % % % % %% %%% %%% % %% % % % %% % % % %% %% % % % % % %% % %%%% %% % %%%% %% % % % % %%% % % % % %%% %%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-5a4cc4c89d6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mlen_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_tweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mpos_tweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtest_tweet\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpos_tweet\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mcommon_pos_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "common_pos_test = []\n",
    "common_neg_test = []\n",
    "\n",
    "pos = sorted(list(set(pos_tweets)))\n",
    "neg = sorted(list(set(neg_tweets)))\n",
    "test_tweets = sorted(list(set(test_tweets)))\n",
    "len_test = len(test_tweets)\n",
    "for i, test_tweet in enumerate(test_tweets) :\n",
    "    for pos_tweet in pos :\n",
    "        if test_tweet == pos_tweet : \n",
    "            common_pos_test.append(test_tweet)\n",
    "            print(\"duplicate pos-test\")\n",
    "            print(\"\\ttweet :\" + str(pos_tweet))  \n",
    "            break\n",
    "    for neg_tweet in neg:\n",
    "        if test_tweet == neg_tweet : \n",
    "            common_neg_test.append(test_tweet)\n",
    "            print(\"duplicate neg-data\")\n",
    "            print(\"\\ttweet :\" + str(pos_tweet))\n",
    "            break\n",
    "    print(\"{:.1f}\".format(i/len_test), \"%\", end='\\r')\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occurences(tweet, tweets):\n",
    "    count = 0\n",
    "    for t in tweets:\n",
    "        if t == tweet:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 data pre-processing\n",
    "In this part we start from raw tweets and preprocess them to achieve uniformization of the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Standardization\n",
    "We start by removing repetitions of more than 3 letters, and replace dots by spaces\n",
    "ex : \n",
    "\n",
    "1) `no_dot` : \"Funny.I\" becomes \"Funny I\" \n",
    "\n",
    "2) `find_repetitions` : \"I looooove\" becomes \"I love\"\n",
    "\n",
    "3) `no_s` : \"dummies\" becomes \"dummy\" and \"carresses\" becomes \"carresse\"\n",
    "            \n",
    "\n",
    "This is done by the function `standardize_tweets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data pos\n",
      "Process data neg\n",
      "Process data test\n"
     ]
    }
   ],
   "source": [
    "#standardize tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = standardize_tweets(pos)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = standardize_tweets(neg) \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = standardize_tweets(data) \n",
    "\n",
    "\n",
    "export(processed_tweets_pos_full,  \"preprocessed_pos_full\")\n",
    "export(processed_tweets_neg_full,  \"preprocessed_neg_full\")\n",
    "export(processed_tweets_test_full, \"preprocessed_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the vocabs\n",
    "Using shell commands `vocab_cut`, we build vocabs for positive, negative and test tweets. We also implemented it in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing items present less than 5 times % %%%%%% %% % % % %% % % %%%\n",
      "Time elpased (hh:mm:ss.ms) 0:05:02.579878\n",
      "removing items present less than 5 times % % %% % %%% % %% % % %% % %% % % %% %%% %% % %%%% % %% %%%%%%%%% % % % %%% %% %% % % %% %%% % % %% %% %% %%% % % %% %% %%%%%% % % % %% %% % % % % % % % %\n",
      "Time elpased (hh:mm:ss.ms) 0:05:41.032892\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:02.932122\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-f4e55e292e32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#build vocabs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwrite_vocab_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"preprocessed_vocab_pos_full\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mwrite_vocab_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"preprocessed_vocab_neg_full\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwrite_vocab_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"preprocessed_vocab_test_full\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\IOTweets.py\u001b[0m in \u001b[0;36mwrite_vocab_to_file\u001b[1;34m(vocab_counter, dest_file_name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#----------------------------import_without_id(path)------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#import tweets written in file stocked under given path as an array of tweets (string), but remove the id in file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;31m# path : the path of the file in which the tweets are written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_without_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "#build word counters\n",
    "vocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#build vocabs\n",
    "write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"preprocessed_vocab_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build and Merge the dataframes \n",
    "We build dataframes containing several informations for each type of tweets, and we combine them all in order to facilitate the stemming we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pos_full = \"preprocessed_vocab_pos_full\"\n",
    "path_neg_full = \"preprocessed_vocab_neg_full\"\n",
    "path_test_data = \"preprocessed_vocab_test_full\"\n",
    "\n",
    "    \n",
    "pos_full =  build_df(path_pos_full)\n",
    "neg_full =  build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build semantic equivalences\n",
    "We define which word is equivalent to each word (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building semantic\")\n",
    "#Build equivalence list\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "print(\"Building haha semantic\")\n",
    "#build \"haha\" equivalences\n",
    "word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    \n",
    "word_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\n",
    "word_haha_list.remove(\"haha\")\n",
    "word_haha_list.insert(0, \"haha\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"filter semantic\")\n",
    "#append the two semantic lists and filter them\n",
    "same_begin.append(list(word_haha_list))\n",
    "semantic = filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Map the equivalent words, stem them all thanks to nltk and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d'un tableau de tweet   \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"cleaned_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"cleaned_pos_full\")\n",
    "export(processed_tweets_test_full, \"cleaned_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the simple vocab (with occurences) to build the dataframes\n",
    "This is done by using shell commands or python code, as presented before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build word counters\n",
    "vocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#build vocabs\n",
    "write_vocab_to_file(vocab_pos, \"cleaned_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"cleaned_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"cleaned_vocab_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 create relevant vocab \n",
    "We keep pertinent words in a special vocab called relevant vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataframes before we \n",
    "    neg_df = build_df(\"cleaned_vocab_neg_full\")\n",
    "    pos_df = build_df(\"cleaned_vocab_pos_full\")\n",
    "    \n",
    "    #Merge dataframes\n",
    "    merged = merging(neg_df, pos_df, False)\n",
    "    \n",
    "    #create relevant vocab\n",
    "    create_relevant_vocab(pertinence_thres_relevant=0.3, min_count_relevant=300, merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Find characteristic words\n",
    "We set a list of words that appear only in neg/pos at least min_diff times such that if we see such a word in a tweet we assume it is a neg/pos tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets = import_(\"cleaned_test_full\")\n",
    "\n",
    "characteristic_words(data_tweets, merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Recapitulative Function\n",
    "This function does the work presented above, we created them for clarity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'import_without_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2cf2aed52c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcharacteristic_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_pos.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train_neg.txt\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"test_data.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_full\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpertinence_thres_relevant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count_relevant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2cf2aed52c43>\u001b[0m in \u001b[0;36mclean_tweets\u001b[1;34m(path_pos, path_neg, path_test, is_full, pertinence_thres_relevant, min_count_relevant, cut_threshold)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#path pos, path_neg, path_data : paths of the raw tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#is_full : True iff we clean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprocess_data_no_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\ProcessTweets.py\u001b[0m in \u001b[0;36mprocess_data_no_stem\u001b[1;34m(path_pos, path_neg, path_test, is_full, cut_threshold)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Import Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;31m#import tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0mtweets_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_without_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[0mtweets_pos\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mtweets_neg\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'import_without_id' is not defined"
     ]
    }
   ],
   "source": [
    "#clean raw tweets\n",
    "def clean_tweets(path_pos, path_neg, path_test, is_full,  pertinence_thres_relevant, min_count_relevant, cut_threshold=5):\n",
    "    #path pos, path_neg, path_data : paths of the raw tweets\n",
    "    #is_full : True iff we clean \n",
    "    process_data_no_stem(path_pos, path_neg, path_test, is_full, cut_threshold)\n",
    "    stemming(is_full, cut_threshold)\n",
    "   \n",
    "    #Load dataframes before we \n",
    "    neg_df = build_df(\"cleaned_vocab_neg\")\n",
    "    pos_df = build_df(\"cleaned_vocab_pos\")\n",
    "    \n",
    "    #Merge dataframes\n",
    "    merged = merging(neg_df, pos_df, False)\n",
    "    \n",
    "    #create relevant vocab\n",
    "    create_relevant_vocab(pertinence_thres_relevant, min_count_relevant, merged)\n",
    "    \n",
    "    data_tweets = import_(\"cleaned_test\")\n",
    "    \n",
    "    #create characteristic_words\n",
    "    characteristic_words(data_tweets, merged)\n",
    "    \n",
    "clean_tweets(\"train_pos.txt\", \"train_neg.txt\" , \"test_data.txt\", is_full=False, pertinence_thres_relevant=0.15, min_count_relevant=250)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
