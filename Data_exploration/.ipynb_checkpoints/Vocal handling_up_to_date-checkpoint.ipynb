{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data one is given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple words do we have, are there some duplicates, is there any odds with the tweets? How are they classified (we are given two datasets, one for the positive tweets one for negatives) i.e. what is a typical word that occurs when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may contain words that are not usefull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than three following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Helper functions and files\n",
    "\n",
    "IOTweets contains the functions :\n",
    "- build_df\n",
    "- import_\n",
    "- export\n",
    "\n",
    "ProcessTweets contains the functions :\n",
    "- merging\n",
    "- filter_single_rep\n",
    "- powerful_words\n",
    "- sem_by_repr\n",
    "- sem_by_repr2\n",
    "- no_dot\n",
    "- find_repetition\n",
    "- no_s\n",
    "- stem_tweet\n",
    "- set_min_diff\n",
    "- contains\n",
    "- process_tweets_1\n",
    "- process_tweets_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import csv\n",
    "#%pylab inline # depreceated, use individual imports instead\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_pos = \"vocab_cut_pos.txt\"\n",
    "vocab_neg = \"vocab_cut_neg.txt\"\n",
    "\n",
    "vocab_pos_full = \"vocab_cut_pos_full.txt\"\n",
    "vocab_neg_full = \"vocab_cut_neg_full.txt\"\n",
    "vocab_test = \"vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos = \"train_pos.txt\"\n",
    "path_tweets_neg = \"train_neg.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"train_neg_full.txt\"\n",
    "path_tweets_test = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An insight into the dataset \n",
    "We begin by analysing the non-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_vocab(tweets, cut_threshold, file_name , bitri):\n",
    "    counter = build_vocab_counter(tweets, cut_threshold, bitri)\n",
    "    write_vocab_to_file(counter, (file_name + \"_cut=\" +str(cut_threshold) +\"_bitri=\"+str(bitri)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:13.013195\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:15.200280\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:01.326959\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:02:38.391326\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:03:03.602309\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '161 1000'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-24840a03fcc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# build the vocab Dataframe (DF) for full positive and negative tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mpos_full_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw_vocab_pos_full_cut=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_bitri=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mneg_full_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw_vocab_neg_full_cut=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_bitri=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\IOTweets.py\u001b[0m in \u001b[0;36mbuild_df\u001b[1;34m(filepath, bitri)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#build the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"occurence\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;31m# arg is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m             \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[1;32mpandas\\_libs\\src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas\\_libs\\lib.c:66440)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\IOTweets.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#build the dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"occurence\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '161 1000'"
     ]
    }
   ],
   "source": [
    "#import tweets\n",
    "pos_tweets = import_(path_tweets_pos)\n",
    "neg_tweets = import_(path_tweets_neg)\n",
    "pos_full_tweets = import_(path_tweets_pos_full)\n",
    "neg_full_tweets = import_(path_tweets_neg_full)\n",
    "test_tweets = import_without_id(path_tweets_test)\n",
    "\n",
    "#Create vocabs\n",
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "write_vocab(pos_tweets, cut_threshold, \"raw_vocab_pos\" , bitri)\n",
    "write_vocab(neg_tweets, cut_threshold, \"raw_vocab_neg\" , bitri)\n",
    "write_vocab(test_tweets, cut_threshold, \"raw_vocab_test\" , bitri)\n",
    "write_vocab(pos_full_tweets, cut_threshold, \"raw_vocab_pos_full\" , bitri)\n",
    "write_vocab(neg_full_tweets, cut_threshold, \"raw_vocab_neg_full\" , bitri)\n",
    "\n",
    "\n",
    "# build the vocab Dataframe (DF) for full positive and negative tweets\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "# build the vocab Dataframe (DF) for partial positive and negative tweets\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bitri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-28f7bcc6cab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcut_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpos_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw_vocab_pos_cut=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_bitri=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mneg_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw_vocab_neg_cut=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_bitri_=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpos_full_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"raw_vocab_pos_full_cut=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_bitri=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbitri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bitri' is not defined"
     ]
    }
   ],
   "source": [
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri_=\"+str(bitri), bitri)\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Study of word pertinence \n",
    "The idea is that the greater a word ratio i, the more pertinent it will be to cosider it when trying to label a tweet that contains it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt;</td>\n",
       "      <td>578374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>544909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>537656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>434097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;url&gt;</td>\n",
       "      <td>427974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>...</td>\n",
       "      <td>413867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>402968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>!</td>\n",
       "      <td>368079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>362361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>to</td>\n",
       "      <td>356966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>256250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>and</td>\n",
       "      <td>242747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>of</td>\n",
       "      <td>220383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>you</td>\n",
       "      <td>200754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my</td>\n",
       "      <td>198536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>179956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>in</td>\n",
       "      <td>168647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>me</td>\n",
       "      <td>159073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-</td>\n",
       "      <td>157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"</td>\n",
       "      <td>150617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td>148608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>?</td>\n",
       "      <td>140983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>this</td>\n",
       "      <td>139045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>:</td>\n",
       "      <td>126285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>it</td>\n",
       "      <td>122849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>with</td>\n",
       "      <td>107030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>frame</td>\n",
       "      <td>102308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>so</td>\n",
       "      <td>101611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>on</td>\n",
       "      <td>92682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>that</td>\n",
       "      <td>89985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78015</th>\n",
       "      <td>71cm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78016</th>\n",
       "      <td>q40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78017</th>\n",
       "      <td>28-135</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78018</th>\n",
       "      <td>lopper</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78019</th>\n",
       "      <td>v-cube</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78020</th>\n",
       "      <td>sound-squared</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78021</th>\n",
       "      <td>backpac</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78022</th>\n",
       "      <td>ellanor</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78023</th>\n",
       "      <td>#eleanorfacts</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78024</th>\n",
       "      <td>ex-broadway</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78025</th>\n",
       "      <td>47d</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78026</th>\n",
       "      <td>imwhywhy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78027</th>\n",
       "      <td>3560</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78028</th>\n",
       "      <td>bent-on-disaster</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78029</th>\n",
       "      <td>35a</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78030</th>\n",
       "      <td>#untouchable</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78031</th>\n",
       "      <td>bonefish</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78032</th>\n",
       "      <td>rattlin</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78033</th>\n",
       "      <td>5019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78034</th>\n",
       "      <td>preamps</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78035</th>\n",
       "      <td>recount</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78036</th>\n",
       "      <td>#sadtimesman</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78037</th>\n",
       "      <td>comprehensible</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78038</th>\n",
       "      <td>#adultcafe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78039</th>\n",
       "      <td>electrocardiography</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78040</th>\n",
       "      <td>speculating</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78041</th>\n",
       "      <td>72nd</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78042</th>\n",
       "      <td>23,73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78043</th>\n",
       "      <td>23,03</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78044</th>\n",
       "      <td>#sit</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78010 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word  occurence\n",
       "0                   <user>     578374\n",
       "1                        (     544909\n",
       "2                        i     537656\n",
       "3                      the     434097\n",
       "4                    <url>     427974\n",
       "5                      ...     413867\n",
       "6                        ,     402968\n",
       "7                        !     368079\n",
       "8                        .     362361\n",
       "9                       to     356966\n",
       "10                       a     256250\n",
       "11                     and     242747\n",
       "12                      of     220383\n",
       "13                     you     200754\n",
       "14                      my     198536\n",
       "15                      is     179956\n",
       "16                      in     168647\n",
       "17                      me     159073\n",
       "18                       -     157895\n",
       "19                       \"     150617\n",
       "20                     for     148608\n",
       "21                       ?     140983\n",
       "22                    this     139045\n",
       "23                       :     126285\n",
       "24                      it     122849\n",
       "25                    with     107030\n",
       "26                   frame     102308\n",
       "27                      so     101611\n",
       "28                      on      92682\n",
       "29                    that      89985\n",
       "...                    ...        ...\n",
       "78015                 71cm          5\n",
       "78016                  q40          5\n",
       "78017               28-135          5\n",
       "78018               lopper          5\n",
       "78019               v-cube          5\n",
       "78020        sound-squared          5\n",
       "78021              backpac          5\n",
       "78022              ellanor          5\n",
       "78023        #eleanorfacts          5\n",
       "78024          ex-broadway          5\n",
       "78025                  47d          5\n",
       "78026             imwhywhy          5\n",
       "78027                 3560          5\n",
       "78028     bent-on-disaster          5\n",
       "78029                  35a          5\n",
       "78030         #untouchable          5\n",
       "78031             bonefish          5\n",
       "78032              rattlin          5\n",
       "78033                 5019          5\n",
       "78034              preamps          5\n",
       "78035              recount          5\n",
       "78036         #sadtimesman          5\n",
       "78037       comprehensible          5\n",
       "78038           #adultcafe          5\n",
       "78039  electrocardiography          5\n",
       "78040          speculating          5\n",
       "78041                 72nd          5\n",
       "78042                23,73          5\n",
       "78043                23,03          5\n",
       "78044                 #sit          5\n",
       "\n",
       "[78010 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence_neg</th>\n",
       "      <th>occurence_pos</th>\n",
       "      <th>difference</th>\n",
       "      <th>somme</th>\n",
       "      <th>ratio</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>362361</td>\n",
       "      <td>379333</td>\n",
       "      <td>16972</td>\n",
       "      <td>741694</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  occurence_neg  occurence_pos  difference   somme     ratio  len\n",
       "8    .         362361         379333       16972  741694  0.022883    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#merge the pos and neg vocabs and study penrtinence of words via ratio\n",
    "merged_full = merging(neg_full_df, pos_full_df)\n",
    "\n",
    "\n",
    "#merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[True, True])\n",
    "\n",
    "\n",
    "merged_full[merged_full[\"word\"] == '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>\n",
    "    \n",
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Study of names with same beginning \n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are words (filtered by length) whose beginning match another word close to this word?\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "\n",
    "#We visualize which word would be mapped on which one \n",
    "filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expected syntaxic divergences\n",
    "As expected, words like \"haha\" can be written in many ways (extended length, typo..). We know that they mean the same,it is therefore interesting to spot them in order to later stem them to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words beginning with \"haha\" or \"ahah\" are sure to be \"instances\" of haha\n",
    "word_haha = test_df.loc[test_df.word.str.startswith(\"haha\") | test_df.word.str.startswith(\"ahah\")]\n",
    "\n",
    "\n",
    "#print(\"Occurence of the words that can be remplaced by haha = \"+ str(int(word_haha.occurence.sum()) - int(word_haha[word_haha.word == \"haha\"].occurence)))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)\n",
    "list(word_haha.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Study duplicates\n",
    "We noticed that several tweets were duplicated in test, pos and neg tweets. We study ho many they are, and which tweets (if any) are shared between the test set and the pos/neg set.\n",
    "We noticed after running that there are no tweets shared between all tweet sets, thus we can label duplicate of the test dataset by directly looking for corresponding duplicates in pos/neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done% %%% %\n"
     ]
    }
   ],
   "source": [
    "common_pos_test = []\n",
    "common_neg_test = []\n",
    "\n",
    "pos = set(pos_tweets)\n",
    "neg = set(neg_tweets)\n",
    "test_tweets = set(test_tweets)\n",
    "len_test = len(test_tweets)\n",
    "for i, test_tweet in enumerate(test_tweets) :\n",
    "    if test_tweet in pos:\n",
    "        common_pos_test.append(test_tweet)\n",
    "        print(\"duplicate pos-test\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))  \n",
    "        break\n",
    "    elif test_tweet in neg:\n",
    "        common_neg_test.append(test_tweet)\n",
    "        print(\"duplicate neg-data\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))\n",
    "        break\n",
    "    print(\"{:.1f}\".format(i/len_test), \"%\", end='\\r')\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occurences(tweet, tweets):\n",
    "    count = 0\n",
    "    for t in tweets:\n",
    "        if t == tweet:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 data pre-processing\n",
    "In this part we start from raw tweets and preprocess them to achieve uniformization of the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Standardization\n",
    "We start by removing repetitions of more than 3 letters, and replace dots by spaces\n",
    "ex : \n",
    "\n",
    "1) `no_dot` : \"Funny.I\" becomes \"Funny I\" \n",
    "\n",
    "2) `find_repetitions` : \"I looooove\" becomes \"I love\"\n",
    "            \n",
    "\n",
    "This is done by the function `standardize_tweets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data pos\n",
      "Process data neg\n",
      "Process data test\n",
      "export processed tweets to files\n"
     ]
    }
   ],
   "source": [
    "#standardize tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = standardize_tweets(pos)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = standardize_tweets(neg) \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = standardize_tweets(data) \n",
    "\n",
    "print(\"export processed tweets to files\")\n",
    "export(processed_tweets_pos_full,  \"preprocessed_pos_full\")\n",
    "export(processed_tweets_neg_full,  \"preprocessed_neg_full\")\n",
    "export(processed_tweets_test_full, \"preprocessed_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the vocabs\n",
    "We build vocabs for positive, negative and test tweets, removing tokens present less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing items present less than 5 times%%%% %%%%% % % %\n",
      "Time elpased (hh:mm:ss.ms) 0:03:21.198569\n",
      "removing items present less than 5 times%% % %% %% % %% % %%% %\n",
      "Time elpased (hh:mm:ss.ms) 0:03:40.127369\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:01.459434\n"
     ]
    }
   ],
   "source": [
    "#build word counters\n",
    "cut_threshold = 5\n",
    "vocab_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#write vocabs to files\n",
    "write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"preprocessed_vocab_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build and Merge the dataframes \n",
    "We build dataframes containing several informations for each type of tweets, and we combine them all in order to facilitate the stemming we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos_full = \"preprocessed_vocab_pos_full\"\n",
    "path_neg_full = \"preprocessed_vocab_neg_full\"\n",
    "path_test_data = \"preprocessed_vocab_test_full\"\n",
    "\n",
    "    \n",
    "pos_full =  build_df(path_pos_full)\n",
    "neg_full =  build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build semantic equivalences\n",
    "We define which word is equivalent to each word (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"Building semantic\")\\n#Build equivalence list\\nsame_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\\nsame_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\\n    \\nprint(\"Building haha semantic\")\\n#build \"haha\" equivalences\\nword_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\\nword_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\\nword_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\\n    \\nword_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\\nword_haha_list.remove(\"haha\")\\nword_haha_list.insert(0, \"haha\")\\n\\n    \\n\\n    \\nprint(\"filter semantic\")\\n#append the two semantic lists and filter them\\nsame_begin.append(list(word_haha_list))\\nsemantic = filter_single_rep(same_begin)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(\"Building semantic\")\n",
    "#Build equivalence list\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "print(\"Building haha semantic\")\n",
    "#build \"haha\" equivalences\n",
    "word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    \n",
    "word_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\n",
    "word_haha_list.remove(\"haha\")\n",
    "word_haha_list.insert(0, \"haha\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"filter semantic\")\n",
    "#append the two semantic lists and filter them\n",
    "same_begin.append(list(word_haha_list))\n",
    "semantic = filter_single_rep(same_begin)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Map the equivalent words, stem them all thanks to nltk and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#process tweets\\nprint(\"Process data pos\")\\nprocessed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\\nprint(\"Process data neg\")\\nprocessed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d\\'un tableau de tweet   \\nprint(\"Process data test\")\\nprocessed_tweets_test_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d\\'un tableau de tweet    \\n    \\n    \\n    \\nprint(\"export data\")\\n#export tweets\\nexport(processed_tweets_neg_full,  \"cleaned_neg_full\")\\nexport(processed_tweets_pos_full,  \"cleaned_pos_full\")\\nexport(processed_tweets_test_full, \"cleaned_test_full\")'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#process tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d'un tableau de tweet   \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"cleaned_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"cleaned_pos_full\")\n",
    "export(processed_tweets_test_full, \"cleaned_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the simple vocab (with occurences) to build the dataframes\n",
    "This is done by using shell commands or python code, as presented before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#build word counters\\nvocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\\nvocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\\nvocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\\n\\n#build vocabs\\nwrite_vocab_to_file(vocab_pos, \"cleaned_vocab_pos_full\")\\nwrite_vocab_to_file(vocab_neg, \"cleaned_vocab_neg_full\")\\nwrite_vocab_to_file(vocab_test, \"cleaned_vocab_test_full\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#build word counters\n",
    "vocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#build vocabs\n",
    "write_vocab_to_file(vocab_pos, \"cleaned_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"cleaned_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"cleaned_vocab_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 create relevant vocab \n",
    "We keep pertinent words in a special vocab called relevant vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataframes before we \n",
    "neg_df = build_df(\"raw_vocab_neg_full_cut=5\")\n",
    "pos_df = build_df(\"raw_vocab_pos_full_cut=5\")\n",
    "\n",
    "#Merge dataframes\n",
    "merged = merging(neg_df, pos_df, False)\n",
    "\n",
    "#create relevant vocab\n",
    "create_relevant_vocab(pertinence_thres=0.3, min_count=300, dataframe=merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Find characteristic words\n",
    "We set a list of words that appear only in neg/pos at least min_diff times such that if we see such a word in a tweet we assume it is a neg/pos tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-470feef5dcbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relevant_vocab_pert=0.3_count=300\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcharacteristic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/EPFL (2013-2015)/Python/Machine Learning/ML_project2/Data_exploration/ProcessTweets.py\u001b[0m in \u001b[0;36mcharacteristic_words\u001b[0;34m(data_tweets, merged)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcharacteristic_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;31m#determine min number of occurence to define a word as characteristic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     \u001b[0mmin_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_min_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;31m#create positive characteristic words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/EPFL (2013-2015)/Python/Machine Learning/ML_project2/Data_exploration/ProcessTweets.py\u001b[0m in \u001b[0;36mset_min_diff\u001b[0;34m(data_tweets, merged)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"not sucessful\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "data_tweets = import_(\"relevant_vocab_pert=0.3_count=300\")\n",
    "\n",
    "characteristic_words(data_tweets, merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Recapitulative Function\n",
    "This function does the work presented above, we created them for clarity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n",
      "Process data pos\n",
      "Process data neg\n",
      "Process data test\n",
      "build vocab data pos\n",
      "removing items present less than 100 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:11.113098\n",
      "build vocab data neg\n",
      "removing items present less than 100 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:14.880160\n",
      "build vocab test data\n",
      "removing items present less than 100 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:01.622162\n",
      "export data\n"
     ]
    }
   ],
   "source": [
    "process_data_no_stem(\"train_pos.txt\", \"train_neg.txt\", \"test_data.txt\", False, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-965564988881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_table(filepath_or_buffer = filepath, encoding=\"utf-8\", quoting=csv.QUOTE_NONE,  header=None, names=[\"word\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'import_without_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2cf2aed52c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcharacteristic_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_tweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_pos.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train_neg.txt\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"test_data.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_full\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpertinence_thres_relevant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count_relevant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2cf2aed52c43>\u001b[0m in \u001b[0;36mclean_tweets\u001b[1;34m(path_pos, path_neg, path_test, is_full, pertinence_thres_relevant, min_count_relevant, cut_threshold)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#path pos, path_neg, path_data : paths of the raw tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#is_full : True iff we clean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprocess_data_no_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Documents\\GitHub\\ML_project2\\Data_exploration\\ProcessTweets.py\u001b[0m in \u001b[0;36mprocess_data_no_stem\u001b[1;34m(path_pos, path_neg, path_test, is_full, cut_threshold)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Import Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;31m#import tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[0mtweets_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_without_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[0mtweets_pos\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mtweets_neg\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'import_without_id' is not defined"
     ]
    }
   ],
   "source": [
    "#stemming(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
