{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Explore and Process the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos = \"./vocab_cut_pos.txt\"\n",
    "path_neg = \"./vocab_cut_neg.txt\"\n",
    "\n",
    "path_pos_full = \"./vocab_cut_pos_full.txt\"\n",
    "path_neg_full = \"./vocab_cut_neg_full.txt\"\n",
    "\n",
    "path_test_data = \"./vocab_test_data.txt\"\n",
    "\n",
    "tweets_pos = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_df(filepath):\n",
    "    \"\"\"return a dataframe which is a mapping of words in tweets\n",
    "    with their occurences in all tweets\n",
    "    take the path of the file of the tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_table(filepath_or_buffer = filepath, header=None, names=[\"word\"])\n",
    "    df[\"occurence\"] = df[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    df[\"word\"] = df[\"word\"].map(lambda x:  x.split()[1])\n",
    "    return df\n",
    "\n",
    "#### -------------- Partial tweets ----------------------------------\n",
    "# build the DF\n",
    "pos = build_df(path_pos)\n",
    "neg = build_df(path_neg)\n",
    "\n",
    "#### -------------- Full tweets ----------------------------------\n",
    "pos_full = build_df(path_pos_full)\n",
    "neg_full = build_df(path_neg_full)\n",
    "\n",
    "#### -------------- Train tweets ----------------------------------\n",
    "\n",
    "test_data = build_df(path_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Neg full shape should be 78028 is : \"+str(neg_full.shape[0]))\n",
    "print(\"Pos full shape should be 46009 is : \"+str(pos_full.shape[0]))\n",
    "print(\"Neg shape should be 16418 is : \"+str(neg.shape[0]))\n",
    "print(\"Pos shape should be 9604 is : \"+str(pos.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merging(neg, pos):\n",
    "\n",
    "    # We merge the two dataframe in order to better handle them\n",
    "    merged = pd.merge(left=neg, right=pos, left_on = \"word\", right_on = \"word\", suffixes=('_neg', '_pos'),  how=\"outer\")\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "\n",
    "    #We only consider words whose occurences dfferences between sad and happy tweets is greater or equal than 5 \n",
    "    merged[\"difference\"] = abs((merged[\"occurence_neg\"]-merged[\"occurence_pos\"]))\n",
    "    merged = merged[merged[\"difference\"]>=5]\n",
    "\n",
    "    #We compute the sum of occurences\n",
    "    merged[\"somme\"] = merged[\"occurence_neg\"]+merged[\"occurence_pos\"]\n",
    "\n",
    "    #The ratio si how relevant it is to judge happyness/sadness of the tweet using the word : 0 if not relevant, 1 if truly relevant\n",
    "    merged[\"ratio\"] = 2* abs(0.5 - merged[\"occurence_pos\"]/(merged[\"occurence_pos\"]+merged[\"occurence_neg\"]))\n",
    "    \n",
    "    #If we want to sort it\n",
    "    #merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False])\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "merged = merging(neg, pos)\n",
    "merged_full = merging(neg_full, pos_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangly strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining does words. We will drop also all the duplicate tweets for training in order to not let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test_data.loc[test_data.word.str.startswith(\"1.26\")])\n",
    "print(test_data.loc[test_data.word.str.startswith(\"misc\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the datafram to keep only ratio = 1 and diff > `MIN_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_diff = 200  # TBD accordingly to : As soon as we see a \"pos_for_sure or a neg_for_sure appearing\n",
    "                # within the same tweet, Min_diff = difference + 1 of the smallest one of the two words.\"\n",
    "\n",
    "mf_max_ratio = merged_full[(merged_full.ratio == 1) & (merged_full.difference >= MIN_diff)]\n",
    "mf_max_ratio = mf_max_ratio[[\"word\",\"difference\"]]\n",
    "word_max_ratio = list(mf_max_ratio.values)\n",
    "\n",
    "def set_min_diff_and_delete_uninteressting()\n",
    "\n",
    "type(word_max_ratio[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_haha = test_data.loc[test_data.word.str.startswith(\"hah\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Occurence of the words that can be remplaced by haha = \"+ str(test_data.occurence.sum()))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_data.txt', 'r') as f:\n",
    "    tweets = [line.strip()[line.find(\",\")+1:] for line in f]     # Make sure to withdraw the \"nbr\", \n",
    "\n",
    "firstTweet = tweets[0]\n",
    "firstTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firstTweet.find(\"doo\")\n",
    "firstTweet.replace(\"doo\", \"fovgho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_and_remplace(semantics, representative, tweets):\n",
    "    \"\"\"Retrun the tweets countaining only the representative of a given semantic \"\"\"\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        for semantic in semantics:          \n",
    "            if (semantic in tweet):\n",
    "                tweets[i] = tweet.replace(semantic, representative)\n",
    "            \n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tweets2 = find_and_remplace(word_haha_list, \"haha\", tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets2[481]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(lower_bound):\n",
    "    \n",
    "    #paths of positive and negative vocabs\n",
    "    path_pos = \"./vocab_cut_pos_full.txt\"\n",
    "    path_neg = \"./vocab_cut_neg_full.txt\"\n",
    "    \n",
    "    # pos is mapping of words in happy tweets with their occurences in all happy tweets\n",
    "    pos = pd.read_table(filepath_or_buffer = path_pos, header=None, names=[\"word\"])\n",
    "    pos[\"occurence\"] = pos[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    pos[\"word\"] = pos[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "\n",
    "    # neg is mapping of words in sad tweets with their occurences in all sad tweets\n",
    "    neg = pd.read_table(filepath_or_buffer = path_neg, header=None, names=[\"word\"])\n",
    "    neg[\"occurence\"] = neg[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    neg[\"word\"] = neg[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "    \n",
    "    # We merge the two dataframe in order to better handle them\n",
    "    merged = pd.merge(left=neg, right=pos, left_on = \"word\", right_on = \"word\", suffixes=('_neg', '_pos'),  how=\"outer\")\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "    #We only consider words whose occurences dfferences between sad and happy tweets is greater or equal than 5 \n",
    "    merged[\"difference\"] = abs((merged[\"occurence_neg\"]-merged[\"occurence_pos\"]))\n",
    "    merged = merged[merged[\"difference\"]>=5]\n",
    "\n",
    "    #We compute the sum of occurences\n",
    "    merged[\"somme\"] = merged[\"occurence_neg\"]+merged[\"occurence_pos\"]\n",
    "\n",
    "    #The ratio si how relevant it is to judge happyness/sadness of the tweet using the word : 0 if not relevant, 1 if truly relevant\n",
    "    merged[\"ratio\"] = 2* abs(0.5 - merged[\"occurence_pos\"]/(merged[\"occurence_pos\"]+merged[\"occurence_neg\"]))\n",
    "    \n",
    "    \n",
    "    def lower_ratio(x) :\n",
    "        if(x[\"somme\"]<MIN_SOMME):\n",
    "            return 0\n",
    "        else:\n",
    "            return x[\"ratio\"]\n",
    "    \n",
    "    \n",
    "    #We only consider with more than 'lower_bound' occurences\n",
    "    merged[\"ratio\"] = merged.apply(lower_ratio, axis = 1) \n",
    "    \n",
    "    #sort the array by ratio and then sum\n",
    "    merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False])\n",
    "    \n",
    "    #store the data\n",
    "    filename = \"relevant_vocab_full_lb=\"+str(lower_bound)+\".txt\"\n",
    "    merged.to_csv(path_or_buf=filename, sep=' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for lb in [50, 100, 500, 1000, 2000, 5000, 10000, 20000, 50000]:\n",
    "create_data(5000)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
