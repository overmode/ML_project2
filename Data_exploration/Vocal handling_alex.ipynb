{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data we are given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple do we have, are there some duplicates, is there any odds with the tweets? How are there classify (we are given to datasets, one for the positive tweets one for negatives) i.e. what is a typcal word that occurence when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may countain words that are not usfull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than two following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "\n",
    "\n",
    "### Part 1 A tour accross the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos = \"./vocab_cut_pos.txt\"\n",
    "path_neg = \"./vocab_cut_neg.txt\"\n",
    "\n",
    "path_pos_full = \"./vocab_cut_pos_full.txt\"\n",
    "path_neg_full = \"./vocab_cut_neg_full.txt\"\n",
    "\n",
    "path_test_data = \"./vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"./train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"./train_neg_full.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_df(filepath):\n",
    "    \"\"\"from a cut_vocab return a dataframe which is a mapping of words in tweets\n",
    "    with their occurences in all tweets\n",
    "    take the path of the file of the tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_table(filepath_or_buffer = filepath, header=None, names=[\"word\"])\n",
    "    df[\"occurence\"] = df[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    df[\"word\"] = df[\"word\"].map(lambda x:  x.split()[1])\n",
    "    return df\n",
    "\n",
    "#### -------------- Partial tweets ----------------------------------\n",
    "# build the DF\n",
    "pos = build_df(path_pos)\n",
    "neg = build_df(path_neg)\n",
    "\n",
    "#### -------------- Full tweets ----------------------------------\n",
    "pos_full = build_df(path_pos_full)\n",
    "neg_full = build_df(path_neg_full)\n",
    "\n",
    "#### -------------- Train tweets ----------------------------------\n",
    "\n",
    "test_data = build_df(path_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d5e3e41fdba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_full\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merging' is not defined"
     ]
    }
   ],
   "source": [
    "m = merging(merged_full,test_data)\n",
    "m.word.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging(neg, pos, only_words = False):\n",
    "\n",
    "    # We merge the two dataframe in order to better handle them\n",
    "    merged = pd.merge(left=neg, right=pos, left_on = \"word\", right_on = \"word\", suffixes=('_neg', '_pos'),  how=\"outer\")\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "    if(not only_words):\n",
    "        #We only consider words whose occurences dfferences between sad and happy tweets is greater or equal than 5 \n",
    "        merged[\"difference\"] = abs((merged[\"occurence_neg\"]-merged[\"occurence_pos\"]))\n",
    "        merged = merged[merged[\"difference\"]>=5]\n",
    "\n",
    "        #We compute the sum of occurences\n",
    "        merged[\"somme\"] = merged[\"occurence_neg\"]+merged[\"occurence_pos\"]\n",
    "\n",
    "        #The ratio si how relevant it is to judge happyness/sadness of the tweet using the word : 0 if not relevant, 1 if truly relevant\n",
    "        merged[\"ratio\"] = 2* abs(0.5 - merged[\"occurence_pos\"]/(merged[\"occurence_pos\"]+merged[\"occurence_neg\"]))\n",
    "    \n",
    "    \n",
    "    merged[\"len\"] = merged[\"word\"].map(len)\n",
    "    \n",
    "    #If we want to sort it\n",
    "    #merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False])\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "merged = merging(neg, pos)\n",
    "merged_full = merging(neg_full, pos_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence_neg</th>\n",
       "      <th>occurence_pos</th>\n",
       "      <th>difference</th>\n",
       "      <th>somme</th>\n",
       "      <th>ratio</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>paperback</td>\n",
       "      <td>3509.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3509.0</td>\n",
       "      <td>3509.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>hardcover</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>ounce</td>\n",
       "      <td>761.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>761.0</td>\n",
       "      <td>761.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>replacement</td>\n",
       "      <td>625.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>cable</td>\n",
       "      <td>505.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  occurence_neg  occurence_pos  difference   somme  ratio  len\n",
       "55     paperback         3509.0            0.0      3509.0  3509.0    1.0    9\n",
       "150    hardcover         1339.0            0.0      1339.0  1339.0    1.0    9\n",
       "249        ounce          761.0            0.0       761.0   761.0    1.0    5\n",
       "293  replacement          625.0            0.0       625.0   625.0    1.0   11\n",
       "359        cable          505.0            0.0       505.0   505.0    1.0    5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence_neg</th>\n",
       "      <th>occurence_pos</th>\n",
       "      <th>difference</th>\n",
       "      <th>somme</th>\n",
       "      <th>ratio</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>a-tech</td>\n",
       "      <td>5329.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5329.0</td>\n",
       "      <td>5329.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ddr</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>manufactured</td>\n",
       "      <td>4676.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4676.0</td>\n",
       "      <td>4676.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1gb</td>\n",
       "      <td>4216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4216.0</td>\n",
       "      <td>4216.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>misc</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>3635.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  occurence_neg  occurence_pos  difference   somme  ratio  \\\n",
       "429        a-tech         5329.0            0.0      5329.0  5329.0    1.0   \n",
       "472           ddr         4844.0            0.0      4844.0  4844.0    1.0   \n",
       "483  manufactured         4676.0            0.0      4676.0  4676.0    1.0   \n",
       "527           1gb         4216.0            0.0      4216.0  4216.0    1.0   \n",
       "614          misc         3635.0            0.0      3635.0  3635.0    1.0   \n",
       "\n",
       "     len  \n",
       "429    6  \n",
       "472    3  \n",
       "483   12  \n",
       "527    3  \n",
       "614    4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 data pre-processing\n",
    "\n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the \n",
    "def filter_single_rep(same_begin):\n",
    "    for l in same_begin:\n",
    "        if len(l) == 1 :\n",
    "            same_begin.remove(l)\n",
    "\n",
    "        for w in l:\n",
    "            if \"-\" in w:\n",
    "                l.remove(w)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['complete', 'completely', 'completed', 'completes', 'completeness'],\n",
       " ['tomorrow',\n",
       "  \"tomorrow's\",\n",
       "  'tomorrows',\n",
       "  'tomorrowww',\n",
       "  'tomorroww',\n",
       "  'tomorrowland'],\n",
       " ['everyone', \"everyone's\", 'everyones', 'everyonee', 'everyoneee'],\n",
       " ['anything'],\n",
       " ['watching', 'watchingg', 'watchinggg'],\n",
       " ['notebook', 'notebooks', \"notebook's\"],\n",
       " ['american', 'americans', 'americana', \"american's\"],\n",
       " ['birthday', 'birthdays', 'birthdayyy', \"birthday's\", 'birthdayy'],\n",
       " ['remember',\n",
       "  'remembered',\n",
       "  'remembering',\n",
       "  'remembers',\n",
       "  \"remember's\",\n",
       "  'rememberd'],\n",
       " ['business',\n",
       "  'businesses',\n",
       "  'businessman',\n",
       "  'businessweek',\n",
       "  'businessmen',\n",
       "  'businesswoman'],\n",
       " [\"couldn't\"],\n",
       " ['together', 'togetherr', 'togethers', 'togetherness'],\n",
       " ['original', 'originally', 'originals', 'original.title', 'originali'],\n",
       " ['official', 'officially', 'officials', 'officialy'],\n",
       " ['includes'],\n",
       " ['personal',\n",
       "  'personalized',\n",
       "  'personality',\n",
       "  'personally',\n",
       "  'personalize',\n",
       "  'personalities',\n",
       "  'personaliz',\n",
       "  'personalizing',\n",
       "  'personalit',\n",
       "  'personalised',\n",
       "  'personals'],\n",
       " ['designed'],\n",
       " ['computer',\n",
       "  'computers',\n",
       "  'computerworld',\n",
       "  \"computer's\",\n",
       "  'computerized',\n",
       "  'computer-generated'],\n",
       " ['saturday', 'saturdays', \"saturday's\", 'saturdayy'],\n",
       " ['horrible', 'horribleee'],\n",
       " ['headache', 'headaches', 'headacheee', 'headachee', \"headache's\"],\n",
       " ['wireless', 'wireless-g', 'wirelessly'],\n",
       " ['favorite',\n",
       "  'favorites',\n",
       "  'favorited',\n",
       "  'favoritest',\n",
       "  'favoritee',\n",
       "  'favoriteee'],\n",
       " ['standard', 'standards', 'standardized', 'standard-issue', 'standard-size'],\n",
       " ['internet', 'internets', \"internet's\", 'internetworking'],\n",
       " ['everyday', 'everydayy'],\n",
       " ['football', 'footballs', \"football's\", 'footballing'],\n",
       " ['somebody', \"somebody's\", 'somebodys', 'somebodyyy'],\n",
       " ['followed', 'followedyou', 'followed.follow', 'followedrt'],\n",
       " ['practice', 'practices', 'practicee', 'practiceee'],\n",
       " ['hospital', 'hospitals', 'hospitality', 'hospitalized', 'hospitalization'],\n",
       " ['pictures', 'picturesque', 'picturess'],\n",
       " ['japanese', 'japanese-only', 'japaneses'],\n",
       " ['manufactured',\n",
       "  'manufact',\n",
       "  'manufactur',\n",
       "  'manufactu',\n",
       "  'manufacture',\n",
       "  'manufacturer',\n",
       "  'manufacturing',\n",
       "  'manufacturers',\n",
       "  \"manufacturer's\",\n",
       "  'manufactures'],\n",
       " ['ultimate', 'ultimately', 'ultimates', \"ultimate's\"],\n",
       " ['homework', 'homeworks', 'homeworkkk'],\n",
       " ['tweeting'],\n",
       " ['supposed', 'supposedly'],\n",
       " ['children', \"children's\", 'childrens'],\n",
       " ['provides', 'providesquiet'],\n",
       " ['annoying', 'annoyinggg', 'annoyingg'],\n",
       " ['facebook', \"facebook's\", 'facebooks', 'facebooking'],\n",
       " ['straight',\n",
       "  'straightened',\n",
       "  'straighten',\n",
       "  'straightener',\n",
       "  'straightening',\n",
       "  'straightforward',\n",
       "  'straighteners',\n",
       "  'straightners',\n",
       "  'straightner',\n",
       "  'straightness',\n",
       "  'straightaway',\n",
       "  'straightt'],\n",
       " ['training'],\n",
       " ['shoulder', 'shoulders', 'shoulderrest', \"shoulder's\"],\n",
       " ['thursday', 'thursdays', \"thursday's\", 'thursdayyy', 'thursdayy'],\n",
       " ['southern', 'southerner', 'southerners'],\n",
       " ['portable', 'portables'],\n",
       " ['national',\n",
       "  'nationals',\n",
       "  'nationality',\n",
       "  'nationalism',\n",
       "  'nationally',\n",
       "  'nationalities'],\n",
       " ['language', 'languages'],\n",
       " ['feelings', 'feelingss'],\n",
       " ['charging'],\n",
       " ['handbook', 'handbooks'],\n",
       " ['historic', 'historical', 'historically', 'historica'],\n",
       " ['honestly'],\n",
       " ['studying', 'studyinggg'],\n",
       " ['research',\n",
       "  'researchers',\n",
       "  'researched',\n",
       "  'researcher',\n",
       "  'researches',\n",
       "  'research-based'],\n",
       " ['terrible', 'terribleee'],\n",
       " ['security', 'securityman'],\n",
       " ['carrying'],\n",
       " ['hahahaha',\n",
       "  'hahahahaha',\n",
       "  'hahahahah',\n",
       "  'hahahahahaha',\n",
       "  'hahahahahahaha',\n",
       "  'hahahahahah',\n",
       "  'hahahahahahahaha',\n",
       "  'hahahahaa',\n",
       "  'hahahahahha',\n",
       "  'hahahahahahahahaha',\n",
       "  'hahahahahahah',\n",
       "  'hahahahahahahahahahaha',\n",
       "  'hahahahahahahah',\n",
       "  'hahahahahaa',\n",
       "  'hahahahaah',\n",
       "  'hahahahahahahahahah',\n",
       "  'hahahahahahahahah',\n",
       "  'hahahahahaaa',\n",
       "  'hahahahahhaa',\n",
       "  'hahahahaahah',\n",
       "  'hahahahahahahha',\n",
       "  'hahahahahahahahahahahahahahahaha',\n",
       "  'hahahahahahaa'],\n",
       " ['european', 'europeans', 'europeanfanswant'],\n",
       " ['download', 'downloaded', 'downloading', 'downloadable', 'downloader'],\n",
       " ['software'],\n",
       " ['lifetime', 'lifetimes'],\n",
       " ['princess', 'princesses', 'princess-cut', 'princesss', \"princess's\"],\n",
       " ['memories'],\n",
       " ['aluminum'],\n",
       " ['exterior', 'exteriors'],\n",
       " ['building', 'buildings'],\n",
       " ['diameter', 'diameters'],\n",
       " ['freaking', 'freakingg'],\n",
       " ['creative', 'creativeworx', 'creatives'],\n",
       " ['magazine', 'magazines', \"magazine's\"],\n",
       " ['medicine', 'medicines'],\n",
       " ['breaking', 'breakingnews'],\n",
       " ['keyboard', 'keyboards', 'keyboardist', 'keyboarding'],\n",
       " ['electric',\n",
       "  'electrical',\n",
       "  'electricity',\n",
       "  'electrician',\n",
       "  'electrics',\n",
       "  'electricians',\n",
       "  'electrically'],\n",
       " ['questions',\n",
       "  'question',\n",
       "  'questioning',\n",
       "  'questioned',\n",
       "  'questionable',\n",
       "  'questionnaire',\n",
       "  'questionnaires'],\n",
       " ['whatever', 'whateverrr', 'whateverr', 'whatevers'],\n",
       " ['shutters'],\n",
       " ['baseball', 'baseballs', \"baseball's\", 'baseballblogs', 'baseballl'],\n",
       " ['multiple', 'multiples', 'multiplex'],\n",
       " ['whenever', 'wheneverr'],\n",
       " ['avengers', 'avengersss'],\n",
       " ['anywhere'],\n",
       " ['revision', 'revisions'],\n",
       " ['powerful', 'powerfully'],\n",
       " ['mountain',\n",
       "  'mountains',\n",
       "  'mountaineers',\n",
       "  'mountaineering',\n",
       "  'mountaineer',\n",
       "  'mountainous',\n",
       "  'mountaintop',\n",
       "  'mountainsmith'],\n",
       " ['laughing', 'laughingg'],\n",
       " ['drinking'],\n",
       " ['although', 'althought'],\n",
       " ['fighting'],\n",
       " ['daughter', 'daughters', \"daughter's\"],\n",
       " ['shipping'],\n",
       " ['gorgeous', 'gorgeously', 'gorgeousss', 'gorgeousness', 'gorgeouss'],\n",
       " ['speakers'],\n",
       " ['analysis'],\n",
       " ['director',\n",
       "  'directory',\n",
       "  \"director's\",\n",
       "  'directors',\n",
       "  'directorial',\n",
       "  'directories'],\n",
       " ['freezing', 'freezinggg'],\n",
       " ['thoughts', 'thoughtstream'],\n",
       " ['backpack',\n",
       "  'backpacks',\n",
       "  'backpacking',\n",
       "  'backpacker',\n",
       "  \"backpacker's\",\n",
       "  'backpackers'],\n",
       " ['industry', \"industry's\", 'industrys', 'industryweek'],\n",
       " ['metallic', 'metallicum', 'metallica', 'metallics', \"metallica's\"],\n",
       " ['planning'],\n",
       " ['exciting', 'excitinggg'],\n",
       " ['designer', 'designers', \"designer's\"],\n",
       " ['accident',\n",
       "  'accidentally',\n",
       "  'accidental',\n",
       "  'accidently',\n",
       "  'accidents',\n",
       "  'accidentaly'],\n",
       " ['approach', 'approaches', 'approaching', 'approached'],\n",
       " ['vacation', 'vacations', 'vacationing', 'vacationland'],\n",
       " ['solution', 'solutions'],\n",
       " ['mentions'],\n",
       " ['cordless'],\n",
       " ['continue', 'continues', 'continued'],\n",
       " ['discover',\n",
       "  'discovery',\n",
       "  'discovered',\n",
       "  'discovering',\n",
       "  'discovers',\n",
       "  'discoveries',\n",
       "  'discoverer',\n",
       "  'discoverers',\n",
       "  \"discovery's\"],\n",
       " ['episodes'],\n",
       " ['calendar', 'calendars'],\n",
       " ['midnight', \"midnight's\"],\n",
       " ['schedule', 'scheduled', 'schedules'],\n",
       " ['material', 'materials', 'materialpatented', 'materialism'],\n",
       " ['response', 'responses'],\n",
       " ['trending'],\n",
       " ['adorable', 'adorablee', 'adorableee', 'adorableness'],\n",
       " ['decision', 'decisions', 'decision-making'],\n",
       " ['religion', 'religions'],\n",
       " ['infinity'],\n",
       " ['recently'],\n",
       " ['strength',\n",
       "  'strengths',\n",
       "  'strengthen',\n",
       "  'strengthening',\n",
       "  'strengthens',\n",
       "  'strengthened'],\n",
       " ['canadian', 'canadians'],\n",
       " ['hologram'],\n",
       " ['standing', 'standings'],\n",
       " ['critical', 'critically'],\n",
       " ['physical', 'physically'],\n",
       " ['category'],\n",
       " ['distance'],\n",
       " ['phillips'],\n",
       " ['bathroom', 'bathrooms'],\n",
       " ['external', 'externally'],\n",
       " ['spending'],\n",
       " ['teaching', 'teachings'],\n",
       " ['shoutout', 'shoutouts', 'shoutouttt', 'shoutoutt'],\n",
       " ['timeline', 'timelines', \"timeline's\"],\n",
       " ['unfollow',\n",
       "  'unfollowed',\n",
       "  'unfollowing',\n",
       "  'unfollows',\n",
       "  'unfollowers',\n",
       "  'unfollower',\n",
       "  'unfollowin'],\n",
       " ['sandwich', 'sandwiches'],\n",
       " ['exercise', 'exercises', 'exerciser', 'exercised'],\n",
       " ['approved'],\n",
       " ['position',\n",
       "  'positions',\n",
       "  'positioning',\n",
       "  'positioner',\n",
       "  'positional',\n",
       "  'positioned'],\n",
       " ['cookbook', 'cookbooks'],\n",
       " ['romantic',\n",
       "  'romanticism',\n",
       "  'romantics',\n",
       "  'romantically',\n",
       "  'romantica',\n",
       "  'romantical'],\n",
       " ['clinical', 'clinically', 'clinicals'],\n",
       " ['survival', 'survivalist'],\n",
       " ['graduate', 'graduated', 'graduates'],\n",
       " ['stranger', 'strangers', 'strangerrr'],\n",
       " ['becoming'],\n",
       " ['weekends'],\n",
       " ['reliever'],\n",
       " ['magnetic', 'magnetics'],\n",
       " ['stunning', 'stunningly'],\n",
       " ['starving', 'starvinggg'],\n",
       " ['acoustic', 'acoustics'],\n",
       " ['pressure', 'pressures'],\n",
       " ['festival', 'festivals'],\n",
       " ['slightly', 'slightlyyy'],\n",
       " ['imperial', 'imperialism'],\n",
       " ['struggle', 'struggles', 'struggled'],\n",
       " ['semester', 'semesters'],\n",
       " ['colorful', 'colorfully'],\n",
       " ['inspiron'],\n",
       " ['customer', 'customers', \"customer's\"],\n",
       " ['receiver', 'receivers'],\n",
       " ['licensed'],\n",
       " ['sometimes', 'sometime', 'sometimess', 'sometimeee'],\n",
       " ['surprise', 'surprised', 'surprises'],\n",
       " ['softball', 'softballs'],\n",
       " ['friendly', \"friendly's\", 'friendlys'],\n",
       " ['universe', 'universes'],\n",
       " ['necklace', 'necklaces'],\n",
       " ['bracelet', 'bracelets'],\n",
       " ['bothered'],\n",
       " ['thousand', 'thousands'],\n",
       " ['specific',\n",
       "  'specifically',\n",
       "  'specifications',\n",
       "  'specification',\n",
       "  'specificati',\n",
       "  'specifica',\n",
       "  'specificall',\n",
       "  'specifical',\n",
       "  'specifics'],\n",
       " ['republic', 'republican', 'republicans', 'republicanism'],\n",
       " ['northern', 'northerners'],\n",
       " ['thankyou', 'thankyouuu', 'thankyouu'],\n",
       " ['latitude', 'latitudes'],\n",
       " ['superior', 'superiority'],\n",
       " ['explorer', 'explorers', \"explorer's\"],\n",
       " ['platform', 'platforms', 'platformer'],\n",
       " ['virginia', \"virginia's\", 'virginian'],\n",
       " ['marriage', 'marriages'],\n",
       " ['marathon', 'marathons'],\n",
       " [\"nature's\"],\n",
       " ['flexible'],\n",
       " ['swimsuit', 'swimsuits'],\n",
       " ['built-in'],\n",
       " ['internal', 'internally', 'internal-external', 'internals'],\n",
       " ['absolutely', 'absolute'],\n",
       " ['thriller', 'thrillers'],\n",
       " ['envelope', 'envelopes'],\n",
       " ['resource', 'resources', 'resourceful'],\n",
       " ['painting', 'paintings', 'paintingiant'],\n",
       " ['positive', 'positives'],\n",
       " ['engineering', 'engineer', 'engineers', 'engineered', \"engineer's\"],\n",
       " ['fabulous', 'fabulously', 'fabulousness'],\n",
       " ['darkness'],\n",
       " ['function',\n",
       "  'functional',\n",
       "  'functions',\n",
       "  'functionality',\n",
       "  'functioning',\n",
       "  'functionally',\n",
       "  'functiona'],\n",
       " ['carolina', 'carolinas'],\n",
       " ['emotions'],\n",
       " ['#believe',\n",
       "  '#believealbumcover',\n",
       "  '#believetour',\n",
       "  '#believetourinmanila',\n",
       "  '#believeinblue',\n",
       "  '#believetouriscoming'],\n",
       " ['workbook', 'workbooks'],\n",
       " ['sunshine', 'sunshineee', 'sunshiney', 'sunshines'],\n",
       " ['possibly'],\n",
       " ['graphics'],\n",
       " ['dreading'],\n",
       " ['sciences'],\n",
       " ['innocent', 'innocents'],\n",
       " ['followers',\n",
       "  'follower',\n",
       "  'followerss',\n",
       "  'followersss',\n",
       "  'followerr',\n",
       "  'followerrr',\n",
       "  \"follower's\"],\n",
       " ['expanded'],\n",
       " ['transfer', 'transferring', 'transfers', 'transferred', 'transfered'],\n",
       " ['waterloo', 'waterlooroad'],\n",
       " ['location', 'locations', 'location-based'],\n",
       " ['password'],\n",
       " ['workshop', 'workshops'],\n",
       " ['michigan', \"michigan's\"],\n",
       " ['handsome', 'handsomely', 'handsomee', 'handsomeee', 'handsomes'],\n",
       " ['charming', 'charmingly'],\n",
       " ['airplane', 'airplanes'],\n",
       " ['nintendo', \"nintendo's\"],\n",
       " ['interesting', 'interest', 'interested', 'intereste', 'interestin'],\n",
       " ['handheld', 'handhelditems'],\n",
       " ['scotland', \"scotland's\"],\n",
       " ['migraine', 'migraines'],\n",
       " ['portrait', 'portraits'],\n",
       " ['selected'],\n",
       " ['economic', 'economics', 'economical', 'economically'],\n",
       " ['bleeding'],\n",
       " ['congrats', 'congratsss', 'congratss'],\n",
       " ['williams', 'williamsburg', 'williamson', \"williams's\"],\n",
       " ['dramatic', 'dramatically'],\n",
       " ['carriage', 'carriages'],\n",
       " ['overview'],\n",
       " ['december', 'decemberists'],\n",
       " ['separate', 'separates', 'separately', 'separated'],\n",
       " ['champions', 'champion', 'championship', \"champion's\"],\n",
       " ['umbrella', 'umbrellas'],\n",
       " ['treasure', 'treasures', 'treasured', 'treasurer'],\n",
       " ['prepared', 'preparedness'],\n",
       " ['accepted'],\n",
       " ['reaction', 'reactions'],\n",
       " ['beliebers', 'belieber', \"belieber's\"],\n",
       " ['polished'],\n",
       " ['cassette', 'cassettes'],\n",
       " ['t-mobile', 't-mobiles'],\n",
       " ['shooting', 'shootings'],\n",
       " ['goodness'],\n",
       " ['documentary',\n",
       "  'document',\n",
       "  'documents',\n",
       "  'documentation',\n",
       "  'documenting',\n",
       "  'documented',\n",
       "  'documentaries',\n",
       "  'documenta'],\n",
       " ['whatsapp', 'whatsapps', 'whatsapping'],\n",
       " ['progress',\n",
       "  'progressive',\n",
       "  'progresso',\n",
       "  'progression',\n",
       "  'progressions',\n",
       "  'progressing',\n",
       "  'progressively',\n",
       "  'progresses'],\n",
       " ['malaysia', 'malaysian', 'malaysians', \"malaysia's\"],\n",
       " ['pakistan', 'pakistani', \"pakistan's\"],\n",
       " ['addition', 'additional', 'additions'],\n",
       " ['opposite', 'opposites'],\n",
       " ['pleasure', 'pleasures'],\n",
       " ['checkout'],\n",
       " ['hangover', 'hangovers'],\n",
       " ['contract',\n",
       "  'contracts',\n",
       "  'contractor',\n",
       "  'contractions',\n",
       "  'contractors',\n",
       "  'contracted',\n",
       "  'contracting',\n",
       "  'contraction'],\n",
       " ['bullshit', 'bullshitting', 'bullshitter', 'bullshiting', 'bullshitt'],\n",
       " ['clothing'],\n",
       " ['delivery'],\n",
       " ['crackers'],\n",
       " ['attached'],\n",
       " ['columbia', 'columbian', \"columbia's\"],\n",
       " ['parallel', 'parallels'],\n",
       " ['hardware'],\n",
       " ['naturals'],\n",
       " ['movement', 'movements'],\n",
       " ['kingston', 'kingstons', \"kingston's\"],\n",
       " ['graphite', 'graphites'],\n",
       " ['monsters'],\n",
       " ['featured'],\n",
       " ['chilling', 'chillingg', 'chillinggg'],\n",
       " ['recorder', 'recorders'],\n",
       " ['checking'],\n",
       " ['accounts'],\n",
       " ['ordinary'],\n",
       " ['matching'],\n",
       " ['textbook', 'textbooks'],\n",
       " ['offering', 'offerings'],\n",
       " ['vertical'],\n",
       " ['describes', 'describe', 'described'],\n",
       " ['explains'],\n",
       " ['tommorow', 'tommorows'],\n",
       " ['athletic', 'athletics', 'athletico'],\n",
       " ['revealed'],\n",
       " ['dressing', 'dressings'],\n",
       " ['innovate', 'innovateunlike'],\n",
       " ['regional', 'regionals'],\n",
       " ['titanium'],\n",
       " ['timeless', 'timelessness'],\n",
       " ['guardian', 'guardians', 'guardianmeet'],\n",
       " ['february'],\n",
       " ['piercing', 'piercings'],\n",
       " ['cheating'],\n",
       " ['attitude', 'attitudes'],\n",
       " ['writings'],\n",
       " ['evidence'],\n",
       " ['two-disc'],\n",
       " ['previous', 'previously'],\n",
       " ['catholic', 'catholicism', 'catholics'],\n",
       " ['printers'],\n",
       " ['artisans'],\n",
       " ['campaign',\n",
       "  'campaigns',\n",
       "  'campaigners',\n",
       "  'campaignlive',\n",
       "  'campaigner',\n",
       "  'campaigned'],\n",
       " ['managing'],\n",
       " ['thailand', \"thailand's\"],\n",
       " ['announce',\n",
       "  'announces',\n",
       "  'announced',\n",
       "  'announcement',\n",
       "  'announcements',\n",
       "  'announcers'],\n",
       " ['polaroid', 'polaroids', \"polaroid's\"],\n",
       " ['something',\n",
       "  'somethin',\n",
       "  \"something's\",\n",
       "  'somethings',\n",
       "  'somethingg',\n",
       "  'somethinn'],\n",
       " ['roommate', \"roommate's\", 'roommates'],\n",
       " ['required'],\n",
       " ['interior', 'interiors'],\n",
       " ['mistakes', 'mistakes.here'],\n",
       " ['talented'],\n",
       " ['negative', 'negatives'],\n",
       " ['chemical', 'chemicals', 'chemically', 'chemical-resistant'],\n",
       " ['jennifer', \"jennifer's\"],\n",
       " ['mounting'],\n",
       " ['victoria', 'victorian'],\n",
       " ['packaged'],\n",
       " ['involved'],\n",
       " ['frontier', 'frontiers', 'frontiercycle'],\n",
       " ['criminal', 'criminals'],\n",
       " ['elephant', 'elephants', \"elephant's\", 'elephantmen'],\n",
       " ['covering'],\n",
       " ['database', 'databases'],\n",
       " ['improved'],\n",
       " ['exchange', 'exchanged'],\n",
       " ['shocking', 'shockingly'],\n",
       " ['premiere', 'premieres'],\n",
       " ['reusable'],\n",
       " ['franklin', \"franklin's\"],\n",
       " ['creation', 'creations'],\n",
       " ['tutorial', 'tutorials'],\n",
       " ['guessing'],\n",
       " ['sneakers'],\n",
       " ['sickness'],\n",
       " ['compared'],\n",
       " ['purchase', 'purchased', 'purchases', 'purchasers', 'purchaser'],\n",
       " ['behavior', 'behavioral', 'behaviors', 'behaviorally'],\n",
       " ['spinning'],\n",
       " ['colorado', \"colorado's\"],\n",
       " ['playlist', 'playlists'],\n",
       " ['disaster', 'disasters'],\n",
       " ['answered'],\n",
       " ['knitting'],\n",
       " ['freeware'],\n",
       " ['adhesive', 'adhesives', 'adhesive-backed'],\n",
       " ['teenager', 'teenagers'],\n",
       " ['spectrum'],\n",
       " ['producer', 'producers', \"producer's\"],\n",
       " ['passport', \"passport's\"],\n",
       " ['fellowes'],\n",
       " ['tactical', 'tactically'],\n",
       " ['increase', 'increased', 'increases'],\n",
       " ['dynamics'],\n",
       " ['considering',\n",
       "  'considered',\n",
       "  'consider',\n",
       "  'considers',\n",
       "  'considerable',\n",
       "  'considerations'],\n",
       " ['springer'],\n",
       " ['patented'],\n",
       " ['launched'],\n",
       " ['grateful', 'gratefully'],\n",
       " ['launches'],\n",
       " ['danielle', \"danielle's\"],\n",
       " ['resistor', 'resistors'],\n",
       " ['measures'],\n",
       " ['wardrobe', 'wardrobes'],\n",
       " ['machines'],\n",
       " ['property'],\n",
       " ['conflict', 'conflicts', 'conflicting', 'conflicted'],\n",
       " ['theology'],\n",
       " ['modeling'],\n",
       " ['iloveyou',\n",
       "  'iloveyouu',\n",
       "  'iloveyouuu',\n",
       "  'iloveyoussosomuch',\n",
       "  'iloveyoubieber',\n",
       "  'iloveyoutoo'],\n",
       " ['fountain', 'fountains', 'fountainhead'],\n",
       " ['reporter', \"reporter's\"],\n",
       " [\"cousin's\"],\n",
       " ['intimate', 'intimates', 'intimately'],\n",
       " ['abstract', 'abstraction', 'abstracts'],\n",
       " ['spelling', 'spellings'],\n",
       " ['florence'],\n",
       " ['compound', 'compounds'],\n",
       " ['referred'],\n",
       " ['hawaiian'],\n",
       " ['visiting'],\n",
       " ['organics'],\n",
       " ['explicit', 'explicitly'],\n",
       " ['canceled'],\n",
       " ['keychain', 'keychains'],\n",
       " ['constantly',\n",
       "  'constant',\n",
       "  'constantine',\n",
       "  'constantin',\n",
       "  'constantinople',\n",
       "  'constants',\n",
       "  'constantinian'],\n",
       " ['wherever'],\n",
       " ['articles'],\n",
       " ['sneezing'],\n",
       " ['hamilton', \"hamilton's\"],\n",
       " ['backyard', 'backyardigans'],\n",
       " ['directed'],\n",
       " ['complain',\n",
       "  'complaining',\n",
       "  'complaints',\n",
       "  'complaint',\n",
       "  'complained',\n",
       "  'complains',\n",
       "  'complainin',\n",
       "  'complainers'],\n",
       " ['superman'],\n",
       " ['shredder', 'shredders'],\n",
       " ['greeting', 'greetings'],\n",
       " ['close-up', 'close-ups'],\n",
       " ['syndrome'],\n",
       " ['nowadays'],\n",
       " ['forehead', 'foreheads'],\n",
       " ['brooklyn', 'brooklynvegan', 'brooklynn', \"brooklyn's\"],\n",
       " ['#missyou', '#missyoualready', '#missyousomuch'],\n",
       " ['stopping'],\n",
       " ['#jealous', '#jealoustweet'],\n",
       " ['stealing'],\n",
       " ['forecast', 'forecasting', 'forecaster', 'forecasts', 'forecasted'],\n",
       " ['terribly'],\n",
       " ['landmark', 'landmarks'],\n",
       " ['datacomm'],\n",
       " ['colonial', 'colonialism'],\n",
       " ['insights'],\n",
       " ['connects'],\n",
       " ['200ashww'],\n",
       " ['#imagine', '#imagines'],\n",
       " ['panthers'],\n",
       " ['egyptian', 'egyptians'],\n",
       " ['academic'],\n",
       " ['29ash138'],\n",
       " ['#loveyou',\n",
       "  '#loveyouguys',\n",
       "  '#loveyoutoo',\n",
       "  '#loveyouzayn',\n",
       "  '#loveyouall',\n",
       "  '#loveyouuu',\n",
       "  '#loveyouu',\n",
       "  '#loveyousomuch',\n",
       "  '#loveyoumore',\n",
       "  '#loveyouliam',\n",
       "  '#loveyougirls',\n",
       "  '#loveyoureyes',\n",
       "  '#loveyouforever',\n",
       "  '#loveyougirl',\n",
       "  '#loveyouboo',\n",
       "  '#loveyoubaby',\n",
       "  '#loveyous',\n",
       "  '#loveyouboth',\n",
       "  '#loveyoubestfriend'],\n",
       " ['disorder', 'disorders', 'disorderly', 'disordered'],\n",
       " ['alphabet', 'alphabets', 'alphabetically'],\n",
       " ['presence'],\n",
       " ['humanity', \"humanity's\"],\n",
       " ['anderson', 'andersonville', \"anderson's\", 'andersons'],\n",
       " ['suspense', 'suspenseful'],\n",
       " ['balloons'],\n",
       " ['200ashgr'],\n",
       " ['130ashbk'],\n",
       " ['thankful', 'thankfully', 'thankfull'],\n",
       " ['brighton'],\n",
       " ['aircraft'],\n",
       " ['splitter', 'splitters'],\n",
       " ['seamless', 'seamlessly'],\n",
       " ['organizer', 'organize', 'organized', 'organizes', 'organizers'],\n",
       " ['textured'],\n",
       " ['smallest'],\n",
       " ['montreal', \"montreal's\"],\n",
       " ['229popch'],\n",
       " ['scottish'],\n",
       " ['coldplay', \"coldplay's\"],\n",
       " ['illinois'],\n",
       " ['discount', 'discounted', 'discounts'],\n",
       " ['beginners', \"beginner's\", 'beginner'],\n",
       " ['sentence', 'sentences', 'sentenced'],\n",
       " ['reported', 'reportedly'],\n",
       " ['domestic', 'domestically'],\n",
       " ['ornament', 'ornaments', 'ornamental'],\n",
       " [\"barron's\"],\n",
       " ['terminal', 'terminals', 'terminally'],\n",
       " ['diamonds', 'diamondscreen'],\n",
       " ['decorate', 'decorated', 'decorates'],\n",
       " ['crossing', 'crossings'],\n",
       " ['surround', 'surrounded', 'surrounding', 'surrounds', 'surroundbar'],\n",
       " ['pleeease', 'pleeeaseee', 'pleeeasee'],\n",
       " ['optiplex'],\n",
       " ['murdered'],\n",
       " ['coasters'],\n",
       " ['crystals'],\n",
       " ['buddhist', 'buddhists'],\n",
       " ['broadway', \"broadway's\"],\n",
       " ['outdoors', 'outdoorsman'],\n",
       " ['yearbook', 'yearbooks'],\n",
       " ['farewell'],\n",
       " ['thorough', 'thoroughly', 'thoroughbred'],\n",
       " ['robotics'],\n",
       " ['marshall', 'marshalltown', 'marshalls'],\n",
       " ['velocity'],\n",
       " ['partying'],\n",
       " ['kitchens'],\n",
       " ['division', 'divisions', 'divisional'],\n",
       " ['showcase', 'showcases', 'showcased'],\n",
       " ['mosquito', 'mosquitoes', 'mosquitos'],\n",
       " ['convince', 'convinced', 'convinces'],\n",
       " ['defender', 'defenders'],\n",
       " ['argument', 'arguments', 'argumentation'],\n",
       " ['floating'],\n",
       " ['dinosaurs', 'dinosaur'],\n",
       " ['memorial'],\n",
       " ['consumer', 'consumers', \"consumer's\", 'consumerism'],\n",
       " ['alliance', 'alliances'],\n",
       " ['feminine'],\n",
       " [\"sister's\"],\n",
       " ['balanced'],\n",
       " ['patients'],\n",
       " ['laughter'],\n",
       " ['kentucky', \"kentucky's\"],\n",
       " ['headband', 'headbands'],\n",
       " ['goodluck', 'goodluckk'],\n",
       " ['hardwood', 'hardwoods'],\n",
       " ['audition', 'auditions', 'auditioning', 'auditioned'],\n",
       " ['valuable', 'valuables'],\n",
       " ['username', 'usernames'],\n",
       " ['insomnia', 'insomniac'],\n",
       " ['infinite', 'infinitely', \"infinite's\"],\n",
       " ['accurate', 'accurately'],\n",
       " ['portland'],\n",
       " ['neighborhood', 'neighbors', 'neighbor', 'neighborhoods', 'neighboring'],\n",
       " ['switched', 'switched-on'],\n",
       " ['maintain', 'maintaining', 'maintains', 'maintained', 'maintainer'],\n",
       " ['district', 'districts'],\n",
       " ['arkansas'],\n",
       " ['200ashch'],\n",
       " ['unlocked', 'unlocked.just'],\n",
       " ['traveler', 'travelers', \"traveler's\"],\n",
       " ['thompson', 'thompsons'],\n",
       " ['resident', 'residents', 'residential'],\n",
       " ['brownies'],\n",
       " ['verbatim', \"verbatim's\"],\n",
       " ['peaceful', 'peacefully', 'peacefull'],\n",
       " ['deserved'],\n",
       " ['shitting'],\n",
       " ['registered', 'register', 'registers'],\n",
       " ['entirely'],\n",
       " ['election', 'elections'],\n",
       " ['climbing'],\n",
       " ['#notcool', '#notcoolbro'],\n",
       " ['offended'],\n",
       " ['incident', 'incidents', 'incidentally'],\n",
       " ['culinary'],\n",
       " ['blankets'],\n",
       " ['triangle', 'triangles'],\n",
       " ['trendnet', 'trendnets'],\n",
       " ['infected'],\n",
       " ['hopeless', 'hopelessly'],\n",
       " ['profiles'],\n",
       " ['miracles', 'miraclesuit'],\n",
       " ['giveaway', 'giveaways'],\n",
       " ['delicate', 'delicately'],\n",
       " ['aquarium', 'aquariums'],\n",
       " ['extender'],\n",
       " ['dragging'],\n",
       " ['creatures', 'creature'],\n",
       " ['calculus'],\n",
       " ['treasury'],\n",
       " ['gripping'],\n",
       " ['fernando'],\n",
       " ['deadline', 'deadlines'],\n",
       " ['audience', 'audiences'],\n",
       " ['numerous'],\n",
       " ['brittany', \"brittany's\", 'brittanys'],\n",
       " ['richmond'],\n",
       " ['lemonade'],\n",
       " ['highland', 'highlander', 'highlands', 'highlanders'],\n",
       " ['#sotired'],\n",
       " ['squirrel', 'squirrels'],\n",
       " ['handling'],\n",
       " ['patience'],\n",
       " ['mushroom', 'mushrooms'],\n",
       " ['injuries'],\n",
       " ['contrast', 'contrasting', 'contrasts'],\n",
       " ['caffeine'],\n",
       " ['wilshere'],\n",
       " ['missouri', \"missouri's\"],\n",
       " ['haunting', 'hauntings', 'hauntingly'],\n",
       " [\"family's\"],\n",
       " ['unlikely'],\n",
       " ['faithful', 'faithfully', 'faithfull'],\n",
       " ['blackout', 'blackouts'],\n",
       " ['seasoned'],\n",
       " ['occasion', 'occasional', 'occasions', 'occasionally'],\n",
       " ['mitchell', \"mitchell's\"],\n",
       " ['circular', 'circulars'],\n",
       " ['skipping'],\n",
       " ['readings'],\n",
       " ['gunmetal'],\n",
       " ['babygirl', 'babygirlll'],\n",
       " ['workouts'],\n",
       " ['lipstick'],\n",
       " ['infrared'],\n",
       " ['basement', 'basements'],\n",
       " ['savannah', \"savannah's\"],\n",
       " ['lawrence', 'lawrenceville'],\n",
       " ['congress', 'congressional', 'congressman'],\n",
       " ['wildlife'],\n",
       " ['patriots'],\n",
       " ['meetings'],\n",
       " ['#notgood', '#notgoodenough'],\n",
       " ['survivor', 'survivors', \"survivor's\"],\n",
       " ['pleaaaseee', 'pleaaase'],\n",
       " ['drilling'],\n",
       " ['diabetes'],\n",
       " ['concrete'],\n",
       " ['collapse', 'collapsed', 'collapses'],\n",
       " ['cardinal', 'cardinals'],\n",
       " ['striking', 'strikingly'],\n",
       " ['robinson', \"robinson's\"],\n",
       " ['partners', 'partnership', 'partnerships'],\n",
       " ['hedgehog', 'hedgehogs'],\n",
       " ['earliest'],\n",
       " ['campbell', \"campbell's\", 'campbells'],\n",
       " ['vocalist', \"vocalist's\", 'vocalists'],\n",
       " ['sticking'],\n",
       " ['munchkin'],\n",
       " ['dazzling'],\n",
       " ['overseas'],\n",
       " ['in-depth'],\n",
       " ['embedded'],\n",
       " ['betrayal', 'betrayals'],\n",
       " ['eternity', \"eternity's\"],\n",
       " ['attorney'],\n",
       " ['riveting'],\n",
       " ['peterson', \"peterson's\"],\n",
       " ['mandarin', 'mandarins'],\n",
       " ['immortal', 'immortals', 'immortality', 'immortalis'],\n",
       " ['forensic', 'forensics'],\n",
       " ['cellular', 'cellularfactory'],\n",
       " ['youngest'],\n",
       " ['sexually'],\n",
       " ['keystone'],\n",
       " ['flipping'],\n",
       " ['flawless', 'flawlesss'],\n",
       " ['facetime', 'facetimed'],\n",
       " ['weathers', 'weatherstrip', 'weatherspoons'],\n",
       " ['leapfrog'],\n",
       " ['glorious', 'gloriously'],\n",
       " ['theories'],\n",
       " ['ceremony'],\n",
       " ['reminder'],\n",
       " ['bastards'],\n",
       " ['tailgate', 'tailgater'],\n",
       " ['prophecy'],\n",
       " ['mortgage', 'mortgages'],\n",
       " ['drenched'],\n",
       " ['nonstick'],\n",
       " ['focusing'],\n",
       " ['composer', 'composers', \"composer's\"],\n",
       " ['brighten', 'brightens', 'brightened'],\n",
       " ['vanguard', 'vanguardia'],\n",
       " ['optional'],\n",
       " ['operator', 'operators'],\n",
       " ['musicians', 'musician', \"musician's\"],\n",
       " ['jetflash'],\n",
       " ['skrillex'],\n",
       " ['homemade'],\n",
       " [\"doctor's\"],\n",
       " ['sulphite'],\n",
       " ['railroad', 'railroads'],\n",
       " ['enduring'],\n",
       " ['cosmetic', 'cosmetics'],\n",
       " ['chestnut', \"chestnut's\", 'chestnuts'],\n",
       " ['steelers', 'steelersmobile'],\n",
       " ['stanford'],\n",
       " ['finishes'],\n",
       " ['emerging'],\n",
       " ['betrayed'],\n",
       " ['athletes'],\n",
       " ['all-stars'],\n",
       " ['sporting'],\n",
       " ['plymouth'],\n",
       " [\"nobody's\"],\n",
       " ['figurine', 'figurines'],\n",
       " ['benjamin', \"benjamin's\"],\n",
       " ['stoppers'],\n",
       " ['novelist', 'novelists'],\n",
       " ['bulldogs'],\n",
       " ['wildcats'],\n",
       " ['pullover'],\n",
       " ['predator', 'predators', 'predatory'],\n",
       " ['nigerian', 'nigerians'],\n",
       " ['maximize', 'maximizer', 'maximizes'],\n",
       " [\"artist's\"],\n",
       " ['planters'],\n",
       " ['majestic', \"majestic's\"],\n",
       " ['keepsake', 'keepsakes'],\n",
       " ['harrison', \"harrison's\"],\n",
       " ['cherokee', 'cherokees'],\n",
       " ['bearings'],\n",
       " ['proliant'],\n",
       " ['otterbox'],\n",
       " ['nauseous'],\n",
       " ['mechanics', 'mechanical', 'mechanic', 'mechanicsville'],\n",
       " ['founding'],\n",
       " ['artistic', 'artistically'],\n",
       " ['uniquely'],\n",
       " ['spoilers'],\n",
       " ['remedies'],\n",
       " ['generous', 'generously'],\n",
       " ['crashing'],\n",
       " ['#sadlife'],\n",
       " ['suburban'],\n",
       " ['pleasant', 'pleasanton', 'pleasantly'],\n",
       " ['plastics'],\n",
       " ['filipino', 'filipinos', \"filipino's\"],\n",
       " ['electrol',\n",
       "  'electroluminescent',\n",
       "  'electrolyte',\n",
       "  'electrolytes',\n",
       "  'electroline',\n",
       "  'electrolux',\n",
       "  'electrolysis'],\n",
       " ['conquest', 'conquests'],\n",
       " ['thirteen', 'thirteen-year-old', 'thirteenth', 'thirteenth-century'],\n",
       " ['nutshell'],\n",
       " ['marinade', 'marinades'],\n",
       " ['heavenly'],\n",
       " ['concerto', 'concertos'],\n",
       " ['tonights'],\n",
       " ['sylvania'],\n",
       " ['mercedes'],\n",
       " ['gemstone', 'gemstones'],\n",
       " ['entrance', 'entranced'],\n",
       " ['cylinder', 'cylinders'],\n",
       " ['zippered'],\n",
       " ['trousers'],\n",
       " ['sounding', 'soundings'],\n",
       " ['provider', 'providers'],\n",
       " ['produces'],\n",
       " ['membrane', 'membranes'],\n",
       " ['hispanic', 'hispanics'],\n",
       " ['auckland'],\n",
       " ['toddlers'],\n",
       " ['samantha', \"samantha's\"],\n",
       " ['oriental', 'orientale'],\n",
       " ['napoleon', \"napoleon's\", 'napoleonic'],\n",
       " ['minister', \"minister's\", 'ministering'],\n",
       " ['casebook', 'casebooks'],\n",
       " ['breakout', 'breakouts'],\n",
       " ['blessing', 'blessings'],\n",
       " ['archival'],\n",
       " ['#nobueno'],\n",
       " ['variable', 'variables'],\n",
       " ['flirting'],\n",
       " ['detector', 'detectors'],\n",
       " ['colombia', 'colombian', \"colombia's\", 'colombiana'],\n",
       " ['verified'],\n",
       " ['tuesdays'],\n",
       " ['snapshot', 'snapshots'],\n",
       " ['emmanuel'],\n",
       " ['unframed'],\n",
       " ['rosemary', \"rosemary's\"],\n",
       " ['retainer', 'retainers'],\n",
       " ['mourning'],\n",
       " ['jamaican', 'jamaicans'],\n",
       " ['feedback', 'feedbacks'],\n",
       " ['divorced'],\n",
       " ['callaway'],\n",
       " ['babydoll'],\n",
       " ['assassin',\n",
       "  'assassins',\n",
       "  'assassination',\n",
       "  \"assassin's\",\n",
       "  'assassinated',\n",
       "  'assassinate'],\n",
       " ['45-0001-'],\n",
       " ['symbolic'],\n",
       " ['postcards', 'postcard'],\n",
       " ['jealousy', \"jealousy's\"],\n",
       " ['dreadful'],\n",
       " ['dialogue', 'dialogues'],\n",
       " ['affected'],\n",
       " ['woodland', 'woodlands'],\n",
       " ['vitamins'],\n",
       " [\"reader's\"],\n",
       " ['nickname', 'nicknames', 'nicknamed'],\n",
       " ['instinct', 'instincts', 'instinctive'],\n",
       " ['hazelnut', 'hazelnuts'],\n",
       " ['assholes'],\n",
       " ['sketches'],\n",
       " ['pulitzer'],\n",
       " ['corvette', 'corvettes'],\n",
       " ['refollow', 'refollowed'],\n",
       " ['missions'],\n",
       " ['makeover', 'makeovers'],\n",
       " ['catalyst'],\n",
       " ['canisters', 'canister'],\n",
       " ['terrific'],\n",
       " ['surgical'],\n",
       " ['smashing'],\n",
       " ['pacifier', 'pacifiers'],\n",
       " ['overcome'],\n",
       " ['garfield', \"garfield's\"],\n",
       " ['assembled', 'assemble', 'assembles', 'assembler'],\n",
       " ['warcraft'],\n",
       " ['troubles',\n",
       "  'troubleshooting',\n",
       "  'troublesome',\n",
       "  'troubleshooters',\n",
       "  'troubleshoot'],\n",
       " ['pitching'],\n",
       " ['pedestal', 'pedestals'],\n",
       " ['homesick'],\n",
       " ['exposure', 'exposures'],\n",
       " ['earphones', 'earphone'],\n",
       " ['crawford', \"crawford's\"],\n",
       " ['swimwear'],\n",
       " ['sardines'],\n",
       " ['ofcourse'],\n",
       " ['motivated', 'motivate'],\n",
       " ['lecturer', 'lecturers'],\n",
       " ['blogging'],\n",
       " ['45-0002-'],\n",
       " ['tortilla', 'tortillas'],\n",
       " ['spitfire', 'spitfires'],\n",
       " ['reunited'],\n",
       " ['pictured'],\n",
       " ['helpless', 'helplessly'],\n",
       " ['feminist', 'feminists'],\n",
       " ['builders'],\n",
       " ['backdrop', 'backdrops'],\n",
       " ['skittles'],\n",
       " ['mousepad'],\n",
       " ['fastener', 'fasteners'],\n",
       " ['arriving'],\n",
       " ['somemore'],\n",
       " ['rechargeable', 'recharge', 'recharged'],\n",
       " ['illusion', 'illusions'],\n",
       " ['dedicated', 'dedicate', 'dedicates'],\n",
       " ['all-time'],\n",
       " [\"school's\"],\n",
       " ['happening', 'happenin'],\n",
       " ['cambodia', 'cambodian'],\n",
       " ['restless', 'restlessness'],\n",
       " ['pringles'],\n",
       " ['moderate', 'moderately', 'moderated'],\n",
       " ['graffiti'],\n",
       " ['doctrine', 'doctrines'],\n",
       " ['chipmunk', 'chipmunks'],\n",
       " ['weddings'],\n",
       " ['stockings', 'stocking'],\n",
       " ['stacking'],\n",
       " ['mobility'],\n",
       " ['fearless', 'fearlessly'],\n",
       " ['clippers'],\n",
       " ['powdered'],\n",
       " ['pinnacle', \"pinnacle's\"],\n",
       " [\"newman's\"],\n",
       " ['morrison', \"morrison's\"],\n",
       " ['inverter', 'inverters'],\n",
       " ['identify', 'identifying'],\n",
       " ['diffuser'],\n",
       " ['comedian', 'comedians'],\n",
       " ['boutique'],\n",
       " ['watering'],\n",
       " ['rosewood'],\n",
       " ['prisoner', 'prisoners'],\n",
       " [\"nation's\"],\n",
       " ['judgmental', 'judgment'],\n",
       " ['feathers'],\n",
       " ['confetti'],\n",
       " ['citizens', 'citizenship'],\n",
       " ['advocate', 'advocates'],\n",
       " ['surfaces'],\n",
       " ['prestige'],\n",
       " ['inventor', 'inventory', 'inventors', \"inventor's\"],\n",
       " ['elevator'],\n",
       " ['downside', 'downsides'],\n",
       " ['childish'],\n",
       " ['acrylics'],\n",
       " ['tabletop'],\n",
       " ['quitting'],\n",
       " ['pioneers'],\n",
       " ['ordering'],\n",
       " ['massacre', 'massacred'],\n",
       " ['margaret', \"margaret's\"],\n",
       " ['farscape'],\n",
       " ['einstein', \"einstein's\", 'einsteins'],\n",
       " ['dividers'],\n",
       " ['bodysuit'],\n",
       " ['barefoot', 'barefootstudent'],\n",
       " ['t-shirts'],\n",
       " ['reeeally'],\n",
       " ['province', 'provinces'],\n",
       " ['magician', 'magicians', \"magician's\"],\n",
       " ['intrigue', 'intrigued'],\n",
       " ['grandmas', 'grandmaster'],\n",
       " ['entitled'],\n",
       " ['disagree', 'disagreed', 'disagreement', 'disagreeing'],\n",
       " ['ceramics'],\n",
       " ['t-handle'],\n",
       " ['ruthless'],\n",
       " ['fighters'],\n",
       " ['commuter', 'commuters'],\n",
       " ['blizzard'],\n",
       " ['balsamic', 'balsamico'],\n",
       " ['#imsorry'],\n",
       " ['valencia', \"valencia's\"],\n",
       " ['showdown'],\n",
       " ['sequence', 'sequences', 'sequencer'],\n",
       " ['scholarship', 'scholars', 'scholarships'],\n",
       " ['purifier', 'purifiers'],\n",
       " ['moroccan', 'moroccanoil'],\n",
       " ['magnolia', 'magnolias'],\n",
       " ['licorice'],\n",
       " ['enriched'],\n",
       " ['circuits'],\n",
       " ['angelina', \"angelina's\"],\n",
       " ['admitted'],\n",
       " ['teething'],\n",
       " ['scrabble'],\n",
       " ['radiator', 'radiators'],\n",
       " ['openings'],\n",
       " ['horribly'],\n",
       " ['bookmark', 'bookmarks', 'bookmarked', 'bookmarking'],\n",
       " ['wrangler', 'wranglers'],\n",
       " ['troubled'],\n",
       " ['stamford'],\n",
       " ['marching'],\n",
       " ['jacquard'],\n",
       " ['freshmen', 'freshmens'],\n",
       " ['foremost'],\n",
       " ['enhancer', 'enhancers'],\n",
       " ['binaural'],\n",
       " ['benedict', 'benedictine', \"benedict's\"],\n",
       " ['spirited'],\n",
       " ['serrated'],\n",
       " ['memorize', 'memorized'],\n",
       " ['homicide'],\n",
       " ['hallmark', 'hallmarks'],\n",
       " ['compiled'],\n",
       " ['clearing'],\n",
       " ['bulletin', 'bulletins'],\n",
       " ['#sacfree'],\n",
       " ['utilizes'],\n",
       " ['sentinel', 'sentinels'],\n",
       " ['relevant'],\n",
       " ['potpourri', 'potpourr'],\n",
       " ['mustache', 'mustaches'],\n",
       " ['imessage', 'imessages'],\n",
       " ['hehehehe', 'hehehehehe', 'heheheheh'],\n",
       " ['abortion', 'abortions'],\n",
       " ['sunglasses', 'sunglass'],\n",
       " ['shutdown'],\n",
       " ['reflects'],\n",
       " ['profound', 'profoundly'],\n",
       " ['monterey'],\n",
       " ['monogrammed', 'monogram'],\n",
       " ['interactive',\n",
       "  'interaction',\n",
       "  'interactions',\n",
       "  'interact',\n",
       "  'interacting',\n",
       "  'interactivity',\n",
       "  'interacts',\n",
       "  'interacti'],\n",
       " ['headline', 'headlines', 'headliner', 'headliners'],\n",
       " ['grilling'],\n",
       " ['airlines'],\n",
       " ['walthers'],\n",
       " ['sweaters'],\n",
       " ['orthodox', 'orthodoxy'],\n",
       " ['maverick', 'mavericks'],\n",
       " ['lifelong'],\n",
       " ['investors', 'investor', \"investor's\"],\n",
       " ['ethereal'],\n",
       " ['draining'],\n",
       " ['bursting'],\n",
       " ['accented'],\n",
       " ['#unloved'],\n",
       " ['unveiled'],\n",
       " ['tripping'],\n",
       " ['textiles'],\n",
       " ['technology',\n",
       "  'technologies',\n",
       "  'technological',\n",
       "  'technolo',\n",
       "  'technolog',\n",
       "  \"technology's\",\n",
       "  'technologist',\n",
       "  'technologically',\n",
       "  'technologists',\n",
       "  'technologie',\n",
       "  'technologi'],\n",
       " ['starfish'],\n",
       " ['seashell', 'seashells'],\n",
       " ['prettier'],\n",
       " ['numbered'],\n",
       " ['murderer', 'murderers'],\n",
       " ['kendrick'],\n",
       " ['interfit'],\n",
       " ['folklore'],\n",
       " ['finalist', 'finalists'],\n",
       " ['disgrace', 'disgraced', 'disgraceful'],\n",
       " ['courtesy'],\n",
       " ['budapest'],\n",
       " ['ar2169lh'],\n",
       " ['#toronto', '#torontofree'],\n",
       " ['wolfgang'],\n",
       " ['tailored'],\n",
       " ['stitched'],\n",
       " ['shepherd', \"shepherd's\", 'shepherds'],\n",
       " ['roadmate'],\n",
       " ['relationship', 'relationships', 'relations', 'relation', 'relational'],\n",
       " ['redskins'],\n",
       " ['overload', 'overloaded'],\n",
       " ['marrying'],\n",
       " ['fourteen', 'fourteen-year-old', 'fourteenth', 'fourteen-year'],\n",
       " ['epidemic', 'epidemics'],\n",
       " ['currency'],\n",
       " ['coupling', 'couplings'],\n",
       " ['clubbing'],\n",
       " ['bushnell'],\n",
       " ['bradford', 'bradfordian'],\n",
       " ['apparently', 'apparent'],\n",
       " ['vodafone'],\n",
       " ['requests'],\n",
       " ['preserve', 'preserved', 'preserves', 'preserver'],\n",
       " ['nativity'],\n",
       " ['headsets'],\n",
       " ['forestry'],\n",
       " ['feminism'],\n",
       " ['clarkson'],\n",
       " ['bromance', 'bromances'],\n",
       " ['berkeley'],\n",
       " ['#calgary', '#calgaryexpo'],\n",
       " ['spanning'],\n",
       " ['rational', 'rationality', 'rationales', 'rationale'],\n",
       " ['porridge'],\n",
       " ['munchies'],\n",
       " ['ignorant'],\n",
       " ['enormous', 'enormously'],\n",
       " ['delaware'],\n",
       " ['zirconia'],\n",
       " ['supplier', 'suppliers'],\n",
       " ['sidewalk', 'sidewalks'],\n",
       " ['scorpion', 'scorpions'],\n",
       " ['rawlings'],\n",
       " ['knocking'],\n",
       " ['ivanovic', \"ivanovic's\"],\n",
       " ['gillette'],\n",
       " ['exposing'],\n",
       " ['destined'],\n",
       " ['battling'],\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word  occurence\n",
      "1152  1.26          5\n",
      "       word  occurence\n",
      "19902  misc         15\n"
     ]
    }
   ],
   "source": [
    "print(test_data.loc[test_data.word.str.startswith(\"1.26\")])\n",
    "print(test_data.loc[test_data.word.str.startswith(\"misc\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the datafram to keep only ratio = 1 and diff > `MIN_diff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_diff = 200  # TBD accordingly to : As soon as we see a \"pos_for_sure or a neg_for_sure appearing\n",
    "                # within the same tweet, Min_diff = difference + 1 of the smallest one of the two words.\"\n",
    "\n",
    "mf_max_ratio = merged_full[(merged_full.ratio == 1) & (merged_full.difference >= MIN_diff)]\n",
    "mf_max_ratio = mf_max_ratio[[\"word\",\"difference\"]]\n",
    "word_max_ratio = list(mf_max_ratio.values)\n",
    "\n",
    "#def set_min_diff_and_delete_uninteressting()\n",
    "\n",
    "type(word_max_ratio[0][1])\n",
    "\n",
    "def powerful_words():\n",
    "    MIN_diff = 200  # TBD accordingly to : As soon as we see a \"pos_for_sure or a neg_for_sure appearing\n",
    "                # within the same tweet, Min_diff = difference + 1 of the smallest one of the two words.\"\n",
    "\n",
    "    mf_max_ratio = merged_full[(merged_full.ratio == 1) & (merged_full.difference >= MIN_diff)]\n",
    "    mf_max_ratio = mf_max_ratio[[\"word\",\"difference\"]]\n",
    "    word_max_ratio = list(mf_max_ratio.values)\n",
    "    return word_max_ratio\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12263</th>\n",
       "      <td>ahah</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12264</th>\n",
       "      <td>ahaha</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12265</th>\n",
       "      <td>ahahaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12266</th>\n",
       "      <td>ahahah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12267</th>\n",
       "      <td>ahahaha</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12268</th>\n",
       "      <td>ahahhaah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17306</th>\n",
       "      <td>haha</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17307</th>\n",
       "      <td>hahaa</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17308</th>\n",
       "      <td>hahaaaha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17309</th>\n",
       "      <td>hahaah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17310</th>\n",
       "      <td>hahah</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17311</th>\n",
       "      <td>hahaha</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17312</th>\n",
       "      <td>hahahaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17313</th>\n",
       "      <td>hahahaaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17314</th>\n",
       "      <td>hahahah</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17315</th>\n",
       "      <td>hahahaha</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17316</th>\n",
       "      <td>hahahahah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17317</th>\n",
       "      <td>hahahahaha</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17318</th>\n",
       "      <td>hahahahahah</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17319</th>\n",
       "      <td>hahahahahaha</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17320</th>\n",
       "      <td>hahahahahahaha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17321</th>\n",
       "      <td>hahahahahh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17322</th>\n",
       "      <td>hahahahha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17323</th>\n",
       "      <td>hahahahhahaha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17324</th>\n",
       "      <td>hahahha</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17325</th>\n",
       "      <td>hahahhaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  occurence\n",
       "12263            ahah          3\n",
       "12264           ahaha         12\n",
       "12265          ahahaa          1\n",
       "12266          ahahah          2\n",
       "12267         ahahaha          8\n",
       "12268        ahahhaah          1\n",
       "17306            haha        249\n",
       "17307           hahaa          6\n",
       "17308        hahaaaha          1\n",
       "17309          hahaah          2\n",
       "17310           hahah         27\n",
       "17311          hahaha         56\n",
       "17312         hahahaa          1\n",
       "17313        hahahaaa          1\n",
       "17314         hahahah          4\n",
       "17315        hahahaha         17\n",
       "17316       hahahahah          1\n",
       "17317      hahahahaha          5\n",
       "17318     hahahahahah          1\n",
       "17319    hahahahahaha          2\n",
       "17320  hahahahahahaha          1\n",
       "17321      hahahahahh          1\n",
       "17322       hahahahha          1\n",
       "17323   hahahahhahaha          1\n",
       "17324         hahahha          2\n",
       "17325        hahahhaa          1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_haha = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurence of the words that can be remplaced by haha = 407\n"
     ]
    }
   ],
   "source": [
    "print(\"Occurence of the words that can be remplaced by haha = \"+ str(word_haha.occurence.sum()))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6cda69b1879e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_data.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfirstTweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfirstTweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_data.txt'"
     ]
    }
   ],
   "source": [
    "with open('test_data.txt', 'r') as f:\n",
    "    tweets = [line.strip()[line.find(\",\")+1:] for line in f]     # Make sure to withdraw the \"nbr\", \n",
    "\n",
    "firstTweet = tweets[0]\n",
    "firstTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 7072: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-09b87a6d408d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtweets_pos_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tweets_pos_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtweets_pos_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tweets_neg_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-09b87a6d408d>\u001b[0m in \u001b[0;36mimport_\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-09b87a6d408d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 7072: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def import_(path):\n",
    "    with open(path, 'r') as f:\n",
    "        tweets = [line.strip()[line.find(\",\")+1:] for line in f]     # Make sure to withdraw the \"nbr\", \n",
    "    return tweets\n",
    "\n",
    "def export(tweets, name):\n",
    "    with open(name, 'w') as f:\n",
    "        f.write(\"\\n\".join(tweets))\n",
    "    \n",
    "\n",
    "tweets_pos_full = import_(path_tweets_pos_full)\n",
    "tweets_pos_full = import_(path_tweets_neg_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sem_by_repr(semantics, representative, tweet):\n",
    "    \"\"\"Retrun a tweet that countain only the representative of a given semantic \"\"\"\n",
    "    \n",
    "    for semantic in semantics: \n",
    "        if (semantic in tweet):\n",
    "            tweet = tweet.replace(semantic,representative)\n",
    "    return tweet\n",
    "\n",
    "def sem_by_repr2(semantics, tweet):\n",
    "    \"\"\"Semanticss is a list of list of semantic where the first one is the representative.\n",
    "    Retrun a tweet that countain only the representative of a given semantic below is an exemple of the semantics\n",
    "    \n",
    "    [['because', 'becausee'],\n",
    "    ['someone', \"someone's\", 'someones', 'someonee'],\n",
    "    ['friends', 'friendship', 'friendships', 'friendss', 'friendster']]\n",
    "    \"\"\"\n",
    "    \n",
    "    for semantic in semantics:\n",
    "        representative = semantic[0]        \n",
    "        sem = semantic[1:]       \n",
    "        sem = sorted(sem, key =len, reverse= True)  # We sort by lenth on a non Ascending way to avoid the case where a\n",
    "                                                    # String is a substring and would chang\n",
    "        \n",
    "        tweet = sem_by_repr(sem , representative, tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bon haha haha yo asdef afsv yo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem = [[\"haha\",\"hahah\", \"hahahhhahhh\", \"okok\",\"dw\"],[\"yo\", \"yooy\", \"yoooooowr\"]]\n",
    "t = \"bon hahahhhahhh haha yoooooowr asdef afsv yooy\"\n",
    "\n",
    "\n",
    "sem_by_repr2(sem, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'dd', 'ddd', 'dddd', 'ddddd']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_dot(tweet):\n",
    "    tweet = tweet.split()    \n",
    "    for i, t in enumerate(tweet):\n",
    "        if t == \"...\":\n",
    "            continue\n",
    "        if \".\" in t:\n",
    "            if t[-1] == \".\":\n",
    "                t = t[:-1]\n",
    "            \n",
    "            t = t.replace(\".\",\" \")\n",
    "            tweet[i] = t\n",
    "    return \" \".join(tweet)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def find_repetition(tweet):    \n",
    "    copy = \"\"    \n",
    "    i = 0\n",
    "    while i<len(tweet):        \n",
    "        if (i<len(tweet)-3 and tweet[i] == tweet[i+1] and tweet[i] == tweet[i+2]):\n",
    "            i = i+2\n",
    "            while( i<len(tweet)-1 and tweet[i] == tweet[i+1] ):\n",
    "                i+=1 \n",
    "            copy += tweet[i]\n",
    "            i+=1\n",
    "        else:\n",
    "            copy += tweet[i]\n",
    "            i+=1    \n",
    "    return copy\n",
    "\n",
    "\n",
    "def no_s(tweet): \n",
    "    tweet = tweet.split()    \n",
    "    for i, t in enumerate(tweet):\n",
    "        \n",
    "        if(len(t)>4 and t[-3:] == \"ies\"):\n",
    "            tweet[i] = t[0:-3] + \"y\"\n",
    "            \n",
    "        if(len(t)>4 and t[-2:] == \"es\"):\n",
    "            tweet[i] = t[0:-2] + \"e\"\n",
    "            \n",
    "        if(len(t)>4 and t[-1]==\"s\"  and t[-2]!=\"s\"):\n",
    "            tweet[i] = t[0:-1]\n",
    "            \n",
    "    tweet = \" \".join(tweet)\n",
    "    return tweet\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tweet(tweet):\n",
    "    tweet = tweet.split()    \n",
    "    for i, t in enumerate(tweet):\n",
    "        t[i] = ps.stem(t)\n",
    "        \n",
    "    return \" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We could improve the processed tweets by first f\n",
    "def process_tweets_1(tweets, semantic):\n",
    "    \"filter and uniform the tweets \"\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tweet = find_repetition(tweet)\n",
    "        tweet = no_s(tweet)      \n",
    "        tweet = no_dot(tweet)\n",
    "    return tweets\n",
    "\n",
    "#We could improve the processed tweets by first f\n",
    "def process_tweets_2(tweets, semantic):\n",
    "    \"filter and uniform the tweets \"\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tweet = sem_by_repr2(semantic, tweet)  #remplace haha\n",
    "        tweets[i] = stem_tweet(tweet)\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweets_pos_full = process_tweets( tweets_pos_full, word_haha_list, \"haha\") #Sous form d'un tableau de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweets_neg_full = process_tweets( tweets_pos_full, word_haha_list, \"haha\") #Sous form d'un tableau de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export(processed_tweets_neg_full, \"processed_tweet_neg_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_tweets_neg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_no_stem():\n",
    "    ########----------Build DF----------------------------------------\n",
    "    #### -------------- Partial tweets -------------------------------\n",
    "   \n",
    "    \n",
    "    #import tweets\n",
    "    with open('test_data.txt', 'r') as f:\n",
    "        tweets_data_full = [line.strip()[line.find(\",\")+1:] for line in f]     # Make sure to withdraw the \"nbr\",     \n",
    "    tweets_pos_full =  import_(path_tweets_pos_full)\n",
    "    tweets_neg_full =  import_(path_tweets_neg_full)\n",
    "    \n",
    "    #process tweets\n",
    "    processed_tweets_pos_full = process_tweets( tweets_pos_full, semantic)\n",
    "    processed_tweets_neg_full = process_tweets( tweets_pos_full, semantic) #Sous form d'un tableau de tweet   \n",
    "    processed_tweets_data_full = process_tweets( tweets_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    #export tweets\n",
    "    export(processed_tweets_neg_full,  \"processed_tweet_neg_full\")\n",
    "    export(processed_tweets_pos_full,  \"processed_tweet_pos_full\")\n",
    "    export(processed_tweets_data_full, \"processed_tweet_data_full\")\n",
    "    \n",
    "\n",
    "def stem_data():\n",
    "    \n",
    "    path_pos_full = \"vocab_pos_full_to_stem\"\n",
    "    path_neg_full = \"vocab_neg_full_to_stem\"\n",
    "    path_test_data = \"vocab_data_to_stem\"\n",
    "    \n",
    "    # build the DF\n",
    "\n",
    "    #### -------------- Full tweets ----------------------------------\n",
    "    pos_full = build_df(path_pos_full)\n",
    "    neg_full = build_df(path_neg_full)\n",
    "\n",
    "    #### -------------- Train tweets ----------------------------------\n",
    "\n",
    "    test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "    #Merge Pos and neg vocabs    \n",
    "    merged = merging(neg, pos, True)\n",
    "    merged_full = merging(neg_full, pos_full, True)\n",
    "    merged_full = merging(merged_full, test_data, True)\n",
    "    \n",
    "    #Build equivalence list\n",
    "    same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "    same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "    \n",
    "    #build \"haha\" equivalences\n",
    "    word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "    word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "    word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    word_haha = list(set(word_haha_test+word_haha_pos+word_haha_neg))\n",
    "    word_haha_list = list(word_haha.word).remove(\"haha\")\n",
    "    word_haha_list.insert(0, \"haha\")\n",
    "    \n",
    "    #append the two semantic lists and filter them\n",
    "    semantic = filter_single_rep(word_haha_list + same_begin)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 7072: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d08f2841f4a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocess_data_no_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-abcf2e347a8a>\u001b[0m in \u001b[0;36mprocess_data_no_stem\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtweets_data_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtweets_pos_full\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tweets_pos_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtweets_neg_full\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_tweets_neg_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#process tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-09b87a6d408d>\u001b[0m in \u001b[0;36mimport_\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-09b87a6d408d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimport_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# Make sure to withdraw the \"nbr\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lucas\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 7072: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "process_data_no_stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_data(lower_bound):\n",
    "    \n",
    "    #paths of positive and negative vocabs\n",
    "    path_pos = \"./vocab_cut_pos_full.txt\"\n",
    "    path_neg = \"./vocab_cut_neg_full.txt\"\n",
    "    \n",
    "    # pos is mapping of words in happy tweets with their occurences in all happy tweets\n",
    "    pos = pd.read_table(filepath_or_buffer = path_pos, header=None, names=[\"word\"])\n",
    "    pos[\"occurence\"] = pos[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    pos[\"word\"] = pos[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "\n",
    "    # neg is mapping of words in sad tweets with their occurences in all sad tweets\n",
    "    neg = pd.read_table(filepath_or_buffer = path_neg, header=None, names=[\"word\"])\n",
    "    neg[\"occurence\"] = neg[\"word\"].map(lambda x:  int(x.split()[0]))\n",
    "    neg[\"word\"] = neg[\"word\"].map(lambda x:  x.encode('utf-8').split()[1])\n",
    "    \n",
    "    # We merge the two dataframe in order to better handle them\n",
    "    merged = pd.merge(left=neg, right=pos, left_on = \"word\", right_on = \"word\", suffixes=('_neg', '_pos'),  how=\"outer\")\n",
    "    merged = merged.fillna(0)\n",
    "\n",
    "    #We only consider words whose occurences dfferences between sad and happy tweets is greater or equal than 5 \n",
    "    merged[\"difference\"] = abs((merged[\"occurence_neg\"]-merged[\"occurence_pos\"]))\n",
    "    merged = merged[merged[\"difference\"]>=5]\n",
    "\n",
    "    #We compute the sum of occurences\n",
    "    merged[\"somme\"] = merged[\"occurence_neg\"]+merged[\"occurence_pos\"]\n",
    "\n",
    "    #The ratio si how relevant it is to judge happyness/sadness of the tweet using the word : 0 if not relevant, 1 if truly relevant\n",
    "    merged[\"ratio\"] = 2* abs(0.5 - merged[\"occurence_pos\"]/(merged[\"occurence_pos\"]+merged[\"occurence_neg\"]))\n",
    "    \n",
    "    \n",
    "    def lower_ratio(x) :\n",
    "        if(x[\"somme\"]<MIN_SOMME):\n",
    "            return 0\n",
    "        else:\n",
    "            return x[\"ratio\"]\n",
    "    \n",
    "    \n",
    "    #We only consider with more than 'lower_bound' occurences\n",
    "    merged[\"ratio\"] = merged.apply(lower_ratio, axis = 1) \n",
    "    \n",
    "    #sort the array by ratio and then sum\n",
    "    merged.sort_values(by = [\"ratio\",\"somme\"], ascending=[False, False])\n",
    "    \n",
    "    #store the data\n",
    "    filename = \"relevant_vocab_full_lb=\"+str(lower_bound)+\".txt\"\n",
    "    merged.to_csv(path_or_buf=filename, sep=' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for lb in [50, 100, 500, 1000, 2000, 5000, 10000, 20000, 50000]:\n",
    "create_data(5000)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
