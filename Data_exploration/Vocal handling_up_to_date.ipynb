{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data one is given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple words do we have, are there some duplicates, is there any odds with the tweets? How are they classified (we are given two datasets, one for the positive tweets one for negatives) i.e. what is a typical word that occurs when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may contain words that are not usefull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than three following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Helper functions and files\n",
    "\n",
    "IOTweets contains the functions :\n",
    "- build_df\n",
    "- import_\n",
    "- export\n",
    "\n",
    "ProcessTweets contains the functions :\n",
    "- merging\n",
    "- filter_single_rep\n",
    "- powerful_words\n",
    "- sem_by_repr\n",
    "- sem_by_repr2\n",
    "- no_dot\n",
    "- find_repetition\n",
    "- no_s\n",
    "- stem_tweet\n",
    "- set_min_diff\n",
    "- contains\n",
    "- process_tweets_1\n",
    "- process_tweets_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import csv\n",
    "#%pylab inline # depreceated, use individual imports instead\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_pos = \"vocab_cut_pos.txt\"\n",
    "vocab_neg = \"vocab_cut_neg.txt\"\n",
    "\n",
    "vocab_pos_full = \"vocab_cut_pos_full.txt\"\n",
    "vocab_neg_full = \"vocab_cut_neg_full.txt\"\n",
    "vocab_test = \"vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos = \"train_pos.txt\"\n",
    "path_tweets_neg = \"train_neg.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"train_neg_full.txt\"\n",
    "path_tweets_test = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An insight into the dataset \n",
    "We begin by analysing the non-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_vocab(tweets, cut_threshold, file_name , bitri):\n",
    "    counter = build_vocab_counter(tweets, cut_threshold, bitri)\n",
    "    write_vocab_to_file(counter, (file_name + \"_cut=\" +str(cut_threshold) +\"_bitri=\"+str(bitri)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tweets\n",
    "pos_tweets = import_(path_tweets_pos)\n",
    "neg_tweets = import_(path_tweets_neg)\n",
    "pos_full_tweets = import_(path_tweets_pos_full)\n",
    "neg_full_tweets = import_(path_tweets_neg_full)\n",
    "test_tweets = import_without_id(path_tweets_test)\n",
    "\n",
    "#Create vocabs\n",
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "write_vocab(pos_tweets, cut_threshold, \"raw_vocab_pos\" , bitri)\n",
    "write_vocab(neg_tweets, cut_threshold, \"raw_vocab_neg\" , bitri)\n",
    "write_vocab(test_tweets, cut_threshold, \"raw_vocab_test\" , bitri)\n",
    "write_vocab(pos_full_tweets, cut_threshold, \"raw_vocab_pos_full\" , bitri)\n",
    "write_vocab(neg_full_tweets, cut_threshold, \"raw_vocab_neg_full\" , bitri)\n",
    "\n",
    "\n",
    "# build the vocab Dataframe (DF) for full positive and negative tweets\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "# build the vocab Dataframe (DF) for partial positive and negative tweets\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Study of word pertinence \n",
    "The idea is that the greater a word ratio i, the more pertinent it will be to cosider it when trying to label a tweet that contains it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_full_df\n",
    "\n",
    "#merge the pos and neg vocabs and study penrtinence of words via ratio\n",
    "merged_full = merging(neg_full_df, pos_full_df)\n",
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[True, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>\n",
    "    \n",
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Study of names with same beginning \n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are words (filtered by length) whose beginning match another word close to this word?\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "\n",
    "#We visualize which word would be mapped on which one \n",
    "filter_single_rep(same_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expected syntaxic divergences\n",
    "As expected, words like \"haha\" can be written in many ways (extended length, typo..). We know that they mean the same,it is therefore interesting to spot them in order to later stem them to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words beginning with \"haha\" or \"ahah\" are sure to be \"instances\" of haha\n",
    "word_haha = test_df.loc[test_df.word.str.startswith(\"haha\") | test_df.word.str.startswith(\"ahah\")]\n",
    "\n",
    "\n",
    "#print(\"Occurence of the words that can be remplaced by haha = \"+ str(int(word_haha.occurence.sum()) - int(word_haha[word_haha.word == \"haha\"].occurence)))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)\n",
    "list(word_haha.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Study duplicates\n",
    "We noticed that several tweets were duplicated in test, pos and neg tweets. We study ho many they are, and which tweets (if any) are shared between the test set and the pos/neg set.\n",
    "We noticed after running that there are no tweets shared between all tweet sets, thus we can label duplicate of the test dataset by directly looking for corresponding duplicates in pos/neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_pos_test = []\n",
    "common_neg_test = []\n",
    "\n",
    "pos = set(pos_tweets)\n",
    "neg = set(neg_tweets)\n",
    "test_tweets = set(test_tweets)\n",
    "len_test = len(test_tweets)\n",
    "for i, test_tweet in enumerate(test_tweets) :\n",
    "    if test_tweet in pos:\n",
    "        common_pos_test.append(test_tweet)\n",
    "        print(\"duplicate pos-test\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))  \n",
    "        break\n",
    "    elif test_tweet in neg:\n",
    "        common_neg_test.append(test_tweet)\n",
    "        print(\"duplicate neg-data\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))\n",
    "        break\n",
    "    print(\"{:.1f}\".format(i/len_test), \"%\", end='\\r')\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occurences(tweet, tweets):\n",
    "    count = 0\n",
    "    for t in tweets:\n",
    "        if t == tweet:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 data pre-processing\n",
    "In this part we start from raw tweets and preprocess them to achieve uniformization of the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Standardization\n",
    "We start by removing repetitions of more than 3 letters, and replace dots by spaces\n",
    "ex : \n",
    "\n",
    "1) `no_dot` : \"Funny.I\" becomes \"Funny I\" \n",
    "\n",
    "2) `find_repetitions` : \"I looooove\" becomes \"I love\"\n",
    "            \n",
    "\n",
    "This is done by the function `standardize_tweets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = standardize_tweets(pos)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = standardize_tweets(neg) \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = standardize_tweets(data) \n",
    "\n",
    "print(\"export processed tweets to files\")\n",
    "export(processed_tweets_pos_full,  \"preprocessed_pos_full\")\n",
    "export(processed_tweets_neg_full,  \"preprocessed_neg_full\")\n",
    "export(processed_tweets_test_full, \"preprocessed_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the vocabs\n",
    "We build vocabs for positive, negative and test tweets, removing tokens present less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build word counters\n",
    "cut_threshold = 5\n",
    "vocab_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#write vocabs to files\n",
    "write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"preprocessed_vocab_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build and Merge the dataframes \n",
    "We build dataframes containing several informations for each type of tweets, and we combine them all in order to facilitate the stemming we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos_full = \"preprocessed_vocab_pos_full\"\n",
    "path_neg_full = \"preprocessed_vocab_neg_full\"\n",
    "path_test_data = \"preprocessed_vocab_test_full\"\n",
    "\n",
    "    \n",
    "pos_full =  build_df(path_pos_full)\n",
    "neg_full =  build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build semantic equivalences\n",
    "We define which word is equivalent to each word (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"Building semantic\")\n",
    "#Build equivalence list\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "print(\"Building haha semantic\")\n",
    "#build \"haha\" equivalences\n",
    "word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    \n",
    "word_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\n",
    "word_haha_list.remove(\"haha\")\n",
    "word_haha_list.insert(0, \"haha\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"filter semantic\")\n",
    "#append the two semantic lists and filter them\n",
    "same_begin.append(list(word_haha_list))\n",
    "semantic = filter_single_rep(same_begin)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Map the equivalent words, stem them all thanks to nltk and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#process tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d'un tableau de tweet   \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"cleaned_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"cleaned_pos_full\")\n",
    "export(processed_tweets_test_full, \"cleaned_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the simple vocab (with occurences) to build the dataframes\n",
    "This is done by using shell commands or python code, as presented before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#build word counters\n",
    "vocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#build vocabs\n",
    "write_vocab_to_file(vocab_pos, \"cleaned_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"cleaned_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"cleaned_vocab_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 create relevant vocab \n",
    "We keep pertinent words in a special vocab called relevant vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataframes before we \n",
    "neg_df = build_df(\"raw_vocab_neg_full_cut=5\")\n",
    "pos_df = build_df(\"raw_vocab_pos_full_cut=5\")\n",
    "\n",
    "#Merge dataframes\n",
    "merged = merging(neg_df, pos_df, False)\n",
    "\n",
    "#create relevant vocab\n",
    "create_relevant_vocab(pertinence_thres=0.3, min_count=300, dataframe=merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Find characteristic words\n",
    "We set a list of words that appear only in neg/pos at least min_diff times such that if we see such a word in a tweet we assume it is a neg/pos tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets = import_(\"relevant_vocab_pert=0.3_count=300\")\n",
    "\n",
    "characteristic_words(data_tweets, merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Recapitulative Function\n",
    "This function does the work presented above, we created them for clarity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean raw tweets\n",
    "def setup(path_pos, path_neg, path_test, is_full,  pertinence_thres_relevant, min_count_relevant, cut_threshold=5):\n",
    "    #path pos, path_neg, path_data : paths of the raw tweets\n",
    "    #is_full : True iff we clean \n",
    "    \n",
    "    #export cleaned tweets and their vocabulary\n",
    "    cleaned_tweets = clean_tweets(path_pos, path_neg, path_test, is_full, cut_threshold)\n",
    "    \n",
    "    #load dataframes    \n",
    "    if is_full:\n",
    "        pos_df = build_df(\"cleaned_vocab_pos_full_bitri=True\", True)\n",
    "        neg_df = build_df(\"cleaned_vocab_neg_full_bitri=True\", True)\n",
    "    else:\n",
    "        pos_df = build_df(\"cleaned_vocab_pos_bitri=True\", True)\n",
    "        neg_df = build_df(\"cleaned_vocab_neg_bitri=True\", True)\n",
    "   \n",
    "    #Merge dataframes\n",
    "    merged = merging(neg_df, pos_df, False)\n",
    "    \n",
    "    #create relevant vocab\n",
    "    create_relevant_vocab(pertinence_thres_relevant, min_count_relevant, merged)\n",
    "    \n",
    "    data_tweets = import_(\"cleaned_test_bitri=True\")\n",
    "    \n",
    "    #create characteristic_words\n",
    "    characteristic_words(data_tweets, merged, is_full)\n",
    "    \n",
    "setup(\"train_pos.txt\", \"train_neg.txt\" , \"test_data.txt\", is_full=False, pertinence_thres_relevant=0.3, min_count_relevant=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
