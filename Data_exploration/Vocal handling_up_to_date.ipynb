{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and pre-process the data\n",
    "\n",
    "\n",
    "\n",
    "In all Machine Learning problem one of the main task is to gain suffitant knowledge about the data one is given. This will ensure to better build the features representation of a tweet and to maximize our \"computational power / prediction accuracy\" ratio.\n",
    "\n",
    "In the first part we will therefore investigate how the data is generated, how many simple words do we have, are there some duplicates, is there any odds with the tweets? How are they classified (we are given two datasets, one for the positive tweets one for negatives) i.e. what is a typical word that occurs when a :( is present in a tweet?\n",
    "\n",
    "\n",
    "In the second part, we normalize the tweets. Some tweets may contain words that are not usefull, or can be categorized. This is done be the following pipeline.\n",
    "\n",
    "1. Import all the tweets from train_pos_full.txt and train_neg_full.txt\n",
    "2. Cast the repetition of more than three following similar letter to one letter\n",
    "3. Find the representative of all words\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "### Helper functions and files\n",
    "\n",
    "IOTweets contains the functions :\n",
    "- build_df\n",
    "- import_\n",
    "- export\n",
    "\n",
    "ProcessTweets contains the functions :\n",
    "- merging\n",
    "- filter_single_rep\n",
    "- powerful_words\n",
    "- sem_by_repr\n",
    "- sem_by_repr2\n",
    "- no_dot\n",
    "- find_repetition\n",
    "- no_s\n",
    "- stem_tweet\n",
    "- set_min_diff\n",
    "- contains\n",
    "- process_tweets_1\n",
    "- process_tweets_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas will be our framework for the first part\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import csv\n",
    "#%pylab inline # depreceated, use individual imports instead\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_pos = \"vocab_cut_pos.txt\"\n",
    "vocab_neg = \"vocab_cut_neg.txt\"\n",
    "\n",
    "vocab_pos_full = \"vocab_cut_pos_full.txt\"\n",
    "vocab_neg_full = \"vocab_cut_neg_full.txt\"\n",
    "vocab_test = \"vocab_test_data.txt\"\n",
    "\n",
    "path_tweets_pos = \"train_pos.txt\"\n",
    "path_tweets_neg = \"train_neg.txt\"\n",
    "\n",
    "path_tweets_pos_full = \"train_pos_full.txt\"\n",
    "path_tweets_neg_full = \"train_neg_full.txt\"\n",
    "path_tweets_test = \"test_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. An insight into the dataset \n",
    "We begin by analysing the non-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_vocab(tweets, cut_threshold, file_name , bitri):\n",
    "    counter = build_vocab_counter(tweets, cut_threshold, bitri)\n",
    "    write_vocab_to_file(counter, (file_name + \"_cut=\" +str(cut_threshold) +\"_bitri=\"+str(bitri)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:12.110466\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:13.933984\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:01.280581\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:02:38.377701\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:03:03.877770\n"
     ]
    }
   ],
   "source": [
    "#import tweets\n",
    "pos_tweets = import_(path_tweets_pos)\n",
    "neg_tweets = import_(path_tweets_neg)\n",
    "pos_full_tweets = import_(path_tweets_pos_full)\n",
    "neg_full_tweets = import_(path_tweets_neg_full)\n",
    "test_tweets = import_without_id(path_tweets_test)\n",
    "\n",
    "#Create vocabs\n",
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "write_vocab(pos_tweets, cut_threshold, \"raw_vocab_pos\" , bitri)\n",
    "write_vocab(neg_tweets, cut_threshold, \"raw_vocab_neg\" , bitri)\n",
    "write_vocab(test_tweets, cut_threshold, \"raw_vocab_test\" , bitri)\n",
    "write_vocab(pos_full_tweets, cut_threshold, \"raw_vocab_pos_full\" , bitri)\n",
    "write_vocab(neg_full_tweets, cut_threshold, \"raw_vocab_neg_full\" , bitri)\n",
    "\n",
    "\n",
    "# build the vocab Dataframe (DF) for full positive and negative tweets\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "# build the vocab Dataframe (DF) for partial positive and negative tweets\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_threshold = 5\n",
    "bitri = True\n",
    "\n",
    "pos_df = build_df(\"raw_vocab_pos_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_df = build_df(\"raw_vocab_neg_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "pos_full_df = build_df(\"raw_vocab_pos_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "neg_full_df = build_df(\"raw_vocab_neg_full_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)\n",
    "\n",
    "#Build the vocab DF for tweets to be decided (i.e. \"test\" tweets)\n",
    "test_df = build_df(\"raw_vocab_test_cut=\"+str(cut_threshold)+\"_bitri=\"+str(bitri), bitri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Study of word pertinence \n",
    "The idea is that the greater a word ratio i, the more pertinent it will be to cosider it when trying to label a tweet that contains it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurence_neg</th>\n",
       "      <th>occurence_pos</th>\n",
       "      <th>difference</th>\n",
       "      <th>somme</th>\n",
       "      <th>ratio</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>(photo,)</td>\n",
       "      <td>3000</td>\n",
       "      <td>3005</td>\n",
       "      <td>5</td>\n",
       "      <td>6005</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>(had, a)</td>\n",
       "      <td>5724</td>\n",
       "      <td>5738</td>\n",
       "      <td>14</td>\n",
       "      <td>11462</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>(all, my)</td>\n",
       "      <td>3308</td>\n",
       "      <td>3299</td>\n",
       "      <td>9</td>\n",
       "      <td>6607</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>(yesterday,)</td>\n",
       "      <td>3799</td>\n",
       "      <td>3786</td>\n",
       "      <td>13</td>\n",
       "      <td>7585</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>(\", .)</td>\n",
       "      <td>1394</td>\n",
       "      <td>1399</td>\n",
       "      <td>5</td>\n",
       "      <td>2793</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>(class,)</td>\n",
       "      <td>5729</td>\n",
       "      <td>5754</td>\n",
       "      <td>25</td>\n",
       "      <td>11483</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>(turn,)</td>\n",
       "      <td>2432</td>\n",
       "      <td>2443</td>\n",
       "      <td>11</td>\n",
       "      <td>4875</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(or,)</td>\n",
       "      <td>24351</td>\n",
       "      <td>24227</td>\n",
       "      <td>124</td>\n",
       "      <td>48578</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>(..., but)</td>\n",
       "      <td>2566</td>\n",
       "      <td>2580</td>\n",
       "      <td>14</td>\n",
       "      <td>5146</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>(next, year)</td>\n",
       "      <td>1576</td>\n",
       "      <td>1567</td>\n",
       "      <td>9</td>\n",
       "      <td>3143</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>(when, he)</td>\n",
       "      <td>1163</td>\n",
       "      <td>1170</td>\n",
       "      <td>7</td>\n",
       "      <td>2333</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>(idea,)</td>\n",
       "      <td>2624</td>\n",
       "      <td>2640</td>\n",
       "      <td>16</td>\n",
       "      <td>5264</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>(that, &lt;user&gt;)</td>\n",
       "      <td>921</td>\n",
       "      <td>927</td>\n",
       "      <td>6</td>\n",
       "      <td>1848</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>(., please)</td>\n",
       "      <td>1532</td>\n",
       "      <td>1542</td>\n",
       "      <td>10</td>\n",
       "      <td>3074</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>(down,)</td>\n",
       "      <td>11021</td>\n",
       "      <td>10947</td>\n",
       "      <td>74</td>\n",
       "      <td>21968</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>(!, !, i)</td>\n",
       "      <td>4970</td>\n",
       "      <td>4935</td>\n",
       "      <td>35</td>\n",
       "      <td>9905</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>(life, .)</td>\n",
       "      <td>1113</td>\n",
       "      <td>1121</td>\n",
       "      <td>8</td>\n",
       "      <td>2234</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>(&lt;user&gt;, no)</td>\n",
       "      <td>6977</td>\n",
       "      <td>7028</td>\n",
       "      <td>51</td>\n",
       "      <td>14005</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>(should, i)</td>\n",
       "      <td>804</td>\n",
       "      <td>798</td>\n",
       "      <td>6</td>\n",
       "      <td>1602</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>(it, when)</td>\n",
       "      <td>1708</td>\n",
       "      <td>1695</td>\n",
       "      <td>13</td>\n",
       "      <td>3403</td>\n",
       "      <td>0.003820</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>(im, in)</td>\n",
       "      <td>761</td>\n",
       "      <td>755</td>\n",
       "      <td>6</td>\n",
       "      <td>1516</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>(is, about)</td>\n",
       "      <td>615</td>\n",
       "      <td>610</td>\n",
       "      <td>5</td>\n",
       "      <td>1225</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>(sometimes,)</td>\n",
       "      <td>2827</td>\n",
       "      <td>2851</td>\n",
       "      <td>24</td>\n",
       "      <td>5678</td>\n",
       "      <td>0.004227</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>(lie,)</td>\n",
       "      <td>1387</td>\n",
       "      <td>1375</td>\n",
       "      <td>12</td>\n",
       "      <td>2762</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5994</th>\n",
       "      <td>(are, all)</td>\n",
       "      <td>681</td>\n",
       "      <td>687</td>\n",
       "      <td>6</td>\n",
       "      <td>1368</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>(!, i, just)</td>\n",
       "      <td>793</td>\n",
       "      <td>786</td>\n",
       "      <td>7</td>\n",
       "      <td>1579</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>(and, she)</td>\n",
       "      <td>1220</td>\n",
       "      <td>1209</td>\n",
       "      <td>11</td>\n",
       "      <td>2429</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>(come, in)</td>\n",
       "      <td>544</td>\n",
       "      <td>549</td>\n",
       "      <td>5</td>\n",
       "      <td>1093</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>(why, not)</td>\n",
       "      <td>613</td>\n",
       "      <td>607</td>\n",
       "      <td>6</td>\n",
       "      <td>1220</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>(about, this)</td>\n",
       "      <td>696</td>\n",
       "      <td>703</td>\n",
       "      <td>7</td>\n",
       "      <td>1399</td>\n",
       "      <td>0.005004</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>(frame, 2, \")</td>\n",
       "      <td>6194</td>\n",
       "      <td>0</td>\n",
       "      <td>6194</td>\n",
       "      <td>6194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>(2, \", wide)</td>\n",
       "      <td>6198</td>\n",
       "      <td>0</td>\n",
       "      <td>6198</td>\n",
       "      <td>6198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>((, kindle, edition)</td>\n",
       "      <td>7139</td>\n",
       "      <td>0</td>\n",
       "      <td>7139</td>\n",
       "      <td>7139</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>(kindle, edition)</td>\n",
       "      <td>7172</td>\n",
       "      <td>0</td>\n",
       "      <td>7172</td>\n",
       "      <td>7172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>((, kindle)</td>\n",
       "      <td>7261</td>\n",
       "      <td>0</td>\n",
       "      <td>7261</td>\n",
       "      <td>7261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>(kit, ()</td>\n",
       "      <td>7394</td>\n",
       "      <td>0</td>\n",
       "      <td>7394</td>\n",
       "      <td>7394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>(), (, paperback)</td>\n",
       "      <td>7944</td>\n",
       "      <td>0</td>\n",
       "      <td>7944</td>\n",
       "      <td>7944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>(wood, frame, ()</td>\n",
       "      <td>9182</td>\n",
       "      <td>0</td>\n",
       "      <td>9182</td>\n",
       "      <td>9182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>((, audio, cd)</td>\n",
       "      <td>9706</td>\n",
       "      <td>0</td>\n",
       "      <td>9706</td>\n",
       "      <td>9706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>(audio, cd)</td>\n",
       "      <td>9823</td>\n",
       "      <td>0</td>\n",
       "      <td>9823</td>\n",
       "      <td>9823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>((, audio)</td>\n",
       "      <td>9995</td>\n",
       "      <td>0</td>\n",
       "      <td>9995</td>\n",
       "      <td>9995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>((, dvd)</td>\n",
       "      <td>10407</td>\n",
       "      <td>0</td>\n",
       "      <td>10407</td>\n",
       "      <td>10407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>(wood, frame)</td>\n",
       "      <td>11775</td>\n",
       "      <td>0</td>\n",
       "      <td>11775</td>\n",
       "      <td>11775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>((, hardcover)</td>\n",
       "      <td>16678</td>\n",
       "      <td>0</td>\n",
       "      <td>16678</td>\n",
       "      <td>16678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>(frame, ()</td>\n",
       "      <td>21266</td>\n",
       "      <td>0</td>\n",
       "      <td>21266</td>\n",
       "      <td>21266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(this, frame, is)</td>\n",
       "      <td>23692</td>\n",
       "      <td>0</td>\n",
       "      <td>23692</td>\n",
       "      <td>23692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(frame, is)</td>\n",
       "      <td>23727</td>\n",
       "      <td>0</td>\n",
       "      <td>23727</td>\n",
       "      <td>23727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(custom, picture)</td>\n",
       "      <td>24002</td>\n",
       "      <td>0</td>\n",
       "      <td>24002</td>\n",
       "      <td>24002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(custom, picture, frame)</td>\n",
       "      <td>24002</td>\n",
       "      <td>0</td>\n",
       "      <td>24002</td>\n",
       "      <td>24002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(this, frame)</td>\n",
       "      <td>24685</td>\n",
       "      <td>0</td>\n",
       "      <td>24685</td>\n",
       "      <td>24685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>(\", wide, complete)</td>\n",
       "      <td>25034</td>\n",
       "      <td>0</td>\n",
       "      <td>25034</td>\n",
       "      <td>25034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(wide, complete)</td>\n",
       "      <td>25035</td>\n",
       "      <td>0</td>\n",
       "      <td>25035</td>\n",
       "      <td>25035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>(picture, frame, /)</td>\n",
       "      <td>25402</td>\n",
       "      <td>0</td>\n",
       "      <td>25402</td>\n",
       "      <td>25402</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>(frame, /, poster)</td>\n",
       "      <td>25405</td>\n",
       "      <td>0</td>\n",
       "      <td>25405</td>\n",
       "      <td>25405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>(/, poster, frame)</td>\n",
       "      <td>25405</td>\n",
       "      <td>0</td>\n",
       "      <td>25405</td>\n",
       "      <td>25405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>(poster, frame)</td>\n",
       "      <td>25408</td>\n",
       "      <td>0</td>\n",
       "      <td>25408</td>\n",
       "      <td>25408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>(/, poster)</td>\n",
       "      <td>25415</td>\n",
       "      <td>0</td>\n",
       "      <td>25415</td>\n",
       "      <td>25415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>(frame, /)</td>\n",
       "      <td>25425</td>\n",
       "      <td>0</td>\n",
       "      <td>25425</td>\n",
       "      <td>25425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>(\", wide)</td>\n",
       "      <td>25488</td>\n",
       "      <td>0</td>\n",
       "      <td>25488</td>\n",
       "      <td>25488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>((, paperback)</td>\n",
       "      <td>40169</td>\n",
       "      <td>0</td>\n",
       "      <td>40169</td>\n",
       "      <td>40169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1079592 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          word  occurence_neg  occurence_pos  difference  \\\n",
       "1201                  (photo,)           3000           3005           5   \n",
       "581                   (had, a)           5724           5738          14   \n",
       "1076                 (all, my)           3308           3299           9   \n",
       "917               (yesterday,)           3799           3786          13   \n",
       "2770                    (\", .)           1394           1399           5   \n",
       "579                   (class,)           5729           5754          25   \n",
       "1492                   (turn,)           2432           2443          11   \n",
       "134                      (or,)          24351          24227         124   \n",
       "1416                (..., but)           2566           2580          14   \n",
       "2430              (next, year)           1576           1567           9   \n",
       "3395                (when, he)           1163           1170           7   \n",
       "1385                   (idea,)           2624           2640          16   \n",
       "4383            (that, <user>)            921            927           6   \n",
       "2504               (., please)           1532           1542          10   \n",
       "291                    (down,)          11021          10947          74   \n",
       "678                  (!, !, i)           4970           4935          35   \n",
       "3585                 (life, .)           1113           1121           8   \n",
       "465               (<user>, no)           6977           7028          51   \n",
       "5067               (should, i)            804            798           6   \n",
       "2192                (it, when)           1708           1695          13   \n",
       "5352                  (im, in)            761            755           6   \n",
       "6696               (is, about)            615            610           5   \n",
       "1287              (sometimes,)           2827           2851          24   \n",
       "2783                    (lie,)           1387           1375          12   \n",
       "5994                (are, all)            681            687           6   \n",
       "5135              (!, i, just)            793            786           7   \n",
       "3202                (and, she)           1220           1209          11   \n",
       "7547                (come, in)            544            549           5   \n",
       "6712                (why, not)            613            607           6   \n",
       "5873             (about, this)            696            703           7   \n",
       "...                        ...            ...            ...         ...   \n",
       "529              (frame, 2, \")           6194              0        6194   \n",
       "526               (2, \", wide)           6198              0        6198   \n",
       "455       ((, kindle, edition)           7139              0        7139   \n",
       "453          (kindle, edition)           7172              0        7172   \n",
       "446                ((, kindle)           7261              0        7261   \n",
       "434                   (kit, ()           7394              0        7394   \n",
       "407          (), (, paperback)           7944              0        7944   \n",
       "354           (wood, frame, ()           9182              0        9182   \n",
       "331             ((, audio, cd)           9706              0        9706   \n",
       "327                (audio, cd)           9823              0        9823   \n",
       "321                 ((, audio)           9995              0        9995   \n",
       "309                   ((, dvd)          10407              0       10407   \n",
       "276              (wood, frame)          11775              0       11775   \n",
       "201             ((, hardcover)          16678              0       16678   \n",
       "161                 (frame, ()          21266              0       21266   \n",
       "142          (this, frame, is)          23692              0       23692   \n",
       "141                (frame, is)          23727              0       23727   \n",
       "138          (custom, picture)          24002              0       24002   \n",
       "139   (custom, picture, frame)          24002              0       24002   \n",
       "132              (this, frame)          24685              0       24685   \n",
       "129        (\", wide, complete)          25034              0       25034   \n",
       "128           (wide, complete)          25035              0       25035   \n",
       "127        (picture, frame, /)          25402              0       25402   \n",
       "125         (frame, /, poster)          25405              0       25405   \n",
       "126         (/, poster, frame)          25405              0       25405   \n",
       "123            (poster, frame)          25408              0       25408   \n",
       "122                (/, poster)          25415              0       25415   \n",
       "121                 (frame, /)          25425              0       25425   \n",
       "120                  (\", wide)          25488              0       25488   \n",
       "67              ((, paperback)          40169              0       40169   \n",
       "\n",
       "      somme     ratio  len  \n",
       "1201   6005  0.000833    1  \n",
       "581   11462  0.001221    2  \n",
       "1076   6607  0.001362    2  \n",
       "917    7585  0.001714    1  \n",
       "2770   2793  0.001790    2  \n",
       "579   11483  0.002177    1  \n",
       "1492   4875  0.002256    1  \n",
       "134   48578  0.002553    1  \n",
       "1416   5146  0.002721    2  \n",
       "2430   3143  0.002864    2  \n",
       "3395   2333  0.003000    2  \n",
       "1385   5264  0.003040    1  \n",
       "4383   1848  0.003247    2  \n",
       "2504   3074  0.003253    2  \n",
       "291   21968  0.003369    1  \n",
       "678    9905  0.003534    3  \n",
       "3585   2234  0.003581    2  \n",
       "465   14005  0.003642    2  \n",
       "5067   1602  0.003745    2  \n",
       "2192   3403  0.003820    2  \n",
       "5352   1516  0.003958    2  \n",
       "6696   1225  0.004082    2  \n",
       "1287   5678  0.004227    1  \n",
       "2783   2762  0.004345    1  \n",
       "5994   1368  0.004386    2  \n",
       "5135   1579  0.004433    3  \n",
       "3202   2429  0.004529    2  \n",
       "7547   1093  0.004575    2  \n",
       "6712   1220  0.004918    2  \n",
       "5873   1399  0.005004    2  \n",
       "...     ...       ...  ...  \n",
       "529    6194  1.000000    3  \n",
       "526    6198  1.000000    3  \n",
       "455    7139  1.000000    3  \n",
       "453    7172  1.000000    2  \n",
       "446    7261  1.000000    2  \n",
       "434    7394  1.000000    2  \n",
       "407    7944  1.000000    3  \n",
       "354    9182  1.000000    3  \n",
       "331    9706  1.000000    3  \n",
       "327    9823  1.000000    2  \n",
       "321    9995  1.000000    2  \n",
       "309   10407  1.000000    2  \n",
       "276   11775  1.000000    2  \n",
       "201   16678  1.000000    2  \n",
       "161   21266  1.000000    2  \n",
       "142   23692  1.000000    3  \n",
       "141   23727  1.000000    2  \n",
       "138   24002  1.000000    2  \n",
       "139   24002  1.000000    3  \n",
       "132   24685  1.000000    2  \n",
       "129   25034  1.000000    3  \n",
       "128   25035  1.000000    2  \n",
       "127   25402  1.000000    3  \n",
       "125   25405  1.000000    3  \n",
       "126   25405  1.000000    3  \n",
       "123   25408  1.000000    2  \n",
       "122   25415  1.000000    2  \n",
       "121   25425  1.000000    2  \n",
       "120   25488  1.000000    2  \n",
       "67    40169  1.000000    2  \n",
       "\n",
       "[1079592 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "neg_full_df\n",
    "\n",
    "#merge the pos and neg vocabs and study penrtinence of words via ratio\n",
    "merged_full = merging(neg_full_df, pos_full_df)\n",
    "merged_full.sort_values(by = [\"ratio\",\"somme\"], ascending=[True, True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From here, we realise that some words have strangely strong occurence in the negative tweets.\n",
    "By seeing the words in context, we realised that some tweets occured more than once.\n",
    "We checked if those words were also in the test_data that we have to classify. The check was positive.\n",
    "We will therefore capture those words, (i.e. \"1gb\" or \"cd-rom\") because they are luckily to be in the test_data set and classify directly the tweets countaining those words. We will also drop all the duplicated tweets for training in order not to let on the side some other words and this will save us power computationnal efficiency.\n",
    "\n",
    "Two example of such tweets are:\n",
    "\n",
    "    1) 1.26 - 7x14 custom picture frame / poster frame 1.265 \" wide complete cherry wood frame ( 440ch this frame is manufactu ... <url>\n",
    "    \n",
    "    2) misc - 50pc diamond burr set - ceramics tile glass lapidary for rotary tools ( misc . assorted shapes and sizes for your ... <url>\n",
    "    \n",
    "Other important notice, some Tweets are almost the same but just fiew things change: Example of a tweet from amazon [here](https://www.amazon.com/Custom-Picture-Frame-Poster-Complete/dp/B004FNYSBA?SubscriptionId=AKIAJ6364XFIEG2FHXPA&tag=megaebookmall-20&linkCode=sp1&camp=2025&creative=165953&creativeASIN=B004FNYSBA) \n",
    "\n",
    "1. `3x14 custom picture frame / poster frame 1.265 \" wide complete black wood frame ( 440bk this frame is manufactur ... <url>`\n",
    "\n",
    "2. `24x35 custom picture frame / poster frame 1.265 \" wide complete green wood frame ( 440gr this frame is manufactu ... <url>`\n",
    "\n",
    "3. `22x31 custom picture frame / poster frame 2 \" wide complete black executive leather frame ( 74093 this frame is ... <url>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Study of names with same beginning \n",
    "Let's try to have an insight of words that have the same beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are words (filtered by length) whose beginning match another word close to this word?\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "\n",
    "#We visualize which word would be mapped on which one \n",
    "same_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Expected syntaxic divergences\n",
    "As expected, words like \"haha\" can be written in many ways (extended length, typo..). We know that they mean the same,it is therefore interesting to spot them in order to later stem them to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words beginning with \"haha\" or \"ahah\" are sure to be \"instances\" of haha\n",
    "word_haha = test_df.loc[test_df.word.str.startswith(\"haha\") | test_df.word.str.startswith(\"ahah\")]\n",
    "\n",
    "\n",
    "#print(\"Occurence of the words that can be remplaced by haha = \"+ str(int(word_haha.occurence.sum()) - int(word_haha[word_haha.word == \"haha\"].occurence)))\n",
    "\n",
    "# We make the list of all those words that have this same semantic\n",
    "word_haha_list = list(word_haha.word)\n",
    "list(word_haha.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Study duplicates\n",
    "We noticed that several tweets were duplicated in test, pos and neg tweets. We study ho many they are, and which tweets (if any) are shared between the test set and the pos/neg set.\n",
    "We noticed after running that there are no tweets shared between all tweet sets, thus we can label duplicate of the test dataset by directly looking for corresponding duplicates in pos/neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_pos_test = []\n",
    "common_neg_test = []\n",
    "\n",
    "pos = set(pos_tweets)\n",
    "neg = set(neg_tweets)\n",
    "test_tweets = set(test_tweets)\n",
    "len_test = len(test_tweets)\n",
    "for i, test_tweet in enumerate(test_tweets) :\n",
    "    if test_tweet in pos:\n",
    "        common_pos_test.append(test_tweet)\n",
    "        print(\"duplicate pos-test\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))  \n",
    "        break\n",
    "    elif test_tweet in neg:\n",
    "        common_neg_test.append(test_tweet)\n",
    "        print(\"duplicate neg-data\")\n",
    "        print(\"\\ttweet :\" + str(pos_tweet))\n",
    "        break\n",
    "    print(\"{:.1f}\".format(i/len_test), \"%\", end='\\r')\n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def occurences(tweet, tweets):\n",
    "    count = 0\n",
    "    for t in tweets:\n",
    "        if t == tweet:\n",
    "            count += 1\n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 data pre-processing\n",
    "In this part we start from raw tweets and preprocess them to achieve uniformization of the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load raw tweets\n",
    "pos = import_(path_tweets_pos_full)\n",
    "neg = import_(path_tweets_neg_full)\n",
    "data = import_without_id(path_tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Data Standardization\n",
    "We start by removing repetitions of more than 3 letters, and replace dots by spaces\n",
    "ex : \n",
    "\n",
    "1) `no_dot` : \"Funny.I\" becomes \"Funny I\" \n",
    "\n",
    "2) `find_repetitions` : \"I looooove\" becomes \"I love\"\n",
    "            \n",
    "\n",
    "This is done by the function `standardize_tweets`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardize tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = standardize_tweets(pos)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = standardize_tweets(neg) \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = standardize_tweets(data) \n",
    "\n",
    "print(\"export processed tweets to files\")\n",
    "export(processed_tweets_pos_full,  \"preprocessed_pos_full\")\n",
    "export(processed_tweets_neg_full,  \"preprocessed_neg_full\")\n",
    "export(processed_tweets_test_full, \"preprocessed_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the vocabs\n",
    "We build vocabs for positive, negative and test tweets, removing tokens present less than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build word counters\n",
    "cut_threshold = 5\n",
    "vocab_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#write vocabs to files\n",
    "write_vocab_to_file(vocab_pos, \"preprocessed_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"preprocessed_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"preprocessed_vocab_test_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Build and Merge the dataframes \n",
    "We build dataframes containing several informations for each type of tweets, and we combine them all in order to facilitate the stemming we will do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos_full = \"preprocessed_vocab_pos_full\"\n",
    "path_neg_full = \"preprocessed_vocab_neg_full\"\n",
    "path_test_data = \"preprocessed_vocab_test_full\"\n",
    "\n",
    "    \n",
    "pos_full =  build_df(path_pos_full)\n",
    "neg_full =  build_df(path_neg_full)\n",
    "test_data = build_df(path_test_data)\n",
    "    \n",
    "    \n",
    "#Merge Pos and neg vocabs    \n",
    "merged_full = merging(neg_full, pos_full, True)\n",
    "merged_full = merging(merged_full, test_data, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Build semantic equivalences\n",
    "We define which word is equivalent to each word (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''print(\"Building semantic\")\n",
    "#Build equivalence list\n",
    "same_begin = list(merged_full[merged_full[\"len\"]==8][\"word\"])\n",
    "same_begin = [list(merged_full.loc[(merged_full.word.str.startswith(w))][\"word\"]) for w in same_begin]\n",
    "    \n",
    "print(\"Building haha semantic\")\n",
    "#build \"haha\" equivalences\n",
    "word_haha_test = test_data.loc[test_data.word.str.startswith(\"haha\") | test_data.word.str.startswith(\"ahah\")]\n",
    "word_haha_pos = pos_full.loc[pos_full.word.str.startswith(\"haha\") | pos_full.word.str.startswith(\"ahah\")]\n",
    "word_haha_neg = neg_full.loc[neg_full.word.str.startswith(\"haha\") | neg_full.word.str.startswith(\"ahah\")]\n",
    "    \n",
    "word_haha_list = list(set(list(word_haha_test.word)+list(word_haha_pos.word)+list(word_haha_neg.word)))\n",
    "word_haha_list.remove(\"haha\")\n",
    "word_haha_list.insert(0, \"haha\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"filter semantic\")\n",
    "#append the two semantic lists and filter them\n",
    "same_begin.append(list(word_haha_list))\n",
    "semantic = filter_single_rep(same_begin)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Map the equivalent words, stem them all thanks to nltk and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#process tweets\n",
    "print(\"Process data pos\")\n",
    "processed_tweets_pos_full = stem_tweets( processed_tweets_pos_full, semantic)\n",
    "print(\"Process data neg\")\n",
    "processed_tweets_neg_full = stem_tweets( processed_tweets_neg_full, semantic) #Sous form d'un tableau de tweet   \n",
    "print(\"Process data test\")\n",
    "processed_tweets_test_full = stem_tweets( processed_tweet_data_full, semantic) #Sous form d'un tableau de tweet    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"export data\")\n",
    "#export tweets\n",
    "export(processed_tweets_neg_full,  \"cleaned_neg_full\")\n",
    "export(processed_tweets_pos_full,  \"cleaned_pos_full\")\n",
    "export(processed_tweets_test_full, \"cleaned_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the simple vocab (with occurences) to build the dataframes\n",
    "This is done by using shell commands or python code, as presented before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''#build word counters\n",
    "vocap_pos =  build_vocab_counter(processed_tweets_pos_full, cut_threshold, True)\n",
    "vocab_neg =  build_vocab_counter(processed_tweets_neg_full, cut_threshold, True)\n",
    "vocab_test = build_vocab_counter(processed_tweets_test_full, cut_threshold, True)\n",
    "\n",
    "#build vocabs\n",
    "write_vocab_to_file(vocab_pos, \"cleaned_vocab_pos_full\")\n",
    "write_vocab_to_file(vocab_neg, \"cleaned_vocab_neg_full\")\n",
    "write_vocab_to_file(vocab_test, \"cleaned_vocab_test_full\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 create relevant vocab \n",
    "We keep pertinent words in a special vocab called relevant vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load dataframes before we \n",
    "neg_df = build_df(\"raw_vocab_neg_full_cut=5\")\n",
    "pos_df = build_df(\"raw_vocab_pos_full_cut=5\")\n",
    "\n",
    "#Merge dataframes\n",
    "merged = merging(neg_df, pos_df, False)\n",
    "\n",
    "#create relevant vocab\n",
    "create_relevant_vocab(pertinence_thres=0.3, min_count=300, dataframe=merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Find characteristic words\n",
    "We set a list of words that appear only in neg/pos at least min_diff times such that if we see such a word in a tweet we assume it is a neg/pos tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tweets = import_(\"relevant_vocab_pert=0.3_count=300\")\n",
    "\n",
    "characteristic_words(data_tweets, merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Recapitulative Function\n",
    "This function does the work presented above, we created them for clarity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n",
      "Process data pos\n",
      "Process data neg\n",
      "Process data test\n",
      "build vocab data pos\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:10.767453\n",
      "build vocab data neg\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:13.057752\n",
      "build vocab test data\n",
      "removing items present less than 5 times\n",
      "Time elpased (hh:mm:ss.ms) 0:00:01.261803\n",
      "export data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'write_relevance_to_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b9457b0a1a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_pos.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_neg.txt\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"test_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_full\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpertinence_thres_relevant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count_relevant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8b9457b0a1a5>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(path_pos, path_neg, path_test, is_full, pertinence_thres_relevant, min_count_relevant, cut_threshold, bitri)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#create relevant vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcreate_relevant_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpertinence_thres_relevant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count_relevant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdata_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cleaned_test_bitri=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbitri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/EPFL (2013-2015)/Python/Machine Learning/ML_project2/Data_exploration/ProcessTweets.py\u001b[0m in \u001b[0;36mcreate_relevant_vocab\u001b[0;34m(pertinence_thres, min_count, dataframe)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mwrite_relevance_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relevant_vocab_pert=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpertinence_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_count=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m#relevant.to_csv(sep=\"\\t\", path_or_buf=(\"relevant_vocab_pert=\"+str(pertinence_thres)+\"_count=\"+str(min_count)), header=True, index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write_relevance_to_file' is not defined"
     ]
    }
   ],
   "source": [
    "#clean raw tweets\n",
    "def setup(path_pos, path_neg, path_test, is_full,  pertinence_thres_relevant, min_count_relevant, cut_threshold=5, bitri = True):\n",
    "    #path pos, path_neg, path_data : paths of the raw tweets\n",
    "    #is_full : True iff we clean \n",
    "    \n",
    "    #export cleaned tweets and their vocabulary\n",
    "    cleaned_tweets = clean_tweets(path_pos, path_neg, path_test, is_full, cut_threshold)\n",
    "    \n",
    "    if is_full:\n",
    "        full_string = \"_full\"\n",
    "    else :\n",
    "        full_string = \"\"\n",
    "    \n",
    "    \n",
    "    #load dataframes    \n",
    "    pos_df = build_df(\"cleaned_vocab_pos\"+str(full_string)+\"_bitri=\"+str(bitri), bitri)\n",
    "    neg_df = build_df(\"cleaned_vocab_neg\"+str(full_string)+\"_bitri=\"+str(bitri), bitri)\n",
    "   \n",
    "    #Merge dataframes\n",
    "    merged = merging(neg_df, pos_df, False)\n",
    "    \n",
    "    #create relevant vocab\n",
    "    create_relevant_vocab(pertinence_thres_relevant, min_count_relevant, merged)\n",
    "    \n",
    "    data_tweets = import_(\"cleaned_test_bitri=\"+str(bitri))\n",
    "    \n",
    "    #create characteristic_words\n",
    "    characteristic_words(data_tweets, merged, is_full)\n",
    "    \n",
    "    #build the global vocabulary\n",
    "    build_global_vocab(is_full, bitri, cut_threshold)\n",
    "    \n",
    "    \n",
    "setup(\"train_pos.txt\", \"train_neg.txt\" , \"test_data.txt\", is_full=False, pertinence_thres_relevant=0.3, min_count_relevant=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
