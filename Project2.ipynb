{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book for the project 2 \n",
    "\n",
    "Kaggle competition link: [Submition]('https://www.kaggle.com/c/epfml17-text/submit')\n",
    "\n",
    "## Pipeline: \n",
    "\n",
    "\n",
    "### Create cooc matrix\n",
    "\n",
    "1. sh build_vocab.sh\n",
    "2. sh cut_vocab.sh\n",
    "3. python3 pickle_vocab.py\n",
    "4. python3 cooc.py\n",
    "\n",
    "Now given the co-occurrence matrix and the vocabulary, it is not hard to train GloVe word embeddings, that is to compute an embedding vector for wach word in the vocabulary. We suggest to implement SGD updates to train the matrix factorization, as in\n",
    "\n",
    "5. python3 glove_template.py\n",
    "\n",
    "Once you tested your system on the small set of 10% of all tweets, we suggest you run on the full datasets pos_train_full.txt, neg_train_full.txt\n",
    "\n",
    "### Building a Text Classifier:\n",
    "\n",
    "1. Construct Features for the Training Texts: Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet).\n",
    "\n",
    "2. Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels indicate if a tweet used to contain a üôÇ or üôÅ smiley.\n",
    "\n",
    "3. Prediction: Predict labels for all tweets in the test set.\n",
    "\n",
    "4. Submission / Evaluation: Submit your predictions to kaggle, and verify the obtained misclassification error score. (You can also use a local separate validation set to get faster feedback on the accuracy of your system). Try to tune your system for best evaluation score.\n",
    "\n",
    "### Extensions:\n",
    "Naturally, there are many ways to improve your solution, both in terms of accuracy and computation speed. More advanced techniques can be found in the recent literature.\n",
    "\n",
    "\n",
    "TODO :\n",
    "\n",
    "- implement cross-validation\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing usefull library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "from sklearn import linear_model, preprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_storing_ts = 'embeddings.npy'\n",
    "file_storing_pos_ts_tweets = 'train_pos.txt'\n",
    "file_storing_neg_ts_tweets = 'train_neg.txt'\n",
    "file_storing_te_tweets = 'test_data.txt'\n",
    "file_storing_relevant_vocab = 'new_vocab'\n",
    "pertinence = 200\n",
    "nb_dim = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_embeddings(file_name='cooc_full.pkl', destination = file_storing_ts):\n",
    "    print(\"loading cooccurrence matrix\")\n",
    "    with open(file_name, 'rb') as f:\n",
    "        cooc = pickle.load(f)\n",
    "    print(\"{} nonzero entries\".format(cooc.nnz))\n",
    "    \n",
    "    nmax = 100\n",
    "    print(\"using nmax =\", nmax, \", cooc.max() =\", cooc.max())\n",
    "    print(\"initializing embeddings\")\n",
    "    embedding_dim = 20\n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "    epochs = 3\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            \n",
    "            f = ((n / nmax)**alpha) if n < nmax else 1\n",
    "            inter_cost = (xs[ix]@(ys[jy]) - np.log(n))\n",
    "            # We compute the gradients for both context and main vector words\n",
    "            grad_main = f * inter_cost * ys[jy]\n",
    "            grad_context = f * inter_cost * xs[ix]\n",
    "    \n",
    "            # Update the vector words\n",
    "            xs[ix] = xs[ix] - (eta * grad_main)\n",
    "            ys[jy] = ys[jy] - (eta * grad_context)\n",
    "            \n",
    "    np.save(destination, xs)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Construct words_embeddings for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cooccurrence matrix\n",
      "46597044 nonzero entries\n",
      "using nmax = 100 , cooc.max() = 2599902\n",
      "initializing embeddings\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n"
     ]
    }
   ],
   "source": [
    "word_embeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Load words for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f :\n",
    "    vocab = pickle.load(f)\n",
    "xs = np.load(file_storing_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3.Construct features for training tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = pd.read_csv(filepath_or_buffer=\"./new_vocab\", sep=\" \")\n",
    "weights = new_vocab[[\"word\", \"ratio\"]]\n",
    "weights = dict(zip(weights.word, weights.ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(tweets):\n",
    "    features = []\n",
    "    invalid_features = 0;\n",
    "\n",
    "    for line in tweets:\n",
    "        #we differentiate tweets containing pertinent words, those in 'weights'\n",
    "        sum_w_pertinent = np.zeros(nb_dim)\n",
    "        sum_w_others = np.zeros(nb_dim)\n",
    "        \n",
    "        count_pertinent = 0\n",
    "        count_other = 0\n",
    "        \n",
    "        for word in line.split():\n",
    "            local_w = vocab.get(word, -1)\n",
    "            if local_w != -1:\n",
    "                weight = weights.get(word, -1)\n",
    "                if weight != -1:                \n",
    "                    \n",
    "                    #If the word is pertinent, we add its word representation to others pertinents word's representation\n",
    "                    count_pertinent += 1 + (weight*pertinence)\n",
    "                    sum_w_pertinent += xs[local_w] * (1 + (weight*pertinence))\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    #If the word is not pertinent, we add its representation to non-pertinent words representations\n",
    "                    count_other += 1 \n",
    "                    sum_w_others += xs[local_w] \n",
    "     \n",
    "            # If we found pertinent words, we only use them\n",
    "        if(count_pertinent != 0):            \n",
    "            features.append(sum_w_pertinent/count_pertinent)\n",
    "            \n",
    "            #if we found only non-pertinent words, we use them anyway\n",
    "        elif count_other!= 0:\n",
    "            features.append(sum_w_others/count_other)\n",
    "            \n",
    "            #if we did not find words that have representation, we do not try to create features and signal how many they were\n",
    "        else:\n",
    "            invalid_features += 1\n",
    "            \n",
    "    features = np.array(features)\n",
    "    \n",
    "    return features, invalid_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = open(file_storing_pos_ts_tweets, 'r').readlines() \n",
    "neg_tweets = open(file_storing_neg_ts_tweets, 'r').readlines()\n",
    "\n",
    "pos_features, invalid_pos = construct_features(pos_tweets)\n",
    "neg_features, invalid_neg = construct_features(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train the linear classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier(n_iter=400, warm_start=True);\n",
    "X = np.concatenate((pos_features, neg_features))\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Y = np.concatenate((np.ones(len(pos_features)), np.full(len(neg_features), -1)))\n",
    "X = scaler.fit_transform(X, Y)\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  63.2153446905 %\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy : \", (1 - (np.sum(np.abs(Y-prediction))/(2*len(Y)))) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
