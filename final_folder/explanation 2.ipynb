{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Note book for the project 2, part 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing usefull libraries and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from split_data import split_data\n",
    "%matplotlib inline\n",
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "from sklearn import linear_model, preprocessing, neural_network\n",
    "import numpy as np\n",
    "from IOTweets import *\n",
    "from ProcessTweets import *\n",
    "import pickle\n",
    "import random\n",
    "#from joblib import Parallel, delayed\n",
    "import csv\n",
    "from feature_helper import *\n",
    "from pickle_vocab import *\n",
    "from cooc import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Variables\n",
    "Then we define the name of the files we will sometimes use as well as the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define filenames and variables\n",
    "\n",
    "#embeddings\n",
    "\n",
    "embeddings = \"embeddings_full.npy\"\n",
    "\n",
    "embeddings_ts =      'embeddings_ts.npy'\n",
    "embeddings_te =      'embeddings_te.npy'\n",
    "embeddings_ts_full = 'embeddings_ts_full.npy'\n",
    "embeddings_te_full = 'embeddings_te_full.npy'\n",
    "\n",
    "embeddings_preprocessed = 'embeddings_preprocessed.npy'\n",
    "#tweets\n",
    "pos_ts_tweets =      'cleaned_pos_bitri=True'\n",
    "neg_ts_tweets =      'cleaned_neg_bitri=True'\n",
    "pos_ts_full_tweets = 'cleaned_pos_full_bitri=True'\n",
    "neg_ts_full_tweets = 'cleaned_neg_full_bitri=True'\n",
    "te_full_tweets =     'cleaned_test_bitri=True'\n",
    "\n",
    "#vocab\n",
    "file_vocab = 'vocab.pkl'\n",
    "file_vocab_preprocess = 'vocab_cut_preprocessed_full.pkl'\n",
    "\n",
    "\n",
    "#coocurrence matrices\n",
    "cooc_full = 'cooc_full.pkl'\n",
    "cooc_partial = 'cooc.pkl'\n",
    "cooc_preprocessed = 'cooc_preprocessed.pkl'\n",
    "cooc_stemed_full = \"cooc_stemed_full.pkl\"\n",
    "\n",
    "#Features variables\n",
    "#pertinence = see construct_features.py\n",
    "nb_dim = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Here we define our own GloVe function and an accuracy detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(prediction, actual_emotions):\n",
    "    return (1 - (np.sum(np.abs(actual_emotions-prediction))/(2*len(actual_emotions)))) * 100\n",
    "\n",
    "\n",
    "def GloVe(file_name=\"cooc_partial\", destination=embeddings_ts, embedding_dim = 50):\n",
    "    #load coocurence matrix\n",
    "    with open(file_name, 'rb') as f:\n",
    "        cooc = pickle.load(f)    \n",
    "    \n",
    "    nmax = 100\n",
    "    \n",
    "    eta = 0.001\n",
    "    alpha = 3 / 4\n",
    "    epochs = 10\n",
    "    \n",
    "    xs = np.random.normal(size=(cooc.shape[0], embedding_dim))\n",
    "    ys = np.random.normal(size=(cooc.shape[1], embedding_dim))\n",
    "   \n",
    "    #Construct vector representations xs for words\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch {}\".format(epoch))\n",
    "        loading_counter = 0\n",
    "        for ix, jy, n in zip(cooc.row, cooc.col, cooc.data):\n",
    "            \n",
    "            f = ((n / nmax)**alpha) if n < nmax else 1\n",
    "            inter_cost = (xs[ix]@(ys[jy]) - np.log(n))\n",
    "            # We compute the gradients for both context and main vector words\n",
    "            grad_main = f * inter_cost * ys[jy]\n",
    "            grad_context = f * inter_cost * xs[ix]\n",
    "    \n",
    "            # Update the vector words\n",
    "            xs[ix] = xs[ix] - (eta * grad_main)\n",
    "            ys[jy] = ys[jy] - (eta * grad_context)\n",
    "            \n",
    "            if loading_counter%20000==1:\n",
    "                    print(\"{:.1f}\".format(loading_counter/len(cooc.col)*100), \"%\", end='\\r')\n",
    "            loading_counter+=1\n",
    "            \n",
    "    #Store xs in destination file\n",
    "    np.save(file=destination, arr=xs)\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating coocurence matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the global vocab, which contains: pos, neg and the test vocab\n",
    "build_global_vocab(False, True, 5)\n",
    "\n",
    "# Create the vocab from the global vocab\n",
    "to_vocab_pickle(\"global_vocab_cut=5\")\n",
    "\n",
    "# Create the cooc matrix from the\n",
    "to_cooc(path_vocab=\"global_vocab.pkl\", path_tweets_pos=\"cleaned_vocab_pos_bitri=True\", path_tweets_neg=\"cleaned_vocab_neg_bitri=True\")\n",
    "\n",
    "# create all tweets\n",
    "        \n",
    "#commande shell to happen all tweets\n",
    "#!cat cleaned_neg_bitri\\=True cleaned_pos_bitri\\=True cleaned_test_bitri\\=True > all_tweets_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the co-occurrence matrix and the vocabulary, it is not hard to train GloVe word embeddings, that is to compute an embedding vector for wach word in the vocabulary. We suggest to implement SGD updates to train the matrix factorization.\n",
    "\n",
    "Construct Features for the Training Texts: Load the training tweets and the built GloVe word embeddings. Using the word embeddings, construct a feature representation of each training tweet (by averaging the word vectors over all words of the tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe(file_name=\"cooc_bitri=True.pkl\", destination=\"embeddings_bitri=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify tweets\n",
    "Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels indicate if a tweet used to contain a üôÇ or üôÅ smiley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_and_predict(pertinent_lb):    \n",
    "    \n",
    "    \n",
    "    #define relevant_vocab file to use\n",
    "    relevant_vocab = 'relevant_vocab_pert=0.3_count=300'#'relevant_vocab_lb='+str(pertinent_lb)+'.txt'\n",
    "    \n",
    "    #load ratios into a dictionary\n",
    "    weights = extract_relevant(relevant_vocab)\n",
    "    \n",
    "    #Split positive tweets into training and testing sets\n",
    "    pos_tweets = np.array(open(pos_ts_full_tweets, 'r').readlines()) \n",
    "    labels_pos = np.ones(len(pos_tweets))\n",
    "    pos_tr, labels_pos_tr, pos_te, labels_pos_te = split_data(pos_tweets, labels_pos, 0.996)\n",
    "    \n",
    "   \n",
    "\n",
    "    #Split negative tweets into training and testing sets\n",
    "    neg_tweets = np.array(open(neg_ts_full_tweets, 'r').readlines())\n",
    "    labels_neg = np.full(len(neg_tweets), -1)\n",
    "    neg_tr, label_neg_tr, neg_te, labels_neg_te = split_data(neg_tweets, labels_neg, 0.996)\n",
    "            \n",
    "    #Find features for each tweet with at least one word within vocab, get indices of unpredictable tweets\n",
    "    #for pos tweetspos_tr\n",
    "    pos_tr_feat, invalid_pos_tr = construct_features(\"global_vocab_cut=5\", pos_tr, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", -1)\n",
    "    pos_te_feat, invalid_pos_te = construct_features(\"global_vocab_cut=5\", pos_te, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", -1)\n",
    "\n",
    "    #for neg tweets\n",
    "    neg_tr_feat, invalid_neg_tr = construct_features(\"global_vocab_cut=5\", neg_tr, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", -1)\n",
    "    neg_te_feat, invalid_neg_te = construct_features(\"global_vocab_cut=5\", neg_te, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", -1)\n",
    "    \n",
    "    print(len(pos_tr_feat), len(invalid_pos_tr))\n",
    "    print(len(pos_te_feat), len(invalid_pos_te))\n",
    "    print(len(neg_tr_feat), len(invalid_neg_tr))\n",
    "    print(len(neg_te_feat), len(invalid_neg_te))\n",
    "    \n",
    "    #Initialize classifier and scaler\n",
    "    neural = neural_network.MLPClassifier(hidden_layer_sizes=(nb_dim, nb_dim, nb_dim/2, 2))\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    #fit classifier on predictable tweets\n",
    "    X = np.concatenate((pos_tr_feat, neg_tr_feat))\n",
    "    Y = np.concatenate((np.ones(len(pos_tr_feat)), np.full(len(neg_tr_feat), -1)))\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "    neural = neural.fit(X, Y)\n",
    "\n",
    "    #scale data that should be predicted\n",
    "    pos_te_feat_scaled = pos_te_feat\n",
    "    pos_te_feat_scaled = scaler.fit_transform(pos_te_feat_scaled, np.ones(len(pos_te_feat))) \n",
    "\n",
    "    neg_te_feat_scaled = neg_te_feat\n",
    "    neg_te_feat_scaled = scaler.fit_transform(neg_te_feat_scaled, np.ones(len(neg_te_feat))) \n",
    "\n",
    "    #predict predictable tweets\n",
    "    pos_prediction = neural.predict(pos_te_feat_scaled)\n",
    "    neg_prediction = neural.predict(neg_te_feat_scaled)\n",
    "\n",
    "    #merge with unpredictable tweets predictions\n",
    "    pos_labels = assemble(pos_prediction, invalid_pos_te)\n",
    "    neg_labels = assemble(neg_prediction, invalid_neg_te)\n",
    "\n",
    "    #merge all predictions\n",
    "    labels = np.concatenate((pos_labels, neg_labels))\n",
    "    true_labels = np.concatenate((np.ones(len(pos_labels)), np.full(len(neg_labels), -1)))\n",
    "\n",
    "    return labels, true_labels    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, true_labels = build_and_predict(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission procedure\n",
    "Prediction: Predict labels for all tweets in the test set.\n",
    "\n",
    "Submission / Evaluation: Submit your predictions to kaggle, and verify the obtained misclassification error score. (You can also use a local separate validation set to get faster feedback on the accuracy of your system). Try to tune your system for best evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(pertinence):\n",
    "    #Load words from tweet set\n",
    "    #xs = np.load(embeddings_ts_full)\n",
    "    \n",
    "    #define relevant_vocab file to use\n",
    "    relevant_vocab = 'relevant_vocab_full_lb=1000.txt'\n",
    "    \n",
    "    #load ratios into a dictionary\n",
    "    weights = extract_relevant(relevant_vocab)\n",
    "    \n",
    "    print(len(new_vocab))\n",
    "    \n",
    "    pos_tweets = np.array(open(file=pos_ts_full_tweets, mode='r', encoding=\"utf8\").readlines()) \n",
    "    neg_tweets = np.array(open(file=neg_ts_full_tweets, mode='r', encoding=\"utf8\").readlines()) \n",
    "    te_tweets = np.array(open(file=te_full_tweets, mode='r', encoding=\"utf8\").readlines()) \n",
    "\n",
    "    \n",
    "    #Find features for each tweet with at least one word within vocab, get indices of unpredictable tweets\n",
    "    #for training tweets\n",
    "    pos_ts_full_feat, invalid_pos_ts_full = construct_features(\"global_vocab_cut=5\", pos_tweets, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", 5)\n",
    "\n",
    "    neg_ts_full_feat, invalid_neg_ts_full = construct_features(\"global_vocab_cut=5\", neg_tweets, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", 5)\n",
    "    \n",
    "    print(len(pos_ts_full_feat), len(invalid_pos_ts_full) )\n",
    "    print(len(neg_ts_full_feat), len(invalid_neg_ts_full) )\n",
    "    \n",
    "    #for test tweets\n",
    "    te_full_feat, invalid_te_full = construct_features(\"global_vocab_cut=5\", te_tweets, \"embeddings_bitri=True.npy\", \"relevant_vocab_pert=0.3_count=300\", 5)\n",
    "    print(len(te_full_feat), len(invalid_te_full) )\n",
    "    \n",
    "     #Initialize classifier and scaler\n",
    "    neural = neural_network.MLPClassifier(hidden_layer_sizes=(nb_dim, nb_dim, nb_dim/2, 2))\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    #fit classifier on predictable tweets\n",
    "    X = np.concatenate((pos_ts_full_feat, neg_ts_full_feat))\n",
    "    Y = np.concatenate((np.ones(len(pos_ts_full_feat)), np.full(len(neg_ts_full_feat), -1)))\n",
    "    X = scaler.fit_transform(X, Y)\n",
    "    neural = neural.fit(X, Y)\n",
    "\n",
    "    #scale data that should be predicted\n",
    "    te_full_feat_scaled = te_full_feat\n",
    "    te_full_feat_scaled = scaler.fit_transform(te_full_feat_scaled, np.ones(len(te_full_feat))) \n",
    "\n",
    "\n",
    "    #predict predictable tweets\n",
    "    te_prediction = neural.predict(te_full_feat_scaled)\n",
    "\n",
    "    #merge with unpredictable tweets predictions\n",
    "    labels = assemble(te_prediction, invalid_te_full)\n",
    "   \n",
    "    return labels\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = submit(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_te = labels\n",
    "with open('submission.csv', 'w',) as f2:\n",
    "    fields = ('Id', 'Prediction')\n",
    "    wr = csv.DictWriter(f2, fieldnames=fields, lineterminator = '\\n')\n",
    "    wr.writeheader()\n",
    "    \n",
    "    for id_tweet, prediction in enumerate(predict_te):\n",
    "        wr.writerow({'Id':id_tweet+1, 'Prediction': (int)(prediction)})\n",
    "        print(id_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_te = submit(35)\n",
    "with open('submission.csv', 'w',) as f2:\n",
    "    fields = ('Id', 'Prediction')\n",
    "    wr = csv.DictWriter(f2, fieldnames=fields, lineterminator = '\\n')\n",
    "    wr.writeheader()\n",
    "    \n",
    "    for id_tweet, prediction in enumerate(predict_te):\n",
    "        wr.writerow({'Id':id_tweet+1, 'Prediction': (int)(prediction)})\n",
    "        print(id_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
